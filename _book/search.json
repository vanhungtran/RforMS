[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Mass Spectrometry",
    "section": "",
    "text": "Preface\nWelcome to “R for Mass Spectrometry” - a comprehensive guide to analyzing mass spectrometry data using the R programming language and its rich ecosystem of specialized packages.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "R for Mass Spectrometry",
    "section": "About This Book",
    "text": "About This Book\nMass spectrometry (MS) has become an indispensable tool in analytical chemistry, proteomics, metabolomics, and many other scientific disciplines. As the complexity and volume of MS data continue to grow, computational tools for data processing and analysis have become essential. R, with its extensive statistical capabilities and specialized packages for mass spectrometry, provides an excellent platform for comprehensive MS data analysis.\nThis book aims to bridge the gap between mass spectrometry theory and practical computational implementation, providing readers with both conceptual understanding and hands-on experience in MS data analysis using R.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-this-book-is-for",
    "href": "index.html#who-this-book-is-for",
    "title": "R for Mass Spectrometry",
    "section": "Who This Book Is For",
    "text": "Who This Book Is For\nThis book is designed for:\n\nGraduate students in analytical chemistry, biochemistry, or related fields\nResearchers working with mass spectrometry data\nData scientists entering the field of analytical chemistry\nBioinformaticians specializing in proteomics or metabolomics\nAnyone interested in learning computational approaches to MS data analysis",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "R for Mass Spectrometry",
    "section": "Prerequisites",
    "text": "Prerequisites\nReaders should have:\n\nBasic knowledge of R programming\nFamiliarity with fundamental mass spectrometry concepts\nUnderstanding of basic statistics\nExperience with data analysis workflows (helpful but not required)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-youll-learn",
    "href": "index.html#what-youll-learn",
    "title": "R for Mass Spectrometry",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\nBy the end of this book, you will be able to:\n\nSet up and configure R environments for MS data analysis\nImport, process, and visualize various MS data formats\nImplement spectral preprocessing and peak detection algorithms\nPerform statistical analysis of MS datasets\nConduct metabolomics and proteomics data analysis workflows\nApply machine learning techniques to MS data\nDevelop and validate analytical methods\nCreate reproducible analysis pipelines",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#book-structure",
    "href": "index.html#book-structure",
    "title": "R for Mass Spectrometry",
    "section": "Book Structure",
    "text": "Book Structure\nThe book is organized into progressively advanced topics:\n\nFundamentals - R basics and MS data structures\nData Handling - File formats and data import/export\nPreprocessing - Spectral cleaning and preparation\nPeak Analysis - Detection and quantification methods\nVisualization - Creating informative plots and graphics\nStatistics - Hypothesis testing and multivariate analysis\nMetabolomics - Specialized workflows for metabolite analysis\nProteomics - Protein identification and quantification\nAdvanced Topics - Machine learning and method development",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "R for Mass Spectrometry",
    "section": "Getting Started",
    "text": "Getting Started\nTo follow along with the examples in this book, you’ll need to install R and several specialized packages. Installation instructions and package setup are covered in the first chapter.\n\n# Example of loading key packages\nlibrary(Spectra)\nlibrary(xcms)\nlibrary(tidyverse)\nlibrary(ggplot2)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "R for Mass Spectrometry",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis book builds upon the excellent work of the R for Mass Spectrometry community and the developers of key packages including Spectra, xcms, MSnbase, and many others.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#feedback-and-updates",
    "href": "index.html#feedback-and-updates",
    "title": "R for Mass Spectrometry",
    "section": "Feedback and Updates",
    "text": "Feedback and Updates\nThis book is a living document. Please report errors, suggest improvements, or request additional topics through the book’s repository.\nLet’s begin our journey into the world of mass spectrometry data analysis with R!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction to Mass Spectrometry in R",
    "section": "",
    "text": "Loading Essential Packages\nThis introduction provides hands-on examples of working with mass spectrometry data in R using the R for Mass Spectrometry ecosystem. We’ll explore real datasets and demonstrate key functionalities.\n# Load core MS packages\nlibrary(Spectra)           # Core MS data infrastructure\nlibrary(msdata)            # Example MS datasets\nlibrary(ProtGenerics)      # Generic functions for proteomics\nlibrary(PSMatch)           # Peptide-spectrum matching\nlibrary(tidyverse)         # Data manipulation\nlibrary(MsDataHub)         # Access to online MS resources\n\ncat(\"Packages loaded successfully!\\n\")\n\nPackages loaded successfully!",
    "crumbs": [
      "Introduction to Mass Spectrometry in R"
    ]
  },
  {
    "objectID": "introduction.html#exploring-example-datasets",
    "href": "introduction.html#exploring-example-datasets",
    "title": "Introduction to Mass Spectrometry in R",
    "section": "Exploring Example Datasets",
    "text": "Exploring Example Datasets\nThe msdata package provides various example MS datasets for learning and testing.\n\nProteomics Data\n\n# List available proteomics files\nproteomics_files &lt;- msdata::proteomics(full.names = TRUE)\ncat(\"Available proteomics files:\\n\")\n\nAvailable proteomics files:\n\nfor (i in seq_along(proteomics_files)) {\n  cat(sprintf(\"%d. %s\\n\", i, basename(proteomics_files[i])))\n}\n\n1. MRM-standmix-5.mzML.gz\n2. MS3TMT10_01022016_32917-33481.mzML.gz\n3. MS3TMT11.mzML\n4. TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz\n5. TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01.mzML.gz\n\n# Load a specific proteomics dataset (TMT experiment)\nf &lt;- msdata::proteomics(pattern = \"2014\", full.names = TRUE)\ncat(\"\\nSelected file:\", basename(f), \"\\n\")\n\n\nSelected file: TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz \n\n\n\n\nLoading and Examining MS Data\n\n# Create Spectra object\nms &lt;- Spectra(f, backend = MsBackendMzR())\n\n# Display basic information\ncat(\"\\nDataset summary:\\n\")\n\n\nDataset summary:\n\ncat(\"  Total spectra:\", length(ms), \"\\n\")\n\n  Total spectra: 7534 \n\ncat(\"  MS levels:\", paste(unique(msLevel(ms)), collapse = \", \"), \"\\n\")\n\n  MS levels: 1, 2 \n\ncat(\"  RT range:\", round(range(rtime(ms)), 2), \"seconds\\n\")\n\n  RT range: 0.46 3601.98 seconds\n\ncat(\"  m/z range:\", round(range(unlist(mz(ms))), 2), \"\\n\")\n\n  m/z range: 100 2008.5 \n\n# Display the object\nms\n\nMSn data (Spectra) with 7534 spectra in a MsBackendMzR backend:\n       msLevel     rtime scanIndex\n     &lt;integer&gt; &lt;numeric&gt; &lt;integer&gt;\n1            1    0.4584         1\n2            1    0.9725         2\n3            1    1.8524         3\n4            1    2.7424         4\n5            1    3.6124         5\n...        ...       ...       ...\n7530         2   3600.47      7530\n7531         2   3600.83      7531\n7532         2   3601.18      7532\n7533         2   3601.57      7533\n7534         2   3601.98      7534\n ... 34 more variables/columns.\n\nfile(s):\nTMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz\n\n\n\n\nDifferent Types of MS Data\n\n# SWATH/DIA data example\nswath_file &lt;- PestMix1_SWATH.mzML()\nswath_data &lt;- Spectra(swath_file, backend = MsBackendMzR())\n\ncat(\"\\nSWATH/DIA Dataset:\\n\")\n\n\nSWATH/DIA Dataset:\n\ncat(\"  File:\", basename(swath_file), \"\\n\")\n\n  File: 95c051226acc_7862 \n\ncat(\"  Spectra count:\", length(swath_data), \"\\n\")\n\n  Spectra count: 8999 \n\ncat(\"  MS levels:\", paste(unique(msLevel(swath_data)), collapse = \", \"), \"\\n\")\n\n  MS levels: 2, 1 \n\n\n\n# Metabolomics data examples\nmetab_file1 &lt;- X20171016_POOL_POS_1_105.134.mzML()\nmetab_data1 &lt;- Spectra(metab_file1, backend = MsBackendMzR())\n\ncat(\"\\nMetabolomics Dataset 1:\\n\")\n\n\nMetabolomics Dataset 1:\n\ncat(\"  File:\", basename(metab_file1), \"\\n\")\n\n  File: 95c08883ab0_7859 \n\ncat(\"  Spectra count:\", length(metab_data1), \"\\n\")\n\n  Spectra count: 931 \n\ncat(\"  Polarity:\", unique(polarity(metab_data1)), \"\\n\")\n\n  Polarity: 1 \n\n\n\n# Another metabolomics example\nmetab_file2 &lt;- X20171016_POOL_POS_3_105.134.mzML()\nmetab_data2 &lt;- Spectra(metab_file2, backend = MsBackendMzR())\n\ncat(\"\\nMetabolomics Dataset 2:\\n\")\n\n\nMetabolomics Dataset 2:\n\ncat(\"  File:\", basename(metab_file2), \"\\n\")\n\n  File: 95c05604db2_7860 \n\ncat(\"  Spectra count:\", length(metab_data2), \"\\n\")\n\n  Spectra count: 931 \n\n\n\n# TMT labeled proteomics data\ntmt_file &lt;- TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01.20141210.mzML.gz()\ntmt_data &lt;- Spectra(tmt_file, backend = MsBackendMzR())\n\ncat(\"\\nTMT Proteomics Dataset:\\n\")\n\n\nTMT Proteomics Dataset:\n\ncat(\"  File:\", basename(tmt_file), \"\\n\")\n\n  File: 95c049704e13_7858 \n\ncat(\"  Spectra count:\", length(tmt_data), \"\\n\")\n\n  Spectra count: 7534 \n\ncat(\"  MS levels:\", paste(unique(msLevel(tmt_data)), collapse = \", \"), \"\\n\")\n\n  MS levels: 1, 2",
    "crumbs": [
      "Introduction to Mass Spectrometry in R"
    ]
  },
  {
    "objectID": "introduction.html#peptide-fragment-calculation",
    "href": "introduction.html#peptide-fragment-calculation",
    "title": "Introduction to Mass Spectrometry in R",
    "section": "Peptide Fragment Calculation",
    "text": "Peptide Fragment Calculation\nThe PSMatch package provides tools for theoretical peptide fragmentation.\n\n# Calculate theoretical fragments for a peptide\npeptide_seq &lt;- \"THSQEEMQHMQR\"\n\ncat(\"Calculating theoretical fragments for:\", peptide_seq, \"\\n\\n\")\n\nCalculating theoretical fragments for: THSQEEMQHMQR \n\nfragments &lt;- calculateFragments(peptide_seq)\n\n# Display fragment information\ncat(\"Theoretical fragments:\\n\")\n\nTheoretical fragments:\n\nprint(head(fragments, 15))\n\n          mz ion type pos z         seq      peptide\n1   102.0550  b1    b   1 1           T THSQEEMQHMQR\n2   239.1139  b2    b   2 1          TH THSQEEMQHMQR\n3   326.1459  b3    b   3 1         THS THSQEEMQHMQR\n4   454.2045  b4    b   4 1        THSQ THSQEEMQHMQR\n5   583.2471  b5    b   5 1       THSQE THSQEEMQHMQR\n6   712.2897  b6    b   6 1      THSQEE THSQEEMQHMQR\n7   843.3301  b7    b   7 1     THSQEEM THSQEEMQHMQR\n8   971.3887  b8    b   8 1    THSQEEMQ THSQEEMQHMQR\n9  1108.4476  b9    b   9 1   THSQEEMQH THSQEEMQHMQR\n10 1239.4881 b10    b  10 1  THSQEEMQHM THSQEEMQHMQR\n11 1367.5467 b11    b  11 1 THSQEEMQHMQ THSQEEMQHMQR\n12  175.1190  y1    y   1 1           R THSQEEMQHMQR\n13  303.1775  y2    y   2 1          QR THSQEEMQHMQR\n14  434.2180  y3    y   3 1         MQR THSQEEMQHMQR\n15  571.2769  y4    y   4 1        HMQR THSQEEMQHMQR\n\ncat(\"\\nFragment types:\", paste(unique(fragments$type), collapse = \", \"), \"\\n\")\n\n\nFragment types: b, y, b_, y_, b*, y* \n\ncat(\"Total fragments:\", nrow(fragments), \"\\n\")\n\nTotal fragments: 58",
    "crumbs": [
      "Introduction to Mass Spectrometry in R"
    ]
  },
  {
    "objectID": "introduction.html#peptide-spectrum-matching",
    "href": "introduction.html#peptide-spectrum-matching",
    "title": "Introduction to Mass Spectrometry in R",
    "section": "Peptide-Spectrum Matching",
    "text": "Peptide-Spectrum Matching\nWorking with identification results from database searches.\n\n# Load identification files\nid_files &lt;- msdata::ident(full.names = TRUE)\ncat(\"Available identification files:\\n\")\n\nAvailable identification files:\n\nfor (i in seq_along(id_files)) {\n  cat(sprintf(\"%d. %s\\n\", i, basename(id_files[i])))\n}\n\n1. TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzid\n\n# Load PSM data\nid &lt;- PSM(id_files)\n\ncat(\"\\nPSM Summary:\\n\")\n\n\nPSM Summary:\n\ncat(\"  Total PSMs:\", nrow(id), \"\\n\")\n\n  Total PSMs: 5802 \n\ncat(\"  Unique peptides:\", length(unique(id$sequence)), \"\\n\")\n\n  Unique peptides: 4938 \n\ncat(\"  Unique proteins:\", length(unique(id$DatabaseAccess)), \"\\n\")\n\n  Unique proteins: 3148 \n\n# Display first few PSMs\ncat(\"\\nFirst few PSMs:\\n\")\n\n\nFirst few PSMs:\n\nprint(head(id, 5))\n\nPSM with 5 rows and 35 columns.\nnames(35): sequence spectrumID ... subReplacementResidue subLocation",
    "crumbs": [
      "Introduction to Mass Spectrometry in R"
    ]
  },
  {
    "objectID": "introduction.html#biological-annotation-and-enrichment",
    "href": "introduction.html#biological-annotation-and-enrichment",
    "title": "Introduction to Mass Spectrometry in R",
    "section": "Biological Annotation and Enrichment",
    "text": "Biological Annotation and Enrichment\nConnecting MS data to biological databases for functional analysis.\n\n# Load Gene Ontology database\nlibrary(GO.db)\n\ncat(\"Gene Ontology database loaded\\n\")\n\nGene Ontology database loaded\n\ncat(\"Available columns:\", paste(columns(GO.db), collapse = \", \"), \"\\n\")\n\nAvailable columns: DEFINITION, GOID, ONTOLOGY, TERM \n\n# Query a specific GO term (focal adhesion)\ngo_term &lt;- \"GO:0005925\"\ngo_info &lt;- select(GO.db, \n                  keys = go_term,\n                  columns = columns(GO.db),\n                  keytype = \"GOID\")\n\ncat(\"\\nGO Term Information for\", go_term, \":\\n\")\n\n\nGO Term Information for GO:0005925 :\n\nprint(go_info)\n\n        GOID\n1 GO:0005925\n                                                                                                                                                                                                                  DEFINITION\n1 A cell-substrate junction that anchors the cell to the extracellular matrix and that forms a point of termination of actin filaments. In insects focal adhesion has also been referred to as hemi-adherens junction (HAJ).\n  ONTOLOGY           TERM\n1       CC focal adhesion\n\n\n\n# Get human genes associated with focal adhesion\nlibrary(org.Hs.eg.db)\n\nGO_0005925 &lt;- AnnotationDbi::select(\n  org.Hs.eg.db,\n  keys = \"GO:0005925\",\n  columns = c(\"ENTREZID\", \"SYMBOL\"),\n  keytype = \"GO\"\n) %&gt;%\n  as_tibble() %&gt;%\n  filter(!duplicated(ENTREZID))\n\ncat(\"\\nGenes associated with Focal Adhesion (GO:0005925):\\n\")\n\n\nGenes associated with Focal Adhesion (GO:0005925):\n\ncat(\"  Total genes:\", nrow(GO_0005925), \"\\n\")\n\n  Total genes: 424 \n\nprint(head(GO_0005925, 10))\n\n# A tibble: 10 × 5\n   GO         EVIDENCE ONTOLOGY ENTREZID SYMBOL\n   &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \n 1 GO:0005925 HDA      CC       60       ACTB  \n 2 GO:0005925 HDA      CC       70       ACTC1 \n 3 GO:0005925 ISS      CC       71       ACTG1 \n 4 GO:0005925 HDA      CC       81       ACTN4 \n 5 GO:0005925 HDA      CC       87       ACTN1 \n 6 GO:0005925 IMP      CC       88       ACTN2 \n 7 GO:0005925 IMP      CC       89       ACTN3 \n 8 GO:0005925 HDA      CC       102      ADAM10\n 9 GO:0005925 HDA      CC       118      ADD1  \n10 GO:0005925 HDA      CC       214      ALCAM \n\n\n\n# Get genes for another GO term (centrosome)\nGO_0005813 &lt;- AnnotationDbi::select(\n  org.Hs.eg.db,\n  keys = \"GO:0005813\",\n  columns = c(\"ENTREZID\", \"SYMBOL\"),\n  keytype = \"GO\"\n) %&gt;%\n  as_tibble() %&gt;%\n  filter(!duplicated(ENTREZID))\n\ncat(\"\\nGenes associated with Centrosome (GO:0005813):\\n\")\n\n\nGenes associated with Centrosome (GO:0005813):\n\ncat(\"  Total genes:\", nrow(GO_0005813), \"\\n\")\n\n  Total genes: 633 \n\nprint(head(GO_0005813, 10))\n\n# A tibble: 10 × 5\n   GO         EVIDENCE ONTOLOGY ENTREZID SYMBOL\n   &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \n 1 GO:0005813 IDA      CC       35       ACADS \n 2 GO:0005813 IDA      CC       324      APC   \n 3 GO:0005813 IDA      CC       328      APEX1 \n 4 GO:0005813 IDA      CC       402      ARL2  \n 5 GO:0005813 IDA      CC       403      ARL3  \n 6 GO:0005813 IDA      CC       468      ATF4  \n 7 GO:0005813 ISS      CC       472      ATM   \n 8 GO:0005813 IBA      CC       582      BBS1  \n 9 GO:0005813 IDA      CC       585      BBS4  \n10 GO:0005813 IDA      CC       598      BCL2L1",
    "crumbs": [
      "Introduction to Mass Spectrometry in R"
    ]
  },
  {
    "objectID": "introduction.html#summary",
    "href": "introduction.html#summary",
    "title": "Introduction to Mass Spectrometry in R",
    "section": "Summary",
    "text": "Summary\nThis introduction demonstrated:\n\nLoading and examining various MS datasets (proteomics, metabolomics, DIA/SWATH)\nUsing the Spectra infrastructure with different backends\nCalculating theoretical peptide fragments with PSMatch\nWorking with peptide-spectrum match (PSM) data\nConnecting MS results to biological annotations (GO terms)\n\nThese examples provide a foundation for the detailed analyses covered in subsequent chapters.",
    "crumbs": [
      "Introduction to Mass Spectrometry in R"
    ]
  },
  {
    "objectID": "01-r-fundamentals.html",
    "href": "01-r-fundamentals.html",
    "title": "1  R Fundamentals for Mass Spectrometry",
    "section": "",
    "text": "1.1 R Environment Setup\nThis chapter introduces R programming concepts specifically relevant to mass spectrometry data analysis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "01-r-fundamentals.html#r-environment-setup",
    "href": "01-r-fundamentals.html#r-environment-setup",
    "title": "1  R Fundamentals for Mass Spectrometry",
    "section": "",
    "text": "1.1.1 Installing Required Packages\nThe R for Mass Spectrometry ecosystem is built on Bioconductor, a collection of R packages for biological data analysis.\n\n# Step 1: Install BiocManager if not already installed\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\n# Step 2: Install core R for Mass Spectrometry packages\nBiocManager::install(c(\n  # Core MS data infrastructure\n  \"Spectra\",              # Modern MS data handling\n  \"MsBackendMzR\",        # Reading raw data files\n  \"MsBackendSql\",        # SQL-based backend\n  \"MsBackendHdf5Peaks\",  # HDF5 storage backend\n  \n  # MS data utilities\n  \"msdata\",              # Example MS datasets\n  \"MsDataHub\",           # Access to online MS resources\n  \"ProtGenerics\",        # Generic functions for proteomics\n  \"MsCoreUtils\",         # Core MS utilities\n  \n  # Proteomics packages\n  \"PSMatch\",             # Peptide-spectrum matching\n  \"QFeatures\",           # Quantitative features\n  \n  # Metabolomics packages\n  \"xcms\",                # LC/MS data processing\n  \"CAMERA\",              # Annotation of adducts and isotopes\n  \"MetaboCoreUtils\",     # Metabolomics utilities\n  \"CompoundDb\",          # Compound databases\n  \n  # Statistical analysis\n  \"limma\",               # Linear models for microarray/proteomics\n  \"DEqMS\"                # Differential expression\n))\n\n# Step 3: Install CRAN packages for data manipulation and visualization\ninstall.packages(c(\n  \"tidyverse\",           # Data science ecosystem\n  \"ggplot2\",             # Advanced plotting\n  \"plotly\",              # Interactive plots\n  \"pheatmap\",            # Heatmaps\n  \"corrplot\",            # Correlation plots\n  \"BiocParallel\"         # Parallel processing\n))\n\n\n\n1.1.2 Verifying Installation\n\n# Check if key packages are installed correctly\nrequired_packages &lt;- c(\"Spectra\", \"QFeatures\", \"xcms\", \"tidyverse\")\n\nfor (pkg in required_packages) {\n  if (requireNamespace(pkg, quietly = TRUE)) {\n    cat(\"✓\", pkg, \"is installed\\n\")\n  } else {\n    cat(\"✗\", pkg, \"is NOT installed\\n\")\n  }\n}\n\n\n\n1.1.3 Loading Essential Libraries\n\n# Core MS packages\nlibrary(Spectra)           # MS data structures\nlibrary(msdata)            # Example datasets\nlibrary(ProtGenerics)      # Generic functions\n\n# Data manipulation\nlibrary(tidyverse)         # Includes dplyr, ggplot2, tidyr, etc.\n\n# Check package versions\ncat(\"Spectra version:\", as.character(packageVersion(\"Spectra\")), \"\\n\")\n\nSpectra version: 1.18.2 \n\ncat(\"tidyverse version:\", as.character(packageVersion(\"tidyverse\")), \"\\n\")\n\ntidyverse version: 2.0.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "01-r-fundamentals.html#understanding-r-for-mass-spectrometry-ecosystem",
    "href": "01-r-fundamentals.html#understanding-r-for-mass-spectrometry-ecosystem",
    "title": "1  R Fundamentals for Mass Spectrometry",
    "section": "1.2 Understanding R for Mass Spectrometry Ecosystem",
    "text": "1.2 Understanding R for Mass Spectrometry Ecosystem\n\n1.2.1 Package Architecture\nThe R for Mass Spectrometry initiative provides a modular ecosystem:\n\nCore Infrastructure: Spectra, QFeatures for data structures\nData Access: MsBackendMzR, MsBackendSql for reading files\nProteomics: PSMatch, ProtGenerics for peptide/protein analysis\nMetabolomics: xcms, CAMERA for small molecule analysis\nUtilities: MsCoreUtils, MetaboCoreUtils for common operations\n\n\n# Display available backends for Spectra\navailable_backends &lt;- c(\n  \"MsBackendMzR\" = \"Read mzML/mzXML files\",\n  \"MsBackendDataFrame\" = \"In-memory storage\",\n  \"MsBackendHdf5Peaks\" = \"HDF5-based storage\",\n  \"MsBackendSql\" = \"SQL database storage\"\n)\n\ncat(\"Available Spectra Backends:\\n\")\n\nAvailable Spectra Backends:\n\nfor (backend in names(available_backends)) {\n  cat(\"  •\", backend, \"-\", available_backends[backend], \"\\n\")\n}\n\n  • MsBackendMzR - Read mzML/mzXML files \n  • MsBackendDataFrame - In-memory storage \n  • MsBackendHdf5Peaks - HDF5-based storage \n  • MsBackendSql - SQL database storage",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "01-r-fundamentals.html#data-structures-in-r-for-ms",
    "href": "01-r-fundamentals.html#data-structures-in-r-for-ms",
    "title": "1  R Fundamentals for Mass Spectrometry",
    "section": "1.3 Data Structures in R for MS",
    "text": "1.3 Data Structures in R for MS\n\n1.3.1 Vectors and Matrices\nMass spectra are fundamentally collections of m/z and intensity pairs, which map naturally to R’s vector and matrix structures.\n\n# Example: Creating a simple spectrum\nmz_values &lt;- c(100.1, 200.2, 300.3, 400.4)\nintensity_values &lt;- c(1000, 2500, 800, 1200)\n\n# Create a simple spectrum data frame\nspectrum_df &lt;- data.frame(\n  mz = mz_values,\n  intensity = intensity_values\n)\n\nprint(spectrum_df)\n\n     mz intensity\n1 100.1      1000\n2 200.2      2500\n3 300.3       800\n4 400.4      1200\n\n\n\n\n1.3.2 Lists for Complex Data\nMS experiments often contain metadata alongside spectral data.\n\n# Example: MS run metadata\nms_run &lt;- list(\n  instrument = \"Orbitrap Fusion\",\n  ionization = \"ESI\",\n  polarity = \"positive\",\n  acquisition_date = Sys.Date(),\n  spectra_count = 1000\n)\n\nprint(ms_run)\n\n$instrument\n[1] \"Orbitrap Fusion\"\n\n$ionization\n[1] \"ESI\"\n\n$polarity\n[1] \"positive\"\n\n$acquisition_date\n[1] \"2025-11-12\"\n\n$spectra_count\n[1] 1000",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "01-r-fundamentals.html#data-importexport-basics",
    "href": "01-r-fundamentals.html#data-importexport-basics",
    "title": "1  R Fundamentals for Mass Spectrometry",
    "section": "1.4 Data Import/Export Basics",
    "text": "1.4 Data Import/Export Basics\n\n1.4.1 Common File Formats in Mass Spectrometry\n\n\n\n\n\n\n\n\n\nFormat\nType\nDescription\nUse Case\n\n\n\n\nmzML\nXML\nVendor-neutral standard format\nRaw MS data storage\n\n\nmzXML\nXML\nOlder standard format\nLegacy data\n\n\nMGF\nText\nMascot Generic Format\nMS/MS for database search\n\n\nCDF\nBinary\nNetCDF format\nGC-MS data\n\n\nmzTab\nText\nTab-delimited results\nAnalysis results\n\n\n\n\n# Example: Reading mzML files using Spectra\nlibrary(msdata)\n\n# Get path to example file\nms_file &lt;- msdata::proteomics(full.names = TRUE)[1]\ncat(\"Example file:\", basename(ms_file), \"\\n\")\n\nExample file: MRM-standmix-5.mzML.gz \n\n# Load the data with error handling for mzR compatibility issues\ntryCatch({\n  ms_data &lt;- Spectra(ms_file, backend = MsBackendMzR())\n  # Convert to in-memory for better compatibility\n  ms_data &lt;- setBackend(ms_data, backend = MsBackendDataFrame())\n  cat(\"\\nSuccessfully loaded real MS data\\n\")\n}, error = function(e) {\n  cat(\"\\nNote: mzR compatibility issue detected, using synthetic data\\n\")\n  cat(\"Error:\", conditionMessage(e), \"\\n\\n\")\n  \n  # Create synthetic data as fallback\n  set.seed(42)\n  n_spectra &lt;- 50\n  \n  library(S4Vectors)\n  library(IRanges)\n  \n  mz_list &lt;- lapply(1:n_spectra, function(i) {\n    sort(runif(sample(50:150, 1), 100, 2000))\n  })\n  \n  intensity_list &lt;- lapply(mz_list, function(mz_vals) {\n    rlnorm(length(mz_vals), meanlog = 8, sdlog = 2)\n  })\n  \n  # Generate MS levels first\n  ms_levels &lt;- sample(1:2, n_spectra, replace = TRUE, prob = c(0.3, 0.7))\n  \n  spd &lt;- DataFrame(\n    msLevel = ms_levels,\n    rtime = seq(100, 3000, length.out = n_spectra),\n    precursorMz = ifelse(ms_levels == 1, NA_real_, runif(n_spectra, 400, 1500)),\n    polarity = rep(1L, n_spectra)\n  )\n  \n  spd$mz &lt;- NumericList(mz_list)\n  spd$intensity &lt;- NumericList(intensity_list)\n  \n  backend &lt;- MsBackendDataFrame()\n  backend &lt;- backendInitialize(backend, spd)\n  ms_data &lt;&lt;- Spectra(backend)\n})\n\n\nNote: mzR compatibility issue detected, using synthetic data\nError: BiocParallel errors\n  1 remote errors, element index: 1\n  0 unevaluated and other errors\n  first remote error:\nError in DataFrame(..., check.names = FALSE): different row counts implied by arguments\n \n\n# Display basic information\ncat(\"\\nDataset summary:\\n\")\n\n\nDataset summary:\n\ncat(\"  Total spectra:\", length(ms_data), \"\\n\")\n\n  Total spectra: 50 \n\ncat(\"  MS levels:\", paste(unique(msLevel(ms_data)), collapse = \", \"), \"\\n\")\n\n  MS levels: 2, 1 \n\ncat(\"  RT range:\", round(range(rtime(ms_data)), 2), \"seconds\\n\")\n\n  RT range: 100 3000 seconds\n\n\n\n\n1.4.2 Working with Spectral Data\n\n# Access spectral data\nfirst_spectrum &lt;- ms_data[1]\n\n# Extract m/z and intensity values\npeaks &lt;- peaksData(first_spectrum)[[1]]\ncat(\"First spectrum has\", nrow(peaks), \"peaks\\n\")\n\nFirst spectrum has 98 peaks\n\ncat(\"m/z range:\", round(range(peaks[, 1]), 2), \"\\n\")\n\nm/z range: 100.45 1978.89 \n\ncat(\"Intensity range:\", round(range(peaks[, 2]), 2), \"\\n\")\n\nIntensity range: 7.07 436079.1 \n\n\n\n\n1.4.3 Exporting Data\n\n# Export spectra to different formats\n\n# Export to mzML\nexport(ms_data[1:10], file = \"subset.mzML\")\n\n# Export to MGF (for MS2 spectra)\nms2_data &lt;- filterMsLevel(ms_data, 2)\nexport(ms2_data, file = \"ms2_spectra.mgf\")\n\n# Export metadata to CSV\nmetadata &lt;- spectraData(ms_data) %&gt;%\n  as.data.frame() %&gt;%\n  select(msLevel, rtime, precursorMz, precursorCharge)\n\nwrite.csv(metadata, \"spectra_metadata.csv\", row.names = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "01-r-fundamentals.html#exercises",
    "href": "01-r-fundamentals.html#exercises",
    "title": "1  R Fundamentals for Mass Spectrometry",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\nPackage Installation: Install the core R for Mass Spectrometry packages and verify the installation\nData Structures: Create vectors representing m/z and intensity values for a hypothetical spectrum with at least 10 peaks\nData Frames: Build a data frame combining multiple spectra with metadata (RT, precursor m/z, charge)\nFile I/O: Practice loading MS data from the msdata package and explore different file formats\nBackend Comparison: Load the same file using different backends and compare memory usage",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "01-r-fundamentals.html#summary",
    "href": "01-r-fundamentals.html#summary",
    "title": "1  R Fundamentals for Mass Spectrometry",
    "section": "1.6 Summary",
    "text": "1.6 Summary\nThis chapter covered the fundamental R concepts needed for MS data analysis:\n\nPackage ecosystem: Core Bioconductor packages for MS analysis (Spectra, QFeatures, xcms)\nData structures: Vectors, matrices, data frames, and lists for MS data\nR for MS architecture: Understanding backends and modular design\nFile formats: Common MS formats (mzML, MGF) and how to read/write them\nBasic operations: Loading MS data and accessing spectral information\n\nWith these fundamentals in place, you’re ready to proceed to more advanced MS data processing and analysis workflows in the following chapters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Fundamentals for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "02-data-formats-import.html",
    "href": "02-data-formats-import.html",
    "title": "2  Mass Spectrometry Data Formats and Import",
    "section": "",
    "text": "2.1 Overview of MS Data Formats\nUnderstanding and working with various MS data formats is crucial for effective data analysis. This chapter covers the main data structures and file formats used in R for Mass Spectrometry.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "02-data-formats-import.html#overview-of-ms-data-formats",
    "href": "02-data-formats-import.html#overview-of-ms-data-formats",
    "title": "2  Mass Spectrometry Data Formats and Import",
    "section": "",
    "text": "2.1.1 mzML Format\nThe mzML format is the current standard for MS data exchange, defined by the Proteomics Standards Initiative (PSI).\n\nlibrary(tidyverse)\nlibrary(Spectra)\nlibrary(msdata)\nlibrary(mzR)\nlibrary(ProtGenerics)\n\n# Load example mzML file\nmzml_file &lt;- proteomics(full.names = TRUE, pattern = \"TMT\")\nbasename(mzml_file)\n\n[1] \"MS3TMT10_01022016_32917-33481.mzML.gz\"                                 \n[2] \"MS3TMT11.mzML\"                                                         \n[3] \"TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz\"\n[4] \"TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01.mzML.gz\"         \n\n\n\n# Read mzML data using Spectra\nms_data &lt;- Spectra(mzml_file)\nms_data\n\nMSn data (Spectra) with 9602 spectra in a MsBackendMzR backend:\n       msLevel     rtime scanIndex\n     &lt;integer&gt; &lt;numeric&gt; &lt;integer&gt;\n1            1   4422.62         1\n2            2   4422.65         2\n3            2   4422.67         3\n4            2   4422.74         4\n5            2   4422.80         5\n...        ...       ...       ...\n9598         2   1321.60       505\n9599         2   1321.89       506\n9600         2   1322.18       507\n9601         2   1322.47       508\n9602         1   1322.95       509\n ... 34 more variables/columns.\n\nfile(s):\nMS3TMT10_01022016_32917-33481.mzML.gz\nMS3TMT11.mzML\nTMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz\n ... 1 more files\n\n\n\n\n2.1.2 Understanding Spectra Objects\nThe Spectra object is the core data structure in R for Mass Spectrometry, providing an abstraction for MS data that allows efficient manipulation and analysis.\n\n# Basic information about the data\ncat(\"Number of spectra:\", length(ms_data), \"\\n\")\n\nNumber of spectra: 9602 \n\ncat(\"MS levels present:\", paste(unique(msLevel(ms_data)), collapse = \", \"), \"\\n\")\n\nMS levels present: 1, 2, 3 \n\ncat(\"Retention time range:\", range(rtime(ms_data)), \"seconds\\n\")\n\nRetention time range: 0.4584 4494.339 seconds\n\n\n\n# MS level distribution\ntable(msLevel(ms_data))\n\n\n   1    2    3 \n1544 7306  752 \n\n\n\n\n2.1.3 Core Spectra Variables\nThe Spectra class defines core variables that characterize each spectrum:\n\n# Core spectra variables\nspectraVariables(ms_data) %&gt;% head(15)\n\n [1] \"msLevel\"                \"rtime\"                  \"acquisitionNum\"        \n [4] \"scanIndex\"              \"dataStorage\"            \"dataOrigin\"            \n [7] \"centroided\"             \"smoothed\"               \"polarity\"              \n[10] \"precScanNum\"            \"precursorMz\"            \"precursorIntensity\"    \n[13] \"precursorCharge\"        \"collisionEnergy\"        \"isolationWindowLowerMz\"\n\n\n\n# Access specific variables\n# Retention times for first 10 spectra\nrtime(ms_data)[1:10]\n\n [1] 4422.620 4422.648 4422.670 4422.735 4422.800 4423.036 4423.053 4423.094\n [9] 4423.367 4423.384\n\n\n\n# Precursor information for MS2 spectra\nms2_data &lt;- filterMsLevel(ms_data, msLevel = 2L)\nif (length(ms2_data) &gt; 0) {\n  precursor_info &lt;- data.frame(\n    scan = acquisitionNum(ms2_data)[1:10],\n    precursor_mz = precursorMz(ms2_data)[1:10],\n    precursor_charge = precursorCharge(ms2_data)[1:10],\n    retention_time = rtime(ms2_data)[1:10]\n  )\n  print(precursor_info)\n}\n\n    scan precursor_mz precursor_charge retention_time\n1  32919     652.3426                3       4422.648\n2  32920     647.3441                3       4422.670\n3  32921     673.3353                3       4422.735\n4  32922     575.0026                3       4422.800\n5  32924     820.9681                2       4423.053\n6  32925     675.6938                3       4423.094\n7  32927     608.2888                2       4423.384\n8  32928     685.7155                3       4423.414\n9  32930     659.0151                3       4423.699\n10 32931     438.7607                2       4423.748\n\n\n\n\n2.1.4 mzXML Format\nOlder but still commonly used XML format.\n\n# Example with mzXML (if available)\n# Similar syntax to mzML",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "02-data-formats-import.html#working-with-spectra-objects",
    "href": "02-data-formats-import.html#working-with-spectra-objects",
    "title": "2  Mass Spectrometry Data Formats and Import",
    "section": "2.2 Working with Spectra Objects",
    "text": "2.2 Working with Spectra Objects\n\n2.2.1 Accessing Spectral Data\n\n# Get the first spectrum\nfirst_spectrum &lt;- ms_data[1]\nmz(first_spectrum)[[1]] %&gt;% head(10)\n\n [1] 376.2217 376.2227 376.2237 376.2247 380.7361 380.7371 380.7381 380.7391\n [9] 380.7401 380.7411\n\nintensity(first_spectrum)[[1]] %&gt;% head(10)\n\n [1]     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00\n [9] 21175.68 57675.16\n\n\n\n\n2.2.2 Filtering Spectra\n\n# Filter by MS level\nms1_spectra &lt;- filterMsLevel(ms_data, msLevel = 1)\nms2_spectra &lt;- filterMsLevel(ms_data, msLevel = 2)\n\ncat(\"MS1 spectra:\", length(ms1_spectra), \"\\n\")\n\nMS1 spectra: 1544 \n\ncat(\"MS2 spectra:\", length(ms2_spectra), \"\\n\")\n\nMS2 spectra: 7306 \n\n\n\n# Filter by retention time\nrt_filtered &lt;- filterRt(ms_data, rt = c(1000, 2000))\nlength(rt_filtered)\n\n[1] 3011\n\n\n\n\n2.2.3 Metadata Extraction\n\n# Extract comprehensive metadata\nspectra_df &lt;- spectraData(ms_data) %&gt;%\n  as.data.frame() %&gt;%\n  select(msLevel, rtime, precursorMz, precursorCharge, \n         precursorIntensity) %&gt;%\n  head(10)\n\nprint(spectra_df)\n\n   msLevel    rtime precursorMz precursorCharge precursorIntensity\n1        1 4422.620          NA              NA                 NA\n2        2 4422.648    652.3426               3            9841531\n3        2 4422.670    647.3441               3            3921567\n4        2 4422.735    673.3353               3            7623700\n5        2 4422.800    575.0026               3            2357085\n6        3 4423.036    490.2629               0                  0\n7        2 4423.053    820.9681               2            1847945\n8        2 4423.094    675.6938               3            4257284\n9        3 4423.367    756.4332               1                  0\n10       2 4423.384    608.2888               2            5362638",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "02-data-formats-import.html#other-important-formats",
    "href": "02-data-formats-import.html#other-important-formats",
    "title": "2  Mass Spectrometry Data Formats and Import",
    "section": "2.3 Other Important Formats",
    "text": "2.3 Other Important Formats\n\n2.3.1 MGF (Mascot Generic Format)\n\n# Reading MGF files\nmgf_file &lt;- \"path/to/file.mgf\"\nmgf_data &lt;- Spectra(mgf_file)\n\n\n\n2.3.2 Text-based Formats\n\n# Converting to data frames for analysis\nms1_df &lt;- ms1_spectra %&gt;%\n  spectraData() %&gt;%\n  as.data.frame() %&gt;%\n  select(rtime, precursorMz, precursorIntensity)\n\nhead(ms1_df)\n\n     rtime precursorMz precursorIntensity\n1 4422.620          NA                 NA\n2 4425.460          NA                 NA\n3 4428.595          NA                 NA\n4 4431.419          NA                 NA\n5 4434.330          NA                 NA\n6 4437.434          NA                 NA",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "02-data-formats-import.html#data-export",
    "href": "02-data-formats-import.html#data-export",
    "title": "2  Mass Spectrometry Data Formats and Import",
    "section": "2.4 Data Export",
    "text": "2.4 Data Export\n\n2.4.1 Exporting to Different Formats\n\n# Export to mzML\nexport(ms_data, MzMLFile(\"output.mzML\"))\n\n# Export to MGF\nexport(ms2_spectra, MgfFile(\"ms2_spectra.mgf\"))\n\n\n\n2.4.2 Exporting Processed Data\n\n# Export metadata to CSV\nwrite.csv(spectra_df, \"spectra_metadata.csv\", row.names = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "02-data-formats-import.html#quality-control-and-data-inspection",
    "href": "02-data-formats-import.html#quality-control-and-data-inspection",
    "title": "2  Mass Spectrometry Data Formats and Import",
    "section": "2.5 Quality Control and Data Inspection",
    "text": "2.5 Quality Control and Data Inspection\n\n2.5.1 Basic QC Checks\n\n# Check for missing data\nsummary(is.na(precursorMz(ms_data)))\n\n   Mode   FALSE    TRUE \nlogical    8058    1544 \n\nsummary(is.na(rtime(ms_data)))\n\n   Mode   FALSE \nlogical    9602 \n\n\n\n\n2.5.2 Visualizing Data Overview\n\nlibrary(ggplot2)\n\n# Retention time distribution\nrt_data &lt;- data.frame(rtime = rtime(ms_data), msLevel = msLevel(ms_data))\n\nggplot(rt_data, aes(x = rtime, fill = factor(msLevel))) +\n  geom_histogram(bins = 50, alpha = 0.7) +\n  facet_wrap(~msLevel) +\n  labs(title = \"Retention Time Distribution by MS Level\",\n       x = \"Retention Time (s)\", y = \"Count\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "02-data-formats-import.html#practical-examples",
    "href": "02-data-formats-import.html#practical-examples",
    "title": "2  Mass Spectrometry Data Formats and Import",
    "section": "2.6 Practical Examples",
    "text": "2.6 Practical Examples\n\n2.6.1 Loading Multiple Files\n\n# Load multiple mzML files\nfile_list &lt;- list.files(\"data/\", pattern = \"*.mzML\", full.names = TRUE)\nall_data &lt;- Spectra(file_list)\n\n\n\n2.6.2 Combining Datasets\n\n# Combine multiple Spectra objects\ncombined_data &lt;- c(ms_data1, ms_data2, ms_data3)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "02-data-formats-import.html#exercises",
    "href": "02-data-formats-import.html#exercises",
    "title": "2  Mass Spectrometry Data Formats and Import",
    "section": "2.7 Exercises",
    "text": "2.7 Exercises\n\nLoad an mzML file and examine its basic properties\nFilter spectra by different criteria (RT, MS level, m/z range)\nExport filtered data to different formats\nCreate quality control plots for your data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "02-data-formats-import.html#summary",
    "href": "02-data-formats-import.html#summary",
    "title": "2  Mass Spectrometry Data Formats and Import",
    "section": "2.8 Summary",
    "text": "2.8 Summary\nThis chapter covered the major MS data formats and how to import, manipulate, and export them using R. Understanding these fundamentals is essential for all downstream analysis tasks.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "03-spectral-preprocessing.html",
    "href": "03-spectral-preprocessing.html",
    "title": "3  Spectral Data Preprocessing",
    "section": "",
    "text": "3.1 Loading Required Libraries\nPreprocessing is a critical step in MS data analysis that improves data quality and enables accurate downstream analysis. This chapter covers baseline correction, smoothing, normalization, and peak picking using modern R for Mass Spectrometry tools.\nlibrary(Spectra)           # Core MS data handling\nlibrary(MsCoreUtils)       # MS processing utilities\nlibrary(msdata)            # Example datasets\nlibrary(ProtGenerics)      # Generic functions\nlibrary(ggplot2)           # Visualization\nlibrary(dplyr)             # Data manipulation\nlibrary(patchwork)         # Plot composition",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "03-spectral-preprocessing.html#understanding-raw-spectral-data",
    "href": "03-spectral-preprocessing.html#understanding-raw-spectral-data",
    "title": "3  Spectral Data Preprocessing",
    "section": "3.2 Understanding Raw Spectral Data",
    "text": "3.2 Understanding Raw Spectral Data\n\n3.2.1 Loading and Inspecting Spectra\n\n# Load example proteomics data\n# Note: Using setBackend to convert to in-memory storage to avoid mzR issues\nms_file &lt;- msdata::proteomics(full.names = TRUE)[1]\n\n# First load with MsBackendMzR, then convert to DataFrame backend\ntryCatch({\n  ms_data &lt;- Spectra(ms_file, backend = MsBackendMzR())\n  # Convert to in-memory backend for better compatibility\n  ms_data &lt;- setBackend(ms_data, backend = MsBackendDataFrame())\n  cat(\"Successfully loaded real MS data\\n\")\n}, error = function(e) {\n  # If mzR fails, create synthetic data for demonstration\n  cat(\"Note: Using synthetic data due to mzR compatibility issues\\n\")\n  cat(\"Error was:\", conditionMessage(e), \"\\n\\n\")\n  \n  # Create synthetic MS2 spectra using proper format\n  set.seed(123)\n  n_spectra &lt;- 100\n  \n  # Create peaks data - separate m/z and intensity lists\n  mz_list &lt;- lapply(1:n_spectra, function(i) {\n    n_peaks &lt;- sample(50:200, 1)\n    sort(runif(n_peaks, 100, 2000))  # Already sorted\n  })\n  \n  intensity_list &lt;- lapply(mz_list, function(mz_vals) {\n    rlnorm(length(mz_vals), meanlog = 8, sdlog = 2)\n  })\n  \n  # Create spectra data frame with metadata\n  library(S4Vectors)\n  library(IRanges)\n  spd &lt;- DataFrame(\n    msLevel = rep(2L, n_spectra),\n    rtime = seq(100, 6000, length.out = n_spectra),\n    precursorMz = runif(n_spectra, 400, 1500),\n    precursorCharge = sample(2:3, n_spectra, replace = TRUE),\n    polarity = rep(1L, n_spectra)\n  )\n  \n  # Add list columns using NumericList\n  spd$mz &lt;- NumericList(mz_list)\n  spd$intensity &lt;- NumericList(intensity_list)\n  \n  # Create Spectra object from DataFrame backend\n  backend &lt;- MsBackendDataFrame()\n  backend &lt;- backendInitialize(backend, spd)\n  ms_data &lt;&lt;- Spectra(backend)\n})\n\nNote: Using synthetic data due to mzR compatibility issues\nError was: BiocParallel errors\n  1 remote errors, element index: 1\n  0 unevaluated and other errors\n  first remote error:\nError in DataFrame(..., check.names = FALSE): different row counts implied by arguments\n \n\n# Focus on MS2 spectra for preprocessing examples\nms2_data &lt;- filterMsLevel(ms_data, 2)\ncat(\"Total MS2 spectra:\", length(ms2_data), \"\\n\")\n\nTotal MS2 spectra: 100 \n\n# Select a representative spectrum\nspectrum &lt;- ms2_data[10]\n\n# Extract peak data\npeaks &lt;- peaksData(spectrum)[[1]]\nmz_vals &lt;- peaks[, 1]\nint_vals &lt;- peaks[, 2]\n\ncat(\"Spectrum contains\", length(mz_vals), \"peaks\\n\")\n\nSpectrum contains 109 peaks\n\ncat(\"m/z range:\", round(range(mz_vals), 2), \"\\n\")\n\nm/z range: 124.91 1985.97 \n\ncat(\"Intensity range:\", sprintf(\"%.2e - %.2e\", min(int_vals), max(int_vals)), \"\\n\")\n\nIntensity range: 1.37e+01 - 9.02e+05 \n\n\n\n\n3.2.2 Visualizing Raw Data\n\n# Create a visualization of raw spectrum\nraw_df &lt;- data.frame(mz = mz_vals, intensity = int_vals)\n\np_raw &lt;- ggplot(raw_df, aes(x = mz, y = intensity)) +\n  geom_segment(aes(xend = mz, yend = 0), color = \"steelblue\", alpha = 0.6) +\n  labs(title = \"Raw MS2 Spectrum\",\n       subtitle = paste(\"Precursor:\", round(precursorMz(spectrum), 3), \"m/z\"),\n       x = \"m/z\", y = \"Intensity\") +\n  theme_minimal()\n\nprint(p_raw)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "03-spectral-preprocessing.html#baseline-correction",
    "href": "03-spectral-preprocessing.html#baseline-correction",
    "title": "3  Spectral Data Preprocessing",
    "section": "3.3 Baseline Correction",
    "text": "3.3 Baseline Correction\n\n3.3.1 Understanding Baseline Issues\n\n# Use the already loaded ms_data from above to avoid reloading\n# Get a representative spectrum\nspectrum &lt;- ms_data[min(100, length(ms_data))]\npeaks &lt;- peaksData(spectrum)[[1]]\nmz_vals &lt;- peaks[, 1]\nint_vals &lt;- peaks[, 2]\n\n# Plot raw spectrum\nplot(mz_vals, int_vals, type = \"l\", \n     main = \"Raw Spectrum\", \n     xlab = \"m/z\", ylab = \"Intensity\")\n\n\n\n\n\n\n\n\n\n\n3.3.2 Baseline Removal Methods\n\n# Simple baseline correction using quantile-based approach\nbaseline_estimate &lt;- quantile(int_vals, 0.05)  # 5th percentile\ncorrected_intensity &lt;- pmax(int_vals - baseline_estimate, 0)\n\n# Plot corrected spectrum\nplot(mz_vals, corrected_intensity, type = \"l\",\n     main = \"Baseline Corrected Spectrum\",\n     xlab = \"m/z\", ylab = \"Intensity\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "03-spectral-preprocessing.html#smoothing-techniques",
    "href": "03-spectral-preprocessing.html#smoothing-techniques",
    "title": "3  Spectral Data Preprocessing",
    "section": "3.4 Smoothing Techniques",
    "text": "3.4 Smoothing Techniques\nSmoothing reduces noise while preserving spectral features. The Spectra package provides built-in smoothing methods.\n\n3.4.1 Savitzky-Golay Smoothing\n\n# Apply Savitzky-Golay smoothing using Spectra\n# halfWindowSize must be an integer\nsmoothed_spectrum &lt;- smooth(spectrum, method = \"SavitzkyGolay\", halfWindowSize = 2L)\n\n# Extract smoothed data\nsmoothed_peaks &lt;- peaksData(smoothed_spectrum)[[1]]\n\n# Compare original and smoothed\ncomparison_df &lt;- data.frame(\n  mz = c(mz_vals, smoothed_peaks[, 1]),\n  intensity = c(int_vals, smoothed_peaks[, 2]),\n  type = rep(c(\"Original\", \"Smoothed\"), c(length(mz_vals), nrow(smoothed_peaks)))\n)\n\np_smooth &lt;- ggplot(comparison_df, aes(x = mz, y = intensity, color = type)) +\n  geom_line(alpha = 0.7) +\n  scale_color_manual(values = c(\"Original\" = \"gray60\", \"Smoothed\" = \"red\")) +\n  labs(title = \"Savitzky-Golay Smoothing\",\n       x = \"m/z\", y = \"Intensity\", color = \"Type\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nprint(p_smooth)\n\n\n\n\n\n\n\n\n\n\n3.4.2 Moving Average Smoothing\n\n# Custom moving average implementation\nmoving_average &lt;- function(x, window = 5) {\n  n &lt;- length(x)\n  smoothed &lt;- numeric(n)\n  half_window &lt;- floor(window / 2)\n  \n  for (i in 1:n) {\n    start_idx &lt;- max(1, i - half_window)\n    end_idx &lt;- min(n, i + half_window)\n    smoothed[i] &lt;- mean(x[start_idx:end_idx])\n  }\n  return(smoothed)\n}\n\n# Apply moving average\nma_intensity &lt;- moving_average(int_vals, window = 5)\n\n# Visualize comparison\nma_df &lt;- data.frame(\n  mz = mz_vals,\n  original = int_vals,\n  moving_avg = ma_intensity\n)\n\nggplot(ma_df) +\n  geom_line(aes(x = mz, y = original), color = \"gray60\", alpha = 0.7) +\n  geom_line(aes(x = mz, y = moving_avg), color = \"blue\", size = 1) +\n  labs(title = \"Moving Average Smoothing (window = 5)\",\n       x = \"m/z\", y = \"Intensity\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n3.4.3 Gaussian Smoothing\n\n# Gaussian smoothing implementation\ngaussian_smooth &lt;- function(x, sigma = 1) {\n  n &lt;- length(x)\n  kernel_size &lt;- ceiling(3 * sigma)\n  kernel &lt;- exp(-((-kernel_size):kernel_size)^2 / (2 * sigma^2))\n  kernel &lt;- kernel / sum(kernel)\n  \n  # Apply convolution (simplified)\n  smoothed &lt;- stats::filter(x, kernel, sides = 2)\n  smoothed[is.na(smoothed)] &lt;- x[is.na(smoothed)]  # Handle edges\n  return(as.numeric(smoothed))\n}\n\n# Apply Gaussian smoothing to the original intensity values\ngaussian_smoothed &lt;- gaussian_smooth(int_vals, sigma = 2)\n\nplot(mz_vals, gaussian_smoothed, type = \"l\", col = \"blue\",\n     main = \"Gaussian Smoothed Spectrum\", xlab = \"m/z\", ylab = \"Intensity\")\nlines(mz_vals, int_vals, col = \"gray\", lty = 2)\nlegend(\"topright\", c(\"Gaussian Smoothed\", \"Original\"), \n       col = c(\"blue\", \"gray\"), lty = c(1, 2))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "03-spectral-preprocessing.html#peak-detection",
    "href": "03-spectral-preprocessing.html#peak-detection",
    "title": "3  Spectral Data Preprocessing",
    "section": "3.5 Peak Detection",
    "text": "3.5 Peak Detection\n\n3.5.1 Simple Peak Picking Algorithm\n\n# Use the moving average smoothed data for peak detection\nsmoothed_intensity &lt;- ma_intensity\n\n# Simple peak detection function\ndetect_peaks &lt;- function(mz, intensity, min_intensity = 1000, min_distance = 0.1) {\n  n &lt;- length(intensity)\n  peaks &lt;- logical(n)\n  \n  for (i in 2:(n-1)) {\n    if (intensity[i] &gt; intensity[i-1] && \n        intensity[i] &gt; intensity[i+1] && \n        intensity[i] &gt; min_intensity) {\n      peaks[i] &lt;- TRUE\n    }\n  }\n  \n  # Filter by minimum distance\n  peak_indices &lt;- which(peaks)\n  if (length(peak_indices) &gt; 1) {\n    keep &lt;- logical(length(peak_indices))\n    keep[1] &lt;- TRUE\n    \n    for (i in 2:length(peak_indices)) {\n      if (mz[peak_indices[i]] - mz[peak_indices[keep][sum(keep)]] &gt; min_distance) {\n        keep[i] &lt;- TRUE\n      }\n    }\n    peak_indices &lt;- peak_indices[keep]\n  }\n  \n  return(list(\n    mz = mz[peak_indices],\n    intensity = intensity[peak_indices],\n    indices = peak_indices\n  ))\n}\n\n# Detect peaks\npeaks &lt;- detect_peaks(mz_vals, smoothed_intensity, min_intensity = 5000)\n\n# Plot spectrum with detected peaks\nplot(mz_vals, smoothed_intensity, type = \"l\",\n     main = \"Peak Detection Results\", xlab = \"m/z\", ylab = \"Intensity\")\npoints(peaks$mz, peaks$intensity, col = \"red\", pch = 19)\n\n\n\n\n\n\n\n\n\n\n3.5.2 Peak Statistics\n\n# Analyze detected peaks\ncat(\"Number of peaks detected:\", length(peaks$mz), \"\\n\")\n\nNumber of peaks detected: 40 \n\ncat(\"Peak m/z range:\", range(peaks$mz), \"\\n\")\n\nPeak m/z range: 170.7168 1915.761 \n\ncat(\"Peak intensity range:\", range(peaks$intensity), \"\\n\")\n\nPeak intensity range: 5006.15 212604.2 \n\n# Create peak list data frame\npeak_list &lt;- data.frame(\n  mz = peaks$mz,\n  intensity = peaks$intensity,\n  relative_intensity = peaks$intensity / max(peaks$intensity) * 100\n)\n\nhead(peak_list)\n\n        mz intensity relative_intensity\n1 170.7168  35103.09           16.51100\n2 208.6494  34931.65           16.43037\n3 266.4594 122379.71           57.56223\n4 309.7539  45276.72           21.29625\n5 374.9435  35739.82           16.81049\n6 400.7718  28922.93           13.60412",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "03-spectral-preprocessing.html#normalization",
    "href": "03-spectral-preprocessing.html#normalization",
    "title": "3  Spectral Data Preprocessing",
    "section": "3.6 Normalization",
    "text": "3.6 Normalization\n\n3.6.1 Total Ion Current (TIC) Normalization\n\n# TIC normalization\ntic_normalize &lt;- function(intensity) {\n  tic &lt;- sum(intensity)\n  return(intensity / tic * 1e6)  # Scale to parts per million\n}\n\nnormalized_intensity &lt;- tic_normalize(smoothed_intensity)\n\n# Compare before and after normalization\npar(mfrow = c(2, 1))\nplot(mz_vals, smoothed_intensity, type = \"l\",\n     main = \"Before TIC Normalization\", xlab = \"m/z\", ylab = \"Intensity\")\nplot(mz_vals, normalized_intensity, type = \"l\",\n     main = \"After TIC Normalization\", xlab = \"m/z\", ylab = \"Normalized Intensity\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n3.6.2 Base Peak Normalization\n\n# Base peak normalization\nbase_peak_normalize &lt;- function(intensity) {\n  base_peak &lt;- max(intensity)\n  return(intensity / base_peak * 100)\n}\n\nbp_normalized &lt;- base_peak_normalize(smoothed_intensity)\n\nplot(mz_vals, bp_normalized, type = \"l\",\n     main = \"Base Peak Normalized Spectrum\", \n     xlab = \"m/z\", ylab = \"Relative Intensity (%)\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "03-spectral-preprocessing.html#processing-multiple-spectra",
    "href": "03-spectral-preprocessing.html#processing-multiple-spectra",
    "title": "3  Spectral Data Preprocessing",
    "section": "3.7 Processing Multiple Spectra",
    "text": "3.7 Processing Multiple Spectra\n\n3.7.1 Batch Processing Function\n\nprocess_spectrum &lt;- function(spec, baseline_quantile = 0.05, \n                            smooth_window = 5, min_peak_intensity = 1000) {\n  mz_vals &lt;- mz(spec)[[1]]\n  int_vals &lt;- intensity(spec)[[1]]\n  \n  # Baseline correction\n  baseline &lt;- quantile(int_vals, baseline_quantile)\n  int_vals &lt;- pmax(int_vals - baseline, 0)\n  \n  # Smoothing\n  int_vals &lt;- moving_average(int_vals, smooth_window)\n  \n  # Normalization\n  int_vals &lt;- base_peak_normalize(int_vals)\n  \n  # Peak detection\n  peaks &lt;- detect_peaks(mz_vals, int_vals, min_peak_intensity)\n  \n  return(list(\n    mz = mz_vals,\n    intensity = int_vals,\n    peaks = peaks\n  ))\n}\n\n# Process first 10 spectra\nprocessed_results &lt;- list()\nfor (i in 1:min(10, length(ms_data))) {\n  processed_results[[i]] &lt;- process_spectrum(ms_data[i])\n}\n\n\n\n3.7.2 Quality Assessment\n\n# Assess processing quality\npeak_counts &lt;- sapply(processed_results, function(x) length(x$peaks$mz))\ncat(\"Peak counts across processed spectra:\\n\")\n\nPeak counts across processed spectra:\n\nsummary(peak_counts)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\n# Plot peak count distribution\nhist(peak_counts, breaks = 10, \n     main = \"Distribution of Peak Counts\", \n     xlab = \"Number of Peaks\", ylab = \"Frequency\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "03-spectral-preprocessing.html#advanced-preprocessing-techniques",
    "href": "03-spectral-preprocessing.html#advanced-preprocessing-techniques",
    "title": "3  Spectral Data Preprocessing",
    "section": "3.8 Advanced Preprocessing Techniques",
    "text": "3.8 Advanced Preprocessing Techniques\n\n3.8.1 Mass Calibration Concepts\n\n# Simple mass calibration example (theoretical)\ncalibrate_mass &lt;- function(observed_mz, reference_mz, expected_mz) {\n  # Linear calibration\n  calibration_factor &lt;- expected_mz / reference_mz\n  calibrated_mz &lt;- observed_mz * calibration_factor\n  return(calibrated_mz)\n}\n\n# Example usage (with hypothetical values)\nobserved &lt;- c(100.0, 200.1, 300.2)\nreference &lt;- 200.1\nexpected &lt;- 200.0\n\ncalibrated &lt;- calibrate_mass(observed, reference, expected)\ncat(\"Original m/z:\", observed, \"\\n\")\n\nOriginal m/z: 100 200.1 300.2 \n\ncat(\"Calibrated m/z:\", calibrated, \"\\n\")\n\nCalibrated m/z: 99.95002 200 300.05",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "03-spectral-preprocessing.html#preprocessing-pipeline",
    "href": "03-spectral-preprocessing.html#preprocessing-pipeline",
    "title": "3  Spectral Data Preprocessing",
    "section": "3.9 Preprocessing Pipeline",
    "text": "3.9 Preprocessing Pipeline\n\n3.9.1 Complete Pipeline Function\n\ncomplete_preprocessing &lt;- function(spectra_obj, \n                                  baseline_quantile = 0.05,\n                                  smooth_sigma = 2,\n                                  normalization = \"base_peak\",\n                                  min_peak_intensity = 1000) {\n  \n  processed_spectra &lt;- list()\n  \n  for (i in seq_along(spectra_obj)) {\n    spec &lt;- spectra_obj[i]\n    mz_vals &lt;- mz(spec)[[1]]\n    int_vals &lt;- intensity(spec)[[1]]\n    \n    # Step 1: Baseline correction\n    baseline &lt;- quantile(int_vals, baseline_quantile)\n    int_vals &lt;- pmax(int_vals - baseline, 0)\n    \n    # Step 2: Smoothing\n    int_vals &lt;- gaussian_smooth(int_vals, sigma = smooth_sigma)\n    \n    # Step 3: Normalization\n    if (normalization == \"tic\") {\n      int_vals &lt;- tic_normalize(int_vals)\n    } else if (normalization == \"base_peak\") {\n      int_vals &lt;- base_peak_normalize(int_vals)\n    }\n    \n    # Step 4: Peak detection\n    peaks &lt;- detect_peaks(mz_vals, int_vals, min_peak_intensity)\n    \n    processed_spectra[[i]] &lt;- list(\n      original_index = i,\n      mz = mz_vals,\n      intensity = int_vals,\n      peaks = peaks,\n      metadata = list(\n        processing_date = Sys.time(),\n        parameters = list(\n          baseline_quantile = baseline_quantile,\n          smooth_sigma = smooth_sigma,\n          normalization = normalization,\n          min_peak_intensity = min_peak_intensity\n        )\n      )\n    )\n  }\n  \n  return(processed_spectra)\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "03-spectral-preprocessing.html#exercises",
    "href": "03-spectral-preprocessing.html#exercises",
    "title": "3  Spectral Data Preprocessing",
    "section": "3.10 Exercises",
    "text": "3.10 Exercises\n\nApply different baseline correction methods and compare results\nExperiment with various smoothing parameters\nImplement and test different peak detection algorithms\nCompare TIC and base peak normalization approaches\nCreate a quality control function for preprocessing results",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "03-spectral-preprocessing.html#summary",
    "href": "03-spectral-preprocessing.html#summary",
    "title": "3  Spectral Data Preprocessing",
    "section": "3.11 Summary",
    "text": "3.11 Summary\nThis chapter covered essential preprocessing techniques for MS data, including baseline correction, smoothing, peak detection, and normalization. These steps are fundamental for preparing data for downstream analysis and ensuring reliable results.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "04-peak-detection-quantification.html",
    "href": "04-peak-detection-quantification.html",
    "title": "4  Peak Detection and Quantification",
    "section": "",
    "text": "4.1 Setting Up the Environment\nPeak detection and quantification form the foundation of MS data analysis workflows. This chapter covers modern algorithms implemented in the R for Mass Spectrometry ecosystem and practical implementations using Spectra and MsCoreUtils.\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(c(\"xcms\", \"Spectra\", \"MetaboCoreUtils\", \"MsCoreUtils\"))\n\nlibrary(Spectra)           # Core MS data structures\nlibrary(MsCoreUtils)       # MS processing utilities\nlibrary(msdata)            # Example datasets\nlibrary(ggplot2)           # Visualization\nlibrary(dplyr)             # Data manipulation\nlibrary(patchwork)         # Plot composition\n# Load example data with error handling\nms_file &lt;- msdata::proteomics(full.names = TRUE)[1]\n\ntryCatch({\n  ms_data &lt;- Spectra(ms_file, backend = MsBackendMzR())\n  ms_data &lt;- setBackend(ms_data, backend = MsBackendDataFrame())\n  cat(\"Successfully loaded real MS data\\n\")\n}, error = function(e) {\n  cat(\"Note: Using synthetic data due to mzR compatibility issues\\n\\n\")\n  \n  # Create synthetic MS2 data\n  set.seed(456)\n  n_spectra &lt;- 100\n  \n  library(S4Vectors)\n  library(IRanges)\n  \n  mz_list &lt;- lapply(1:n_spectra, function(i) {\n    sort(runif(sample(80:200, 1), 100, 2000))\n  })\n  \n  intensity_list &lt;- lapply(mz_list, function(mz_vals) {\n    rlnorm(length(mz_vals), meanlog = 8, sdlog = 2)\n  })\n  \n  spd &lt;- DataFrame(\n    msLevel = rep(2L, n_spectra),\n    rtime = seq(100, 6000, length.out = n_spectra),\n    precursorMz = runif(n_spectra, 400, 1500),\n    precursorCharge = sample(2:3, n_spectra, replace = TRUE),\n    polarity = rep(1L, n_spectra)\n  )\n  \n  spd$mz &lt;- NumericList(mz_list)\n  spd$intensity &lt;- NumericList(intensity_list)\n  \n  backend &lt;- MsBackendDataFrame()\n  backend &lt;- backendInitialize(backend, spd)\n  ms_data &lt;&lt;- Spectra(backend)\n})\n\nNote: Using synthetic data due to mzR compatibility issues\n\n# Focus on MS2 spectra\nms2_data &lt;- filterMsLevel(ms_data, 2)\n\n# Select a representative spectrum for analysis\ntest_spectrum &lt;- ms2_data[min(50, length(ms2_data))]\npeaks &lt;- peaksData(test_spectrum)[[1]]\nmz_vals &lt;- peaks[, 1]\nint_vals &lt;- peaks[, 2]\n\ncat(\"Selected spectrum:\\n\")\n\nSelected spectrum:\n\ncat(\"  Precursor m/z:\", round(precursorMz(test_spectrum), 3), \"\\n\")\n\n  Precursor m/z: 754.184 \n\ncat(\"  Number of peaks:\", length(mz_vals), \"\\n\")\n\n  Number of peaks: 133 \n\ncat(\"  Intensity range:\", sprintf(\"%.2e - %.2e\", min(int_vals), max(int_vals)), \"\\n\")\n\n  Intensity range: 1.19e+01 - 2.37e+06",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "04-peak-detection-quantification.html#modern-peak-detection-with-spectra",
    "href": "04-peak-detection-quantification.html#modern-peak-detection-with-spectra",
    "title": "4  Peak Detection and Quantification",
    "section": "4.2 Modern Peak Detection with Spectra",
    "text": "4.2 Modern Peak Detection with Spectra\n\n4.2.1 Built-in Peak Picking\nThe Spectra package provides optimized peak picking methods:\n\n# Apply MAD (Median Absolute Deviation) based peak picking\npicked_spectrum &lt;- pickPeaks(test_spectrum, \n                             method = \"MAD\",\n                             snr = 2,  # Signal-to-noise ratio threshold\n                             k = 1L,   # Half window size (must be integer)\n                             descending = FALSE)\n\n# Compare before and after\noriginal_peaks &lt;- peaksData(test_spectrum)[[1]]\npicked_peaks &lt;- peaksData(picked_spectrum)[[1]]\n\ncat(\"Peak reduction:\\n\")\n\nPeak reduction:\n\ncat(\"  Original peaks:\", nrow(original_peaks), \"\\n\")\n\n  Original peaks: 133 \n\ncat(\"  After picking:\", nrow(picked_peaks), \"\\n\")\n\n  After picking: 24 \n\ncat(\"  Reduction:\", round((1 - nrow(picked_peaks)/nrow(original_peaks)) * 100, 1), \"%\\n\")\n\n  Reduction: 82 %\n\n# Visualize the difference\ncomparison_df &lt;- data.frame(\n  mz = c(original_peaks[, 1], picked_peaks[, 1]),\n  intensity = c(original_peaks[, 2], picked_peaks[, 2]),\n  type = rep(c(\"Original\", \"Picked\"), c(nrow(original_peaks), nrow(picked_peaks)))\n)\n\nggplot(comparison_df, aes(x = mz, y = intensity, color = type)) +\n  geom_segment(aes(xend = mz, yend = 0), alpha = 0.6) +\n  scale_color_manual(values = c(\"Original\" = \"gray70\", \"Picked\" = \"red\")) +\n  labs(title = \"Peak Picking with MAD Method\",\n       subtitle = paste(\"SNR threshold:\", 2),\n       x = \"m/z\", y = \"Intensity\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n4.2.2 Noise Estimation\n\n# Estimate noise level using MAD (Median Absolute Deviation)\n# Using a simple implementation since MsCoreUtils::noise has specific requirements\nestimate_noise_mad &lt;- function(x) {\n  # Use MAD (Median Absolute Deviation) for robust noise estimation\n  median_val &lt;- median(x, na.rm = TRUE)\n  mad_val &lt;- median(abs(x - median_val), na.rm = TRUE)\n  # MAD-based noise estimate\n  return(mad_val * 1.4826)  # Scale factor for normal distribution\n}\n\nnoise_level &lt;- estimate_noise_mad(int_vals)\ncat(\"Estimated noise level:\", round(noise_level, 2), \"\\n\")\n\nEstimated noise level: 3277.76 \n\n# Calculate signal-to-noise ratios\nsnr_values &lt;- int_vals / noise_level\ncat(\"SNR statistics:\\n\")\n\nSNR statistics:\n\ncat(\"  Median SNR:\", round(median(snr_values), 2), \"\\n\")\n\n  Median SNR: 0.75 \n\ncat(\"  Mean SNR:\", round(mean(snr_values), 2), \"\\n\")\n\n  Mean SNR: 9.31 \n\ncat(\"  Max SNR:\", round(max(snr_values), 2), \"\\n\")\n\n  Max SNR: 723.41 \n\n# Visualize SNR distribution\nsnr_df &lt;- data.frame(snr = snr_values)\nggplot(snr_df, aes(x = snr)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = 2, color = \"red\", linetype = \"dashed\") +\n  scale_x_log10() +\n  labs(title = \"Signal-to-Noise Ratio Distribution\",\n       subtitle = \"Red line indicates common SNR threshold (2)\",\n       x = \"SNR (log scale)\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Matched filter approach for peak detection\nmatched_filter_detection &lt;- function(mz, intensity, \n                                   peak_width = 0.1,  # Expected peak width\n                                   threshold = 0.1) {\n  \n  # Create Gaussian template\n  sigma &lt;- peak_width / (2 * sqrt(2 * log(2)))  # Convert FWHM to sigma\n  template_size &lt;- ceiling(6 * sigma)  # Template size\n  x &lt;- seq(-template_size, template_size, length.out = 2 * template_size + 1)\n  template &lt;- exp(-x^2 / (2 * sigma^2))\n  template &lt;- template / sum(template)\n  \n  # Apply matched filter\n  filtered &lt;- stats::filter(intensity, template, sides = 2)\n  filtered[is.na(filtered)] &lt;- 0\n  \n  # Threshold and find local maxima\n  above_threshold &lt;- filtered &gt; threshold * max(filtered)\n  peaks &lt;- logical(length(intensity))\n  \n  for (i in 2:(length(intensity)-1)) {\n    if (above_threshold[i] && \n        filtered[i] &gt; filtered[i-1] && \n        filtered[i] &gt; filtered[i+1]) {\n      peaks[i] &lt;- TRUE\n    }\n  }\n  \n  return(list(\n    peaks = which(peaks),\n    filtered_signal = filtered\n  ))\n}\n\n# Apply matched filter\nmf_result &lt;- matched_filter_detection(mz_vals, int_vals)\n\n# Plot results\npar(mfrow = c(2, 1))\nplot(mz_vals, int_vals, type = \"l\", main = \"Original Signal\", \n     xlab = \"m/z\", ylab = \"Intensity\")\nplot(mz_vals, mf_result$filtered_signal, type = \"l\", main = \"Matched Filter Response\",\n     xlab = \"m/z\", ylab = \"Filtered Response\", col = \"blue\")\npoints(mz_vals[mf_result$peaks], mf_result$filtered_signal[mf_result$peaks], \n       col = \"red\", pch = 19)\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "04-peak-detection-quantification.html#peak-quantification-methods",
    "href": "04-peak-detection-quantification.html#peak-quantification-methods",
    "title": "4  Peak Detection and Quantification",
    "section": "4.3 Peak Quantification Methods",
    "text": "4.3 Peak Quantification Methods\n\n4.3.1 Peak Integration\n\n# Peak integration function\nintegrate_peak &lt;- function(mz, intensity, peak_center, integration_method = \"trapezoid\") {\n  # Find peak boundaries\n  peak_idx &lt;- which.min(abs(mz - peak_center))\n  \n  # Simple peak boundary detection\n  left_bound &lt;- peak_idx\n  right_bound &lt;- peak_idx\n  \n  # Find left boundary (where intensity drops significantly)\n  while (left_bound &gt; 1 && \n         intensity[left_bound - 1] &gt; 0.1 * intensity[peak_idx]) {\n    left_bound &lt;- left_bound - 1\n  }\n  \n  # Find right boundary\n  while (right_bound &lt; length(intensity) && \n         intensity[right_bound + 1] &gt; 0.1 * intensity[peak_idx]) {\n    right_bound &lt;- right_bound + 1\n  }\n  \n  # Integration\n  if (integration_method == \"trapezoid\") {\n    # Trapezoidal rule\n    mz_region &lt;- mz[left_bound:right_bound]\n    int_region &lt;- intensity[left_bound:right_bound]\n    \n    area &lt;- sum(diff(mz_region) * (int_region[-1] + int_region[-length(int_region)]) / 2)\n  } else if (integration_method == \"sum\") {\n    # Simple sum\n    area &lt;- sum(intensity[left_bound:right_bound])\n  }\n  \n  return(list(\n    area = area,\n    peak_height = intensity[peak_idx],\n    peak_mz = mz[peak_idx],\n    left_bound = left_bound,\n    right_bound = right_bound,\n    boundaries_mz = c(mz[left_bound], mz[right_bound])\n  ))\n}\n\n# Demonstrate peak integration\nif (length(mf_result$peaks) &gt; 0) {\n  # Integrate first detected peak\n  peak_result &lt;- integrate_peak(mz_vals, int_vals, mz_vals[mf_result$peaks[1]])\n  \n  # Visualize integration region\n  plot(mz_vals, int_vals, type = \"l\", \n       main = \"Peak Integration Example\", xlab = \"m/z\", ylab = \"Intensity\")\n  \n  # Highlight integration region\n  int_region &lt;- peak_result$left_bound:peak_result$right_bound\n  polygon(c(mz_vals[int_region], rev(mz_vals[int_region])),\n           c(int_vals[int_region], rep(0, length(int_region))),\n           col = \"lightblue\", border = NA)\n  lines(mz_vals, int_vals)\n  points(peak_result$peak_mz, peak_result$peak_height, col = \"red\", pch = 19, cex = 1.5)\n  \n  cat(\"Peak Area:\", round(peak_result$area, 2), \"\\n\")\n  cat(\"Peak Height:\", round(peak_result$peak_height, 2), \"\\n\")\n  cat(\"Peak m/z:\", round(peak_result$peak_mz, 4), \"\\n\")\n}\n\n\n\n\n\n\n\n\nPeak Area: 0 \nPeak Height: 2371160 \nPeak m/z: 719.2207 \n\n\n\n\n4.3.2 Gaussian Fitting for Peak Quantification\n\n# Gaussian peak fitting\nfit_gaussian_peak &lt;- function(mz, intensity, peak_center) {\n  # Extract region around peak\n  peak_idx &lt;- which.min(abs(mz - peak_center))\n  \n  # Define fitting region (±20 points around peak)\n  start_idx &lt;- max(1, peak_idx - 20)\n  end_idx &lt;- min(length(mz), peak_idx + 20)\n  \n  mz_region &lt;- mz[start_idx:end_idx]\n  int_region &lt;- intensity[start_idx:end_idx]\n  \n  # Gaussian function\n  gaussian &lt;- function(x, params) {\n    A &lt;- params[1]  # Amplitude\n    mu &lt;- params[2] # Mean (center)\n    sigma &lt;- params[3] # Standard deviation\n    baseline &lt;- params[4] # Baseline\n    \n    return(baseline + A * exp(-(x - mu)^2 / (2 * sigma^2)))\n  }\n  \n  # Objective function for fitting\n  objective &lt;- function(params) {\n    predicted &lt;- gaussian(mz_region, params)\n    return(sum((int_region - predicted)^2))\n  }\n  \n  # Initial parameters\n  initial_params &lt;- c(\n    max(int_region),  # Amplitude\n    mz_region[which.max(int_region)],  # Center\n    0.05,  # Width\n    min(int_region)  # Baseline\n  )\n  \n  # Fit (simple optimization)\n  tryCatch({\n    fit_result &lt;- optim(initial_params, objective, method = \"Nelder-Mead\")\n    fitted_params &lt;- fit_result$par\n    \n    # Calculate R-squared\n    fitted_values &lt;- gaussian(mz_region, fitted_params)\n    ss_res &lt;- sum((int_region - fitted_values)^2)\n    ss_tot &lt;- sum((int_region - mean(int_region))^2)\n    r_squared &lt;- 1 - ss_res / ss_tot\n    \n    # Calculate area under Gaussian curve\n    gaussian_area &lt;- fitted_params[1] * fitted_params[3] * sqrt(2 * pi)\n    \n    return(list(\n      amplitude = fitted_params[1],\n      center = fitted_params[2],\n      sigma = fitted_params[3],\n      baseline = fitted_params[4],\n      area = gaussian_area,\n      r_squared = r_squared,\n      fitted_values = fitted_values,\n      mz_region = mz_region\n    ))\n  }, error = function(e) {\n    return(NULL)\n  })\n}\n\n# Demonstrate Gaussian fitting\nif (length(mf_result$peaks) &gt; 0) {\n  peak_mz &lt;- mz_vals[mf_result$peaks[1]]\n  gaussian_fit &lt;- fit_gaussian_peak(mz_vals, int_vals, peak_mz)\n  \n  if (!is.null(gaussian_fit)) {\n    # Get the indices for plotting\n    peak_idx &lt;- which.min(abs(mz_vals - peak_mz))\n    plot_start_idx &lt;- max(1, peak_idx - 20)\n    plot_end_idx &lt;- min(length(mz_vals), peak_idx + 20)\n    \n    # Plot fitting results\n    plot(gaussian_fit$mz_region, int_vals[plot_start_idx:plot_end_idx], \n         pch = 19, col = \"blue\",\n         main = \"Gaussian Peak Fitting\",\n         xlab = \"m/z\", ylab = \"Intensity\")\n    lines(gaussian_fit$mz_region, gaussian_fit$fitted_values, \n          col = \"red\", lwd = 2)\n    legend(\"topright\", c(\"Data\", \"Gaussian Fit\"), \n           col = c(\"blue\", \"red\"), lty = c(NA, 1), pch = c(19, NA))\n    \n    cat(\"Gaussian Fit Results:\\n\")\n    cat(\"  Center:\", round(gaussian_fit$center, 4), \"\\n\")\n    cat(\"  Area:\", round(gaussian_fit$area, 2), \"\\n\")\n    cat(\"  R-squared:\", round(gaussian_fit$r_squared, 3), \"\\n\")\n  } else {\n    cat(\"Gaussian fitting failed for the selected peak.\\n\")\n  }\n} else {\n  cat(\"No peaks detected for Gaussian fitting demonstration.\\n\")\n}\n\n\n\n\n\n\n\n\nGaussian Fit Results:\n  Center: 719.1114 \n  Area: -6674634 \n  R-squared: 0.994",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "04-peak-detection-quantification.html#peak-quality-assessment",
    "href": "04-peak-detection-quantification.html#peak-quality-assessment",
    "title": "4  Peak Detection and Quantification",
    "section": "4.4 Peak Quality Assessment",
    "text": "4.4 Peak Quality Assessment\n\n4.4.1 Signal-to-Noise Ratio Calculation\n\ncalculate_snr &lt;- function(mz, intensity, peak_center, noise_region_width = 2.0) {\n  peak_idx &lt;- which.min(abs(mz - peak_center))\n  peak_intensity &lt;- intensity[peak_idx]\n  \n  # Define noise region (away from peak)\n  noise_indices &lt;- which(abs(mz - peak_center) &gt; noise_region_width)\n  \n  if (length(noise_indices) &gt; 10) {\n    noise_level &lt;- sd(intensity[noise_indices])\n    snr &lt;- peak_intensity / noise_level\n  } else {\n    # Fallback: use entire spectrum for noise estimation\n    snr &lt;- peak_intensity / sd(intensity)\n  }\n  \n  return(snr)\n}\n\n# Calculate SNR for detected peaks\nsnr_values &lt;- numeric(length(mf_result$peaks))\nfor (i in seq_along(mf_result$peaks)) {\n  snr_values[i] &lt;- calculate_snr(mz_vals, int_vals, mz_vals[mf_result$peaks[i]])\n}\n\ncat(\"SNR values for detected peaks:\\n\")\n\nSNR values for detected peaks:\n\nprint(round(snr_values, 2))\n\n[1] 69.69\n\n\n\n\n4.4.2 Peak Symmetry Assessment\n\nassess_peak_symmetry &lt;- function(mz, intensity, peak_center) {\n  peak_idx &lt;- which.min(abs(mz - peak_center))\n  peak_height &lt;- intensity[peak_idx]\n  \n  # Find half-maximum points\n  half_max &lt;- peak_height / 2\n  \n  # Find left half-maximum point\n  left_idx &lt;- peak_idx\n  while (left_idx &gt; 1 && intensity[left_idx] &gt; half_max) {\n    left_idx &lt;- left_idx - 1\n  }\n  \n  # Find right half-maximum point\n  right_idx &lt;- peak_idx\n  while (right_idx &lt; length(intensity) && intensity[right_idx] &gt; half_max) {\n    right_idx &lt;- right_idx + 1\n  }\n  \n  # Calculate asymmetry factor\n  left_width &lt;- mz[peak_idx] - mz[left_idx]\n  right_width &lt;- mz[right_idx] - mz[peak_idx]\n  \n  asymmetry_factor &lt;- right_width / left_width\n  \n  return(list(\n    asymmetry_factor = asymmetry_factor,\n    fwhm = left_width + right_width,\n    left_width = left_width,\n    right_width = right_width\n  ))\n}\n\n# Assess symmetry for first peak\nif (length(mf_result$peaks) &gt; 0) {\n  symmetry_result &lt;- assess_peak_symmetry(mz_vals, int_vals, mz_vals[mf_result$peaks[1]])\n  \n  cat(\"Peak Symmetry Assessment:\\n\")\n  cat(\"  Asymmetry Factor:\", round(symmetry_result$asymmetry_factor, 3), \"\\n\")\n  cat(\"  FWHM:\", round(symmetry_result$fwhm, 4), \"\\n\")\n}\n\nPeak Symmetry Assessment:\n  Asymmetry Factor: 5.621 \n  FWHM: 22.7887",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "04-peak-detection-quantification.html#peak-list-management",
    "href": "04-peak-detection-quantification.html#peak-list-management",
    "title": "4  Peak Detection and Quantification",
    "section": "4.5 Peak List Management",
    "text": "4.5 Peak List Management\n\n4.5.1 Creating Comprehensive Peak Lists\n\ncreate_peak_list &lt;- function(mz, intensity, peak_indices) {\n  peak_data &lt;- data.frame(\n    peak_id = seq_along(peak_indices),\n    mz = mz[peak_indices],\n    intensity = intensity[peak_indices],\n    relative_intensity = intensity[peak_indices] / max(intensity) * 100\n  )\n  \n  # Add quality metrics\n  peak_data$snr &lt;- sapply(peak_indices, function(idx) {\n    calculate_snr(mz, intensity, mz[idx])\n  })\n  \n  peak_data$asymmetry &lt;- sapply(peak_indices, function(idx) {\n    sym_result &lt;- assess_peak_symmetry(mz, intensity, mz[idx])\n    return(sym_result$asymmetry_factor)\n  })\n  \n  # Add area calculations\n  peak_data$area &lt;- sapply(peak_indices, function(idx) {\n    integration_result &lt;- integrate_peak(mz, intensity, mz[idx])\n    return(integration_result$area)\n  })\n  \n  return(peak_data)\n}\n\n# Create comprehensive peak list\nif (length(mf_result$peaks) &gt; 0) {\n  comprehensive_peaks &lt;- create_peak_list(mz_vals, int_vals, mf_result$peaks)\n  print(comprehensive_peaks)\n}\n\n  peak_id       mz intensity relative_intensity      snr asymmetry area\n1       1 719.2207   2371160                100 69.68987  5.620902    0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "04-peak-detection-quantification.html#batch-peak-processing",
    "href": "04-peak-detection-quantification.html#batch-peak-processing",
    "title": "4  Peak Detection and Quantification",
    "section": "4.6 Batch Peak Processing",
    "text": "4.6 Batch Peak Processing\n\n4.6.1 Processing Multiple Spectra\n\nprocess_multiple_spectra &lt;- function(spectra_obj, max_spectra = 10) {\n  all_peak_lists &lt;- list()\n  \n  for (i in 1:min(length(spectra_obj), max_spectra)) {\n    # Extract spectrum data\n    mz_vals &lt;- mz(spectra_obj[i])[[1]]\n    int_vals &lt;- intensity(spectra_obj[i])[[1]]\n    \n    # Skip empty spectra\n    if (length(mz_vals) == 0 || max(int_vals) == 0) next\n    \n    # Detect peaks\n    mf_result &lt;- matched_filter_detection(mz_vals, int_vals)\n    \n    if (length(mf_result$peaks) &gt; 0) {\n      # Create peak list\n      peak_list &lt;- create_peak_list(mz_vals, int_vals, mf_result$peaks)\n      peak_list$spectrum_id &lt;- i\n      peak_list$retention_time &lt;- rtime(spectra_obj[i])\n      \n      all_peak_lists[[i]] &lt;- peak_list\n    }\n  }\n  \n  # Combine all peak lists\n  combined_peaks &lt;- do.call(rbind, all_peak_lists)\n  return(combined_peaks)\n}\n\n# Process multiple spectra\nbatch_results &lt;- process_multiple_spectra(ms_data, max_spectra = 5)\nif (!is.null(batch_results)) {\n  cat(\"Total peaks detected across spectra:\", nrow(batch_results), \"\\n\")\n  print(head(batch_results))\n}\n\nTotal peaks detected across spectra: 60 \n  peak_id       mz intensity relative_intensity       snr  asymmetry    area\n1       1 244.7057  42771.88           12.34308 0.9532201  2.5576214       0\n2       2 327.8421  92756.94           26.76772 2.0967084  0.5159425 1722408\n3       3 499.9734  48168.09           13.90031 1.0818725  0.4781007       0\n4       4 583.4235 226616.75           65.39688 5.3889942 73.3594800       0\n5       5 625.0181 148558.39           42.87086 3.4045120 15.9325747       0\n6       6 653.3221 136722.88           39.45538 3.1273898  0.3150271       0\n  spectrum_id retention_time\n1           1            100\n2           1            100\n3           1            100\n4           1            100\n5           1            100\n6           1            100",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "04-peak-detection-quantification.html#exercises",
    "href": "04-peak-detection-quantification.html#exercises",
    "title": "4  Peak Detection and Quantification",
    "section": "4.7 Exercises",
    "text": "4.7 Exercises\n\nImplement and compare different peak detection algorithms\nEvaluate the effect of different integration methods on quantification\nCreate quality filters based on SNR and peak symmetry\nDevelop a peak alignment algorithm for multiple spectra\nBuild a complete peak processing pipeline with quality control",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "04-peak-detection-quantification.html#summary",
    "href": "04-peak-detection-quantification.html#summary",
    "title": "4  Peak Detection and Quantification",
    "section": "4.8 Summary",
    "text": "4.8 Summary\nThis chapter covered advanced peak detection and quantification methods, including CWT-based detection, matched filtering, various integration approaches, and quality assessment metrics. These techniques form the foundation for reliable quantitative analysis in mass spectrometry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "05-data-visualization.html",
    "href": "05-data-visualization.html",
    "title": "5  Data Visualization for Mass Spectrometry",
    "section": "",
    "text": "5.1 Setting Up Visualization Environment\nEffective visualization is crucial for understanding MS data, identifying patterns, and communicating results. This chapter covers comprehensive visualization techniques using the R for Mass Spectrometry infrastructure.\nlibrary(Spectra)            # Core MS data structures\nlibrary(QFeatures)         # Quantitative features\nlibrary(msdata)            # Example data\nlibrary(ggplot2)           # Grammar of graphics\nlibrary(plotly)            # Interactive plots  \nlibrary(dplyr)             # Data manipulation\nlibrary(viridis)           # Color scales\nlibrary(gridExtra)         # Multiple plots\nlibrary(RColorBrewer)      # Color palettes\nlibrary(patchwork)         # Plot composition\nlibrary(MsCoreUtils)       # MS utilities\n# Load example data with comprehensive error handling\nms_file &lt;- msdata::proteomics(full.names = TRUE)[1]\n\ntryCatch({\n  ms_data &lt;- Spectra(ms_file, backend = MsBackendMzR())\n  ms_data &lt;- setBackend(ms_data, backend = MsBackendDataFrame())\n  cat(\"Successfully loaded real MS data\\n\")\n}, error = function(e) {\n  cat(\"Note: Using synthetic data due to mzR compatibility issues\\n\")\n  cat(\"Error details:\", conditionMessage(e), \"\\n\\n\")\n  \n  # Create synthetic MS data for visualization examples\n  set.seed(789)\n  n_spectra &lt;- 100\n  \n  library(S4Vectors)\n  library(IRanges)\n  \n  # Generate peak data\n  mz_list &lt;- lapply(1:n_spectra, function(i) {\n    sort(runif(sample(60:180, 1), 100, 2000))\n  })\n  \n  intensity_list &lt;- lapply(mz_list, function(mz_vals) {\n    rlnorm(length(mz_vals), meanlog = 8, sdlog = 2)\n  })\n  \n  # Generate MS levels consistently\n  ms_levels &lt;- sample(1:2, n_spectra, replace = TRUE, prob = c(0.4, 0.6))\n  \n  # Create metadata DataFrame\n  spd &lt;- DataFrame(\n    msLevel = ms_levels,\n    rtime = seq(50, 3500, length.out = n_spectra),\n    precursorMz = ifelse(ms_levels == 1, NA_real_, runif(n_spectra, 400, 1500)),\n    precursorCharge = ifelse(ms_levels == 1, NA_integer_, sample(2:3, n_spectra, replace = TRUE)),\n    polarity = rep(1L, n_spectra),\n    collisionEnergy = ifelse(ms_levels == 1, NA_real_, runif(n_spectra, 20, 40))\n  )\n  \n  # Add peak data as list columns\n  spd$mz &lt;- NumericList(mz_list)\n  spd$intensity &lt;- NumericList(intensity_list)\n  \n  # Initialize backend and create Spectra object\n  backend &lt;- MsBackendDataFrame()\n  backend &lt;- backendInitialize(backend, spd)\n  ms_data &lt;&lt;- Spectra(backend)\n})\n\nNote: Using synthetic data due to mzR compatibility issues\nError details: BiocParallel errors\n  1 remote errors, element index: 1\n  0 unevaluated and other errors\n  first remote error:\nError in DataFrame(..., check.names = FALSE): different row counts implied by arguments\n \n\ncat(\"\\nDataset summary:\\n\")\n\n\nDataset summary:\n\ncat(\"Total spectra:\", length(ms_data), \"\\n\")\n\nTotal spectra: 100 \n\ncat(\"MS levels:\", paste(unique(msLevel(ms_data)), collapse = \", \"), \"\\n\")\n\nMS levels: 1, 2 \n\ncat(\"RT range:\", round(range(rtime(ms_data)), 2), \"seconds\\n\")\n\nRT range: 50 3500 seconds",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "05-data-visualization.html#spectral-data-exploration",
    "href": "05-data-visualization.html#spectral-data-exploration",
    "title": "5  Data Visualization for Mass Spectrometry",
    "section": "5.2 Spectral Data Exploration",
    "text": "5.2 Spectral Data Exploration\n\n5.2.1 Core Spectral Variables\nUnderstanding the key variables in Spectra objects is essential for effective visualization:\n\n# Examine core spectral variables\ncore_vars &lt;- spectraVariables(ms_data)\ncat(\"Core spectral variables:\\n\")\n\nCore spectral variables:\n\nprint(core_vars)\n\n [1] \"msLevel\"                 \"rtime\"                  \n [3] \"acquisitionNum\"          \"scanIndex\"              \n [5] \"dataStorage\"             \"dataOrigin\"             \n [7] \"centroided\"              \"smoothed\"               \n [9] \"polarity\"                \"precScanNum\"            \n[11] \"precursorMz\"             \"precursorIntensity\"     \n[13] \"precursorCharge\"         \"collisionEnergy\"        \n[15] \"isolationWindowLowerMz\"  \"isolationWindowTargetMz\"\n[17] \"isolationWindowUpperMz\" \n\n# Display key metadata for first few spectra\nspectral_summary &lt;- data.frame(\n  msLevel = msLevel(ms_data)[1:10],\n  rtime = round(rtime(ms_data)[1:10], 2),\n  precursorMz = round(precursorMz(ms_data)[1:10], 3),\n  collisionEnergy = collisionEnergy(ms_data)[1:10],\n  polarity = polarity(ms_data)[1:10]\n)\n\nprint(head(spectral_summary))\n\n  msLevel  rtime precursorMz collisionEnergy polarity\n1       1  50.00          NA              NA        1\n2       1  84.85          NA              NA        1\n3       1 119.70          NA              NA        1\n4       2 154.55    1190.050        22.58851        1\n5       2 189.39    1338.415        37.03558        1\n6       1 224.24          NA              NA        1\n\n\n\n\n5.2.2 TIC and BPC Visualization\n\n# Calculate Total Ion Chromatogram (TIC) and Base Peak Chromatogram (BPC)\ntic_data &lt;- tic(ms_data)\nrt_data &lt;- rtime(ms_data) \n\n# Create chromatogram visualization\nchrom_data &lt;- data.frame(\n  rtime = rt_data,\n  tic = tic_data,\n  ms_level = factor(msLevel(ms_data))\n)\n\n# TIC plot colored by MS level\np_tic &lt;- ggplot(chrom_data, aes(x = rtime, y = tic, color = ms_level)) +\n  geom_line(alpha = 0.7) +\n  scale_color_viridis_d(name = \"MS Level\") +\n  labs(title = \"Total Ion Chromatogram\",\n       x = \"Retention Time (seconds)\", \n       y = \"Total Ion Current\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nprint(p_tic)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "05-data-visualization.html#single-spectrum-visualization",
    "href": "05-data-visualization.html#single-spectrum-visualization",
    "title": "5  Data Visualization for Mass Spectrometry",
    "section": "5.3 Single Spectrum Visualization",
    "text": "5.3 Single Spectrum Visualization\n\n5.3.1 Enhanced Spectrum Plots\n\n# Select a representative MS2 spectrum\nms2_data &lt;- filterMsLevel(ms_data, 2)\nspec_idx &lt;- 10\nsingle_spec &lt;- ms2_data[spec_idx]\n\n# Extract peak data\npeak_data &lt;- peaksData(single_spec)[[1]]\nmz_vals &lt;- peak_data[, 1]\nint_vals &lt;- peak_data[, 2]\n\n# Enhanced spectrum plot with annotations\nspectrum_df &lt;- data.frame(mz = mz_vals, intensity = int_vals)\n\np_spectrum &lt;- ggplot(spectrum_df, aes(x = mz, y = intensity)) +\n  geom_segment(aes(xend = mz, yend = 0), color = \"steelblue\", alpha = 0.8) +\n  geom_point(color = \"darkblue\", size = 0.8) +\n  labs(\n    title = paste(\"MS2 Spectrum\"),\n    subtitle = paste(\"Precursor m/z:\", round(precursorMz(single_spec), 3),\n                    \"| RT:\", round(rtime(single_spec), 2), \"sec\",\n                    \"| CE:\", collisionEnergy(single_spec)),\n    x = \"m/z\", y = \"Intensity\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\nprint(p_spectrum)\n\n\n\n\n\n\n\n\n\n\n5.3.2 Interactive Spectrum Visualization\n\n# Create interactive spectrum plot\np_interactive &lt;- plot_ly(spectrum_df, \n                        x = ~mz, y = ~intensity, \n                        type = \"scatter\", mode = \"lines+markers\",\n                        hovertemplate = \"m/z: %{x:.4f}&lt;br&gt;Intensity: %{y:.0f}&lt;extra&gt;&lt;/extra&gt;\",\n                        line = list(color = \"steelblue\")) %&gt;%\n  layout(\n    title = \"Interactive MS2 Spectrum\",\n    xaxis = list(title = \"m/z\"),\n    yaxis = list(title = \"Intensity\"),\n    hovermode = \"closest\"\n  )\n\np_interactive",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "05-data-visualization.html#spectral-comparison-and-mirror-plots",
    "href": "05-data-visualization.html#spectral-comparison-and-mirror-plots",
    "title": "5  Data Visualization for Mass Spectrometry",
    "section": "5.4 Spectral Comparison and Mirror Plots",
    "text": "5.4 Spectral Comparison and Mirror Plots\n\n5.4.1 Mirror Plot Implementation\n\n# Create mirror plot for spectrum comparison\ncreate_mirror_plot &lt;- function(spec1, spec2, \n                               title1 = \"Spectrum 1\", \n                               title2 = \"Spectrum 2\") {\n  \n  # Extract peak data\n  peaks1 &lt;- peaksData(spec1)[[1]]\n  peaks2 &lt;- peaksData(spec2)[[1]]\n  \n  # Extract m/z and intensity\n  mz1 &lt;- peaks1[, 1]\n  int1 &lt;- peaks1[, 2]\n  mz2 &lt;- peaks2[, 1]\n  int2 &lt;- peaks2[, 2]\n  \n  # Normalize intensities to 100%\n  int1_norm &lt;- int1 / max(int1) * 100\n  int2_norm &lt;- int2 / max(int2) * -100  # Negative for mirror effect\n  \n  # Combine data\n  plot_data &lt;- rbind(\n    data.frame(mz = mz1, intensity = int1_norm, spectrum = title1),\n    data.frame(mz = mz2, intensity = int2_norm, spectrum = title2)\n  )\n  \n  ggplot(plot_data, aes(x = mz, y = intensity, color = spectrum)) +\n    geom_segment(aes(xend = mz, yend = 0), linewidth = 0.5) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    scale_color_manual(values = c(\"blue\", \"red\")) +\n    labs(title = \"Mirror Plot Comparison\",\n         x = \"m/z\", y = \"Relative Intensity (%)\",\n         color = \"Spectrum\") +\n    theme_minimal() +\n    theme(legend.position = \"top\")\n}\n\n# Create mirror plot\nif (length(ms_data) &gt; 50) {\n  mirror_plot &lt;- create_mirror_plot(ms_data[10], ms_data[50], \n                                   \"Spectrum 10\", \"Spectrum 50\")\n  print(mirror_plot)\n}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "05-data-visualization.html#chromatographic-visualizations",
    "href": "05-data-visualization.html#chromatographic-visualizations",
    "title": "5  Data Visualization for Mass Spectrometry",
    "section": "5.5 Chromatographic Visualizations",
    "text": "5.5 Chromatographic Visualizations\n\n5.5.1 Total Ion Chromatogram (TIC)\n\n# Calculate TIC\ncalculate_tic &lt;- function(spectra_obj) {\n  tic_data &lt;- data.frame(\n    retention_time = rtime(spectra_obj),\n    tic = sapply(seq_along(spectra_obj), function(i) {\n      sum(intensity(spectra_obj[i])[[1]], na.rm = TRUE)\n    }),\n    ms_level = msLevel(spectra_obj)\n  )\n  return(tic_data)\n}\n\ntic_data &lt;- calculate_tic(ms_data)\n\n# Plot TIC\nggplot(tic_data, aes(x = retention_time, y = tic)) +\n  geom_line(color = \"darkblue\", size = 0.7) +\n  labs(title = \"Total Ion Chromatogram (TIC)\",\n       x = \"Retention Time (seconds)\", y = \"Total Ion Current\") +\n  scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n5.5.2 Base Peak Chromatogram (BPC)\n\n# Calculate BPC\ncalculate_bpc &lt;- function(spectra_obj) {\n  bpc_data &lt;- data.frame(\n    retention_time = rtime(spectra_obj),\n    bpc = sapply(seq_along(spectra_obj), function(i) {\n      max(intensity(spectra_obj[i])[[1]], na.rm = TRUE)\n    }),\n    ms_level = msLevel(spectra_obj)\n  )\n  return(bpc_data)\n}\n\nbpc_data &lt;- calculate_bpc(ms_data)\n\n# Combined TIC/BPC plot\ncombined_chromato &lt;- rbind(\n  data.frame(\n    retention_time = bpc_data$retention_time,\n    intensity = bpc_data$bpc,\n    ms_level = bpc_data$ms_level,\n    type = \"BPC\"\n  ),\n  data.frame(\n    retention_time = tic_data$retention_time,\n    intensity = tic_data$tic,\n    ms_level = tic_data$ms_level,\n    type = \"TIC\"\n  )\n)\n\nggplot(combined_chromato, aes(x = retention_time, y = intensity, color = type)) +\n  geom_line(linewidth = 0.7) +\n  scale_color_manual(values = c(\"BPC\" = \"red\", \"TIC\" = \"blue\")) +\n  labs(title = \"Chromatographic Profiles\",\n       x = \"Retention Time (seconds)\", y = \"Intensity\",\n       color = \"Type\") +\n  scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n5.5.3 Extracted Ion Chromatogram (EIC)\n\n# Extract ion chromatogram for specific m/z\nextract_ion_chromatogram &lt;- function(spectra_obj, target_mz, tolerance = 0.01) {\n  eic_data &lt;- data.frame(\n    retention_time = rtime(spectra_obj),\n    intensity = sapply(seq_along(spectra_obj), function(i) {\n      mz_vals &lt;- mz(spectra_obj[i])[[1]]\n      int_vals &lt;- intensity(spectra_obj[i])[[1]]\n      \n      # Find ions within tolerance\n      mz_mask &lt;- abs(mz_vals - target_mz) &lt;= tolerance\n      \n      if (any(mz_mask)) {\n        return(max(int_vals[mz_mask]))\n      } else {\n        return(0)\n      }\n    }),\n    ms_level = msLevel(spectra_obj)\n  )\n  \n  return(eic_data)\n}\n\n# Create EIC for multiple target masses\ntarget_masses &lt;- c(200, 300, 400, 500)\neic_plots &lt;- list()\n\nfor (i in seq_along(target_masses)) {\n  eic_data &lt;- extract_ion_chromatogram(ms_data, target_masses[i])\n  \n  eic_plots[[i]] &lt;- ggplot(eic_data, aes(x = retention_time, y = intensity)) +\n    geom_line(color = rainbow(length(target_masses))[i], size = 0.7) +\n    labs(title = paste(\"EIC m/z\", target_masses[i]),\n         x = \"Retention Time (s)\", y = \"Intensity\") +\n    theme_minimal() +\n    theme(plot.title = element_text(size = 10))\n}\n\n# Arrange EIC plots\ngrid.arrange(grobs = eic_plots, ncol = 2)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "05-data-visualization.html#heat-maps-and-2d-visualizations",
    "href": "05-data-visualization.html#heat-maps-and-2d-visualizations",
    "title": "5  Data Visualization for Mass Spectrometry",
    "section": "5.6 Heat Maps and 2D Visualizations",
    "text": "5.6 Heat Maps and 2D Visualizations\n\n5.6.1 m/z vs Retention Time Heat Map\n\n# Create 2D representation of MS data\ncreate_ms_heatmap &lt;- function(spectra_obj, mz_range = c(200, 800), \n                              mz_bins = 100, rt_bins = 50) {\n  \n  # Create binning grids\n  mz_breaks &lt;- seq(mz_range[1], mz_range[2], length.out = mz_bins + 1)\n  rt_values &lt;- rtime(spectra_obj)\n  rt_breaks &lt;- seq(min(rt_values), max(rt_values), length.out = rt_bins + 1)\n  \n  # Initialize intensity matrix\n  intensity_matrix &lt;- matrix(0, nrow = rt_bins, ncol = mz_bins)\n  \n  # Fill matrix\n  for (i in seq_along(spectra_obj)) {\n    mz_vals &lt;- mz(spectra_obj[i])[[1]]\n    int_vals &lt;- intensity(spectra_obj[i])[[1]]\n    rt_val &lt;- rt_values[i]\n    \n    # Skip if outside RT range\n    rt_bin &lt;- findInterval(rt_val, rt_breaks)\n    if (rt_bin &lt; 1 || rt_bin &gt; rt_bins) next\n    \n    # Process each peak\n    for (j in seq_along(mz_vals)) {\n      if (mz_vals[j] &gt;= mz_range[1] && mz_vals[j] &lt;= mz_range[2]) {\n        mz_bin &lt;- findInterval(mz_vals[j], mz_breaks)\n        if (mz_bin &gt;= 1 && mz_bin &lt;= mz_bins) {\n          intensity_matrix[rt_bin, mz_bin] &lt;- \n            max(intensity_matrix[rt_bin, mz_bin], int_vals[j])\n        }\n      }\n    }\n  }\n  \n  # Convert to data frame for plotting\n  rt_centers &lt;- rt_breaks[-length(rt_breaks)] + diff(rt_breaks)/2\n  mz_centers &lt;- mz_breaks[-length(mz_breaks)] + diff(mz_breaks)/2\n  \n  heat_data &lt;- expand.grid(\n    retention_time = rt_centers,\n    mz = mz_centers\n  )\n  heat_data$intensity &lt;- as.vector(intensity_matrix)\n  \n  return(heat_data)\n}\n\n# Create and plot heatmap\nheat_data &lt;- create_ms_heatmap(ms_data[1:100])  # Use subset for faster processing\n\nggplot(heat_data, aes(x = retention_time, y = mz, fill = intensity)) +\n  geom_raster() +\n  scale_fill_viridis_c(trans = \"sqrt\", name = \"Intensity\") +\n  labs(title = \"MS Data Heat Map\",\n       x = \"Retention Time (seconds)\", y = \"m/z\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "05-data-visualization.html#interactive-visualizations",
    "href": "05-data-visualization.html#interactive-visualizations",
    "title": "5  Data Visualization for Mass Spectrometry",
    "section": "5.7 Interactive Visualizations",
    "text": "5.7 Interactive Visualizations\n\n5.7.1 Interactive Spectrum Plot\n\n# Create interactive spectrum plot\ncreate_interactive_spectrum &lt;- function(spectrum_obj, title = \"Interactive Spectrum\") {\n  mz_vals &lt;- mz(spectrum_obj)[[1]]\n  int_vals &lt;- intensity(spectrum_obj)[[1]]\n  \n  plot_data &lt;- data.frame(mz = mz_vals, intensity = int_vals)\n  \n  p &lt;- ggplot(plot_data, aes(x = mz, y = intensity, \n                            text = paste(\"m/z:\", round(mz, 4), \n                                       \"&lt;br&gt;Intensity:\", round(intensity, 0)))) +\n    geom_line(color = \"blue\", linewidth = 0.5) +\n    labs(title = title, x = \"m/z\", y = \"Intensity\") +\n    theme_minimal()\n  \n  ggplotly(p, tooltip = \"text\")\n}\n\n# Create interactive plot\nif (length(ms_data) &gt; 0) {\n  interactive_plot &lt;- create_interactive_spectrum(ms_data[50])\n  interactive_plot\n}\n\n\n\n\n\n\n\n5.7.2 Interactive Chromatogram\n\n# Interactive TIC with zoom capability\ncreate_interactive_tic &lt;- function(tic_data) {\n  p &lt;- ggplot(tic_data, aes(x = retention_time, y = tic,\n                           text = paste(\"RT:\", round(retention_time, 2), \"s&lt;br&gt;\",\n                                       \"TIC:\", format(tic, scientific = TRUE)))) +\n    geom_line(color = \"darkblue\", size = 0.7) +\n    labs(title = \"Interactive Total Ion Chromatogram\",\n         x = \"Retention Time (seconds)\", y = \"Total Ion Current\") +\n    theme_minimal()\n  \n  ggplotly(p, tooltip = \"text\")\n}\n\n# Create interactive TIC\ninteractive_tic &lt;- create_interactive_tic(tic_data)\ninteractive_tic",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "05-data-visualization.html#specialized-ms-visualizations",
    "href": "05-data-visualization.html#specialized-ms-visualizations",
    "title": "5  Data Visualization for Mass Spectrometry",
    "section": "5.8 Specialized MS Visualizations",
    "text": "5.8 Specialized MS Visualizations\n\n5.8.1 Mass Defect Plot\n\n# Mass defect analysis\ncreate_mass_defect_plot &lt;- function(spectra_obj, intensity_threshold = 1000) {\n  # Extract all peaks above threshold\n  all_peaks &lt;- data.frame()\n  \n  for (i in seq_along(spectra_obj)[1:50]) {  # Use subset for demo\n    mz_vals &lt;- mz(spectra_obj[i])[[1]]\n    int_vals &lt;- intensity(spectra_obj[i])[[1]]\n    \n    # Filter by intensity threshold\n    mask &lt;- int_vals &gt; intensity_threshold\n    if (any(mask)) {\n      peaks &lt;- data.frame(\n        mz = mz_vals[mask],\n        intensity = int_vals[mask],\n        spectrum_id = i,\n        retention_time = rtime(spectra_obj[i])\n      )\n      all_peaks &lt;- rbind(all_peaks, peaks)\n    }\n  }\n  \n  if (nrow(all_peaks) &gt; 0) {\n    # Calculate mass defect\n    all_peaks$nominal_mass &lt;- floor(all_peaks$mz)\n    all_peaks$mass_defect &lt;- all_peaks$mz - all_peaks$nominal_mass\n    \n    # Plot mass defect\n    ggplot(all_peaks, aes(x = nominal_mass, y = mass_defect, \n                         color = log10(intensity))) +\n      geom_point(alpha = 0.6) +\n      scale_color_viridis_c(name = \"log10(Intensity)\") +\n      labs(title = \"Mass Defect Plot\",\n           x = \"Nominal Mass (Da)\", y = \"Mass Defect (Da)\") +\n      theme_minimal()\n  }\n}\n\n# Create mass defect plot\nmass_defect_plot &lt;- create_mass_defect_plot(ms_data)\nif (!is.null(mass_defect_plot)) {\n  print(mass_defect_plot)\n}\n\n\n\n\n\n\n\n\n\n\n5.8.2 3D Visualization\n\n# 3D surface plot for MS data\ncreate_3d_ms_plot &lt;- function(heat_data) {\n  # Reshape data for 3D plotting\n  rt_unique &lt;- sort(unique(heat_data$retention_time))\n  mz_unique &lt;- sort(unique(heat_data$mz))\n  \n  # Create intensity matrix\n  intensity_matrix &lt;- matrix(0, nrow = length(rt_unique), ncol = length(mz_unique))\n  \n  for (i in seq_along(rt_unique)) {\n    for (j in seq_along(mz_unique)) {\n      mask &lt;- heat_data$retention_time == rt_unique[i] & \n              heat_data$mz == mz_unique[j]\n      if (any(mask)) {\n        intensity_matrix[i, j] &lt;- heat_data$intensity[mask][1]\n      }\n    }\n  }\n  \n  # Create 3D plot\n  plot_ly(\n    x = ~mz_unique,\n    y = ~rt_unique,\n    z = ~intensity_matrix,\n    type = \"surface\",\n    colorscale = \"Viridis\"\n  ) %&gt;%\n    layout(\n      title = \"3D MS Data Visualization\",\n      scene = list(\n        xaxis = list(title = \"m/z\"),\n        yaxis = list(title = \"Retention Time (s)\"),\n        zaxis = list(title = \"Intensity\")\n      )\n    )\n}\n\n# Create 3D plot\nif (exists(\"heat_data\")) {\n  plot_3d &lt;- create_3d_ms_plot(heat_data[heat_data$intensity &gt; 0, ])\n  plot_3d\n}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "05-data-visualization.html#quality-control-visualizations",
    "href": "05-data-visualization.html#quality-control-visualizations",
    "title": "5  Data Visualization for Mass Spectrometry",
    "section": "5.9 Quality Control Visualizations",
    "text": "5.9 Quality Control Visualizations\n\n5.9.1 Intensity Distribution\n\n# Visualize intensity distributions\nplot_intensity_distribution &lt;- function(spectra_obj) {\n  all_intensities &lt;- unlist(lapply(seq_along(spectra_obj)[1:100], function(i) {\n    intensity(spectra_obj[i])[[1]]\n  }))\n  \n  # Remove zeros for log scale\n  all_intensities &lt;- all_intensities[all_intensities &gt; 0]\n  \n  ggplot(data.frame(intensity = all_intensities), aes(x = intensity)) +\n    geom_histogram(bins = 50, fill = \"skyblue\", alpha = 0.7) +\n    scale_x_log10() +\n    labs(title = \"Intensity Distribution (Log Scale)\",\n         x = \"Intensity (log10)\", y = \"Frequency\") +\n    theme_minimal()\n}\n\nintensity_dist_plot &lt;- plot_intensity_distribution(ms_data)\nprint(intensity_dist_plot)\n\n\n\n\n\n\n\n\n\n\n5.9.2 MS Level Distribution\n\n# Visualize MS level distribution over time\nms_level_data &lt;- data.frame(\n  retention_time = rtime(ms_data),\n  ms_level = factor(msLevel(ms_data))\n)\n\nggplot(ms_level_data, aes(x = retention_time, y = ms_level, color = ms_level)) +\n  geom_point(alpha = 0.6, size = 0.5) +\n  labs(title = \"MS Level Distribution Over Time\",\n       x = \"Retention Time (seconds)\", y = \"MS Level\",\n       color = \"MS Level\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "05-data-visualization.html#exercises",
    "href": "05-data-visualization.html#exercises",
    "title": "5  Data Visualization for Mass Spectrometry",
    "section": "5.10 Exercises",
    "text": "5.10 Exercises\n\nCreate a function to generate spectral annotations with peak labels\nDevelop a multi-panel visualization showing TIC, BPC, and selected EICs\nImplement a peak picking visualization with adjustable thresholds\nCreate an interactive dashboard for MS data exploration\nDesign visualization templates for different types of MS experiments",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "05-data-visualization.html#summary",
    "href": "05-data-visualization.html#summary",
    "title": "5  Data Visualization for Mass Spectrometry",
    "section": "5.11 Summary",
    "text": "5.11 Summary\nThis chapter covered comprehensive visualization techniques for MS data, from basic spectral plots to advanced interactive visualizations. Effective visualization is essential for data exploration, quality assessment, and result communication in mass spectrometry analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "06-statistical-analysis.html",
    "href": "06-statistical-analysis.html",
    "title": "6  Statistical Analysis of MS Data",
    "section": "",
    "text": "6.1 Setting Up the Statistical Environment\nStatistical analysis is fundamental to extracting meaningful biological insights from mass spectrometry data. This chapter covers statistical methods integrated with the R for Mass Spectrometry ecosystem, including univariate and multivariate approaches.\nlibrary(Spectra)           # Core MS data structures\nlibrary(QFeatures)         # Quantitative features\nlibrary(msdata)            # Example datasets\nlibrary(tidyverse)         # Data manipulation and visualization\nlibrary(broom)             # Tidy model outputs\nlibrary(limma)             # Linear models for omics\nlibrary(corrplot)          # Correlation plots\nlibrary(cluster)           # Clustering methods\nlibrary(factoextra)        # Enhanced PCA visualization\nlibrary(pheatmap)          # Heatmaps\nlibrary(ggrepel)           # Label repulsion\nlibrary(patchwork)         # Plot composition\n# Create a realistic experimental dataset for statistical analysis\nset.seed(123)\n\n# Experiment design: 2 conditions, 3 time points, 5 replicates\nn_conditions &lt;- 2\nn_timepoints &lt;- 3\nn_replicates &lt;- 5\nn_samples &lt;- n_conditions * n_timepoints * n_replicates\nn_features &lt;- 100\n\n# Sample metadata\nexperimental_data &lt;- expand.grid(\n  condition = c(\"Control\", \"Treatment\"),\n  timepoint = c(\"T0\", \"T1\", \"T2\"),\n  replicate = 1:n_replicates\n) %&gt;%\n  mutate(\n    sample_id = paste0(\"S\", 1:n()),\n    batch = rep(1:3, length.out = n())\n  )\n\n# Simulate feature intensities with biological effects\nfeature_matrix &lt;- matrix(\n  rlnorm(n_samples * n_features, meanlog = 10, sdlog = 0.8),\n  nrow = n_samples,\n  ncol = n_features\n)\n\n# Add treatment effects to specific features\ntreatment_idx &lt;- experimental_data$condition == \"Treatment\"\ntime_effect &lt;- as.numeric(factor(experimental_data$timepoint)) - 1\n\n# Features 1-20: Treatment effect\nfor (i in 1:20) {\n  effect_size &lt;- runif(1, 0.3, 0.8)\n  feature_matrix[treatment_idx, i] &lt;- feature_matrix[treatment_idx, i] * \n    exp(effect_size)\n}\n\n# Features 21-40: Time effect\nfor (i in 21:40) {\n  time_coef &lt;- runif(1, 0.1, 0.3)\n  feature_matrix[, i] &lt;- feature_matrix[, i] * exp(time_coef * time_effect)\n}\n\n# Features 41-60: Interaction effect\nfor (i in 41:60) {\n  interaction_coef &lt;- runif(1, 0.2, 0.5)\n  feature_matrix[treatment_idx, i] &lt;- feature_matrix[treatment_idx, i] * \n    exp(interaction_coef * time_effect[treatment_idx])\n}\n\ncolnames(feature_matrix) &lt;- paste0(\"Feature_\", 1:n_features)\nrownames(feature_matrix) &lt;- experimental_data$sample_id\n\ncat(\"Dataset created:\\n\")\n\nDataset created:\n\ncat(\"  Samples:\", n_samples, \"\\n\")\n\n  Samples: 30 \n\ncat(\"  Features:\", n_features, \"\\n\")\n\n  Features: 100 \n\ncat(\"  Design: 2 conditions × 3 timepoints × 5 replicates\\n\")\n\n  Design: 2 conditions × 3 timepoints × 5 replicates",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "06-statistical-analysis.html#descriptive-statistics",
    "href": "06-statistical-analysis.html#descriptive-statistics",
    "title": "6  Statistical Analysis of MS Data",
    "section": "6.2 Descriptive Statistics",
    "text": "6.2 Descriptive Statistics\n\n6.2.1 Basic Summary Statistics\n\n# Calculate summary statistics for features\nsummary_stats &lt;- data.frame(\n  feature = colnames(feature_matrix),\n  mean = apply(feature_matrix, 2, mean),\n  median = apply(feature_matrix, 2, median),\n  sd = apply(feature_matrix, 2, sd),\n  cv = apply(feature_matrix, 2, function(x) sd(x) / mean(x) * 100),  # Coefficient of variation\n  min = apply(feature_matrix, 2, min),\n  max = apply(feature_matrix, 2, max)\n)\n\n# Display first few features\nhead(summary_stats, 10)\n\n              feature     mean   median        sd        cv       min      max\nFeature_1   Feature_1 38539.95 30587.06  36945.31  95.86238  7683.549 154764.7\nFeature_2   Feature_2 43532.54 32455.23  40455.91  92.93257  6380.475 205635.6\nFeature_3   Feature_3 37879.54 27653.87  34593.29  91.32447  5644.510 184580.6\nFeature_4   Feature_4 36481.56 23965.46  31656.02  86.77265  6037.189 131991.5\nFeature_5   Feature_5 33341.32 23482.69  30827.79  92.46123  4261.610 118194.5\nFeature_6   Feature_6 57532.78 31559.10 104430.56 181.51490  8037.520 568784.5\nFeature_7   Feature_7 38466.44 23346.28  37830.52  98.34684  7718.372 165906.0\nFeature_8   Feature_8 38978.13 25492.49  34694.03  89.00897  8192.558 173304.9\nFeature_9   Feature_9 50905.26 27036.39  52624.87 103.37805 11720.590 224033.0\nFeature_10 Feature_10 42747.82 33157.06  28850.95  67.49104  5802.359 116445.9\n\n\n\n\n6.2.2 Distribution Analysis\n\n# Analyze distribution of coefficient of variation\nggplot(summary_stats, aes(x = cv)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = median(summary_stats$cv), \n             color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Distribution of Coefficient of Variation\",\n       subtitle = paste(\"Median CV =\", round(median(summary_stats$cv), 2), \"%\"),\n       x = \"Coefficient of Variation (%)\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n6.2.3 Missing Value Analysis\n\n# Simulate some missing values\nfeature_matrix_with_na &lt;- feature_matrix\nmissing_indices &lt;- sample(length(feature_matrix), size = length(feature_matrix) * 0.05)\nfeature_matrix_with_na[missing_indices] &lt;- NA\n\n# Calculate missing value statistics\nmissing_stats &lt;- data.frame(\n  feature = colnames(feature_matrix_with_na),\n  missing_count = apply(feature_matrix_with_na, 2, function(x) sum(is.na(x))),\n  missing_percent = apply(feature_matrix_with_na, 2, function(x) sum(is.na(x)) / length(x) * 100)\n)\n\n# Visualize missing value patterns\nmissing_pattern &lt;- missing_stats %&gt;%\n  filter(missing_count &gt; 0) %&gt;%\n  head(20)\n\nif (nrow(missing_pattern) &gt; 0) {\n  ggplot(missing_pattern, aes(x = reorder(feature, missing_percent), y = missing_percent)) +\n    geom_bar(stat = \"identity\", fill = \"coral\") +\n    coord_flip() +\n    labs(title = \"Missing Value Patterns\",\n         x = \"Feature\", y = \"Missing Percentage (%)\") +\n    theme_minimal()\n}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "06-statistical-analysis.html#hypothesis-testing",
    "href": "06-statistical-analysis.html#hypothesis-testing",
    "title": "6  Statistical Analysis of MS Data",
    "section": "6.3 Hypothesis Testing",
    "text": "6.3 Hypothesis Testing\n\n6.3.1 Two-Sample t-tests\n\n# Perform t-tests for each feature comparing conditions\nperform_ttest &lt;- function(feature_data, groups) {\n  control_data &lt;- feature_data[groups == \"Control\"]\n  treatment_data &lt;- feature_data[groups == \"Treatment\"]\n  \n  # Check for sufficient data\n  if (length(control_data) &lt; 3 || length(treatment_data) &lt; 3) {\n    return(data.frame(p.value = NA, statistic = NA, estimate = NA))\n  }\n  \n  # Perform t-test\n  test_result &lt;- t.test(control_data, treatment_data)\n  \n  return(data.frame(\n    p.value = test_result$p.value,\n    statistic = test_result$statistic,\n    estimate_diff = test_result$estimate[2] - test_result$estimate[1]\n  ))\n}\n\n# Apply t-tests to all features\nttest_results &lt;- data.frame()\nfor (i in 1:ncol(feature_matrix)) {\n  result &lt;- perform_ttest(feature_matrix[, i], experimental_data$condition)\n  result$feature &lt;- colnames(feature_matrix)[i]\n  ttest_results &lt;- rbind(ttest_results, result)\n}\n\n# Add multiple testing correction\nttest_results$p.adjusted &lt;- p.adjust(ttest_results$p.value, method = \"fdr\")\n\n# Display significant results\nsignificant_features &lt;- ttest_results[ttest_results$p.adjusted &lt; 0.05 & !is.na(ttest_results$p.adjusted), ]\ncat(\"Number of significant features (FDR &lt; 0.05):\", nrow(significant_features), \"\\n\")\n\nNumber of significant features (FDR &lt; 0.05): 0 \n\nhead(significant_features)\n\n[1] p.value       statistic     estimate_diff feature       p.adjusted   \n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\n6.3.2 Volcano Plot\n\n# Create volcano plot\nvolcano_data &lt;- ttest_results %&gt;%\n  mutate(\n    log2_fold_change = log2(abs(estimate_diff) + 1),  # Add 1 to avoid log(0)\n    neg_log10_p = -log10(p.value),\n    significant = p.adjusted &lt; 0.05 & !is.na(p.adjusted)\n  )\n\nggplot(volcano_data, aes(x = log2_fold_change, y = neg_log10_p)) +\n  geom_point(aes(color = significant), alpha = 0.7) +\n  scale_color_manual(values = c(\"FALSE\" = \"gray\", \"TRUE\" = \"red\")) +\n  geom_hline(yintercept = -log10(0.05), linetype = \"dashed\", color = \"blue\") +\n  labs(title = \"Volcano Plot\",\n       x = \"Log2 Fold Change\", y = \"-Log10 P-value\",\n       color = \"Significant\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "06-statistical-analysis.html#anova-for-multiple-groups",
    "href": "06-statistical-analysis.html#anova-for-multiple-groups",
    "title": "6  Statistical Analysis of MS Data",
    "section": "6.4 ANOVA for Multiple Groups",
    "text": "6.4 ANOVA for Multiple Groups\n\n# Add a third condition for ANOVA demonstration\nexperimental_data_extended &lt;- rbind(\n  experimental_data,\n  data.frame(\n    condition = rep(\"Treatment2\", 10),\n    timepoint = rep(\"T0\", 10),\n    replicate = rep(1:5, 2),\n    sample_id = paste0(\"S\", (nrow(experimental_data) + 1):(nrow(experimental_data) + 10)),\n    batch = rep(1:2, each = 5)\n  )\n)\n\n# Extend feature matrix\nadditional_samples &lt;- matrix(\n  rlnorm(10 * n_features, meanlog = 10.2, sdlog = 1),\n  nrow = 10,\n  ncol = n_features\n)\ncolnames(additional_samples) &lt;- colnames(feature_matrix)\nrownames(additional_samples) &lt;- experimental_data_extended$sample_id[31:40]\n\nfeature_matrix_extended &lt;- rbind(feature_matrix, additional_samples)\n\n# Perform one-way ANOVA for each feature\nperform_anova &lt;- function(feature_data, groups) {\n  if (length(unique(groups)) &lt; 2) return(data.frame(p.value = NA, f.statistic = NA))\n  \n  anova_result &lt;- aov(feature_data ~ groups)\n  summary_result &lt;- summary(anova_result)\n  \n  return(data.frame(\n    p.value = summary_result[[1]][1, \"Pr(&gt;F)\"],\n    f.statistic = summary_result[[1]][1, \"F value\"]\n  ))\n}\n\n# Apply ANOVA to all features\nanova_results &lt;- lapply(1:ncol(feature_matrix_extended), function(i) {\n  result &lt;- perform_anova(feature_matrix_extended[, i], experimental_data_extended$condition)\n  result$feature &lt;- colnames(feature_matrix_extended)[i]\n  return(result)\n})\nanova_results &lt;- do.call(rbind, anova_results)\n\n# Add multiple testing correction\nanova_results$p.adjusted &lt;- p.adjust(anova_results$p.value, method = \"fdr\")\n\n# Display significant ANOVA results\nsignificant_anova &lt;- anova_results[anova_results$p.adjusted &lt; 0.05 & !is.na(anova_results$p.adjusted), ]\ncat(\"Number of significant features (ANOVA FDR &lt; 0.05):\", nrow(significant_anova), \"\\n\")\n\nNumber of significant features (ANOVA FDR &lt; 0.05): 0",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "06-statistical-analysis.html#correlation-analysis",
    "href": "06-statistical-analysis.html#correlation-analysis",
    "title": "6  Statistical Analysis of MS Data",
    "section": "6.5 Correlation Analysis",
    "text": "6.5 Correlation Analysis\n\n6.5.1 Feature-Feature Correlations\n\n# Calculate correlation matrix for a subset of features\nfeature_subset &lt;- feature_matrix[, 1:20]  # Use subset for visualization\ncor_matrix &lt;- cor(feature_subset, use = \"complete.obs\")\n\n# Visualize correlation matrix\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", \n         order = \"hclust\", tl.cex = 0.8, tl.col = \"black\")\ntitle(\"Feature-Feature Correlation Matrix\")\n\n\n\n\n\n\n\n\n\n\n6.5.2 Correlation with Experimental Factors\n\n# Encode experimental factors as numeric for correlation\nexperimental_numeric &lt;- experimental_data %&gt;%\n  mutate(\n    condition_numeric = ifelse(condition == \"Control\", 0, 1),\n    batch_numeric = as.numeric(batch)\n  )\n\n# Calculate correlations between features and experimental factors\ncor_with_condition &lt;- apply(feature_matrix, 2, function(x) {\n  cor(x, experimental_numeric$condition_numeric, use = \"complete.obs\")\n})\n\ncor_with_batch &lt;- apply(feature_matrix, 2, function(x) {\n  cor(x, experimental_numeric$batch_numeric, use = \"complete.obs\")\n})\n\n# Visualize correlations\ncorrelation_df &lt;- data.frame(\n  feature = names(cor_with_condition),\n  condition_cor = cor_with_condition,\n  batch_cor = cor_with_batch\n)\n\nggplot(correlation_df, aes(x = condition_cor, y = batch_cor)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Feature Correlations with Experimental Factors\",\n       x = \"Correlation with Treatment\", y = \"Correlation with Batch\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "06-statistical-analysis.html#principal-component-analysis-pca",
    "href": "06-statistical-analysis.html#principal-component-analysis-pca",
    "title": "6  Statistical Analysis of MS Data",
    "section": "6.6 Principal Component Analysis (PCA)",
    "text": "6.6 Principal Component Analysis (PCA)\n\n6.6.1 Performing PCA\n\n# Standardize data for PCA\nfeature_matrix_scaled &lt;- scale(feature_matrix)\n\n# Perform PCA\npca_result &lt;- prcomp(feature_matrix_scaled, center = FALSE, scale. = FALSE)\n\n# Extract PC scores\npca_scores &lt;- data.frame(pca_result$x) %&gt;%\n  mutate(\n    sample_id = experimental_data$sample_id,\n    condition = experimental_data$condition,\n    batch = factor(experimental_data$batch)\n  )\n\n# Variance explained\nvariance_explained &lt;- (pca_result$sdev^2) / sum(pca_result$sdev^2) * 100\n\n\n\n6.6.2 PCA Visualization\n\n# PCA scores plot\nggplot(pca_scores, aes(x = PC1, y = PC2, color = condition, shape = batch)) +\n  geom_point(size = 3, alpha = 0.8) +\n  stat_ellipse(aes(group = condition), alpha = 0.3) +\n  labs(title = \"PCA Scores Plot\",\n       x = paste0(\"PC1 (\", round(variance_explained[1], 1), \"%)\"),\n       y = paste0(\"PC2 (\", round(variance_explained[2], 1), \"%)\"),\n       color = \"Condition\", shape = \"Batch\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n6.6.3 Scree Plot\n\n# Scree plot\nscree_data &lt;- data.frame(\n  PC = 1:min(10, length(variance_explained)),\n  Variance = variance_explained[1:min(10, length(variance_explained))]\n)\n\nggplot(scree_data, aes(x = PC, y = Variance)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_point(color = \"red\", size = 3) +\n  labs(title = \"Scree Plot\",\n       x = \"Principal Component\", y = \"Variance Explained (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n6.6.4 PCA Loadings\n\n# Extract and visualize loadings\nloadings_data &lt;- data.frame(\n  feature = colnames(feature_matrix),\n  PC1 = pca_result$rotation[, 1],\n  PC2 = pca_result$rotation[, 2]\n)\n\n# Plot loadings\nggplot(loadings_data, aes(x = PC1, y = PC2)) +\n  geom_point(alpha = 0.6) +\n  geom_text(aes(label = feature), size = 2, check_overlap = TRUE) +\n  labs(title = \"PCA Loadings Plot\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "06-statistical-analysis.html#clustering-analysis",
    "href": "06-statistical-analysis.html#clustering-analysis",
    "title": "6  Statistical Analysis of MS Data",
    "section": "6.7 Clustering Analysis",
    "text": "6.7 Clustering Analysis\n\n6.7.1 Hierarchical Clustering\n\n# Perform hierarchical clustering\ndist_matrix &lt;- dist(feature_matrix_scaled)\nhclust_result &lt;- hclust(dist_matrix, method = \"ward.D2\")\n\n# Cut tree to get clusters\nn_clusters &lt;- 3\ncluster_assignments &lt;- cutree(hclust_result, k = n_clusters)\n\n# Add cluster assignments to experimental data\nexperimental_data$cluster &lt;- factor(cluster_assignments)\n\n# Visualize dendrogram\nfviz_dend(hclust_result, k = n_clusters, \n          cex = 0.8,\n          color_labels_by_k = TRUE,\n          main = \"Hierarchical Clustering Dendrogram\")\n\n\n\n\n\n\n\n\n\n\n6.7.2 K-means Clustering\n\n# Perform k-means clustering\nset.seed(123)\nkmeans_result &lt;- kmeans(feature_matrix_scaled, centers = 3, nstart = 25)\n\n# Add k-means clusters to data\nexperimental_data$kmeans_cluster &lt;- factor(kmeans_result$cluster)\n\n# Visualize clusters in PCA space\nggplot(pca_scores, aes(x = PC1, y = PC2)) +\n  geom_point(aes(color = experimental_data$kmeans_cluster), size = 3, alpha = 0.8) +\n  labs(title = \"K-means Clustering in PCA Space\",\n       x = paste0(\"PC1 (\", round(variance_explained[1], 1), \"%)\"),\n       y = paste0(\"PC2 (\", round(variance_explained[2], 1), \"%)\"),\n       color = \"K-means Cluster\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n6.7.3 Cluster Validation\n\n# Silhouette analysis\nsil_scores &lt;- silhouette(kmeans_result$cluster, dist_matrix)\n\n# Visualize silhouette plot\nfviz_silhouette(sil_scores, main = \"Silhouette Plot for K-means Clustering\")\n\n  cluster size ave.sil.width\n1       1   22          0.10\n2       2    7         -0.03\n3       3    1          0.00\n\n\n\n\n\n\n\n\n# Average silhouette width\navg_sil_width &lt;- mean(sil_scores[, 3])\ncat(\"Average silhouette width:\", round(avg_sil_width, 3), \"\\n\")\n\nAverage silhouette width: 0.071",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "06-statistical-analysis.html#heat-map-analysis",
    "href": "06-statistical-analysis.html#heat-map-analysis",
    "title": "6  Statistical Analysis of MS Data",
    "section": "6.8 Heat Map Analysis",
    "text": "6.8 Heat Map Analysis\n\n6.8.1 Feature Heat Map\n\n# Create heat map of top variable features\ntop_variable_features &lt;- summary_stats %&gt;%\n  top_n(30, cv) %&gt;%\n  pull(feature)\n\nheatmap_data &lt;- feature_matrix[, top_variable_features]\n\n# Create annotation for samples\nannotation_df &lt;- experimental_data %&gt;%\n  select(condition, batch) %&gt;%\n  data.frame(row.names = experimental_data$sample_id)\n\n# Generate heat map\npheatmap(t(scale(heatmap_data)), \n         annotation_col = annotation_df,\n         show_rownames = FALSE,\n         show_colnames = TRUE,\n         clustering_distance_rows = \"euclidean\",\n         clustering_distance_cols = \"euclidean\",\n         main = \"Heat Map of Top Variable Features\")",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "06-statistical-analysis.html#power-analysis",
    "href": "06-statistical-analysis.html#power-analysis",
    "title": "6  Statistical Analysis of MS Data",
    "section": "6.9 Power Analysis",
    "text": "6.9 Power Analysis\n\n6.9.1 Sample Size Calculation\n\n# Function for power analysis\ncalculate_power &lt;- function(effect_size, sample_size_per_group, alpha = 0.05) {\n  # Calculate power for two-sample t-test\n  delta &lt;- effect_size\n  n &lt;- sample_size_per_group\n  \n  # Non-centrality parameter\n  ncp &lt;- delta * sqrt(n/2)\n  \n  # Critical value\n  t_crit &lt;- qt(1 - alpha/2, df = 2*n - 2)\n  \n  # Power calculation\n  power &lt;- 1 - pt(t_crit, df = 2*n - 2, ncp = ncp) + \n           pt(-t_crit, df = 2*n - 2, ncp = ncp)\n  \n  return(power)\n}\n\n# Power analysis for different effect sizes and sample sizes\neffect_sizes &lt;- seq(0.2, 2.0, by = 0.2)\nsample_sizes &lt;- seq(5, 30, by = 5)\n\npower_results &lt;- expand.grid(\n  effect_size = effect_sizes,\n  sample_size = sample_sizes\n) %&gt;%\n  rowwise() %&gt;%\n  mutate(power = calculate_power(effect_size, sample_size))\n\n# Visualize power analysis\nggplot(power_results, aes(x = sample_size, y = power, color = factor(effect_size))) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Power Analysis for Two-Sample t-test\",\n       x = \"Sample Size per Group\", y = \"Statistical Power\",\n       color = \"Effect Size\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "06-statistical-analysis.html#exercises",
    "href": "06-statistical-analysis.html#exercises",
    "title": "6  Statistical Analysis of MS Data",
    "section": "6.10 Exercises",
    "text": "6.10 Exercises\n\nPerform statistical analysis on your own MS dataset\nImplement different multiple testing correction methods and compare results\nConduct time-series analysis for longitudinal MS data\nApply machine learning classification to distinguish sample groups\nDevelop quality control metrics based on statistical properties",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "06-statistical-analysis.html#summary",
    "href": "06-statistical-analysis.html#summary",
    "title": "6  Statistical Analysis of MS Data",
    "section": "6.11 Summary",
    "text": "6.11 Summary\nThis chapter covered essential statistical methods for MS data analysis, including descriptive statistics, hypothesis testing, multivariate analysis, and clustering. These statistical tools are fundamental for extracting meaningful biological insights from mass spectrometry experiments.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "07-metabolomics-analysis.html",
    "href": "07-metabolomics-analysis.html",
    "title": "7  Metabolomics Data Analysis",
    "section": "",
    "text": "7.1 Setting Up Metabolomics Environment\nMetabolomics involves the comprehensive analysis of small molecules (metabolites) in biological systems. This chapter covers LC-MS metabolomics data processing using the R for Mass Spectrometry ecosystem, with emphasis on the xcms package and Spectra integration.\n# Load required packages (with conditional loading for Bioconductor packages)\nif (requireNamespace(\"xcms\", quietly = TRUE)) {\n  library(xcms)              # LC-MS data processing\n} else {\n  message(\"Note: xcms package not installed. Install with: BiocManager::install('xcms')\")\n}\n\nif (requireNamespace(\"Spectra\", quietly = TRUE)) {\n  library(Spectra)           # Core MS data structures\n} else {\n  message(\"Note: Spectra package not installed. Install with: BiocManager::install('Spectra')\")\n}\n\nif (requireNamespace(\"MetaboCoreUtils\", quietly = TRUE)) {\n  library(MetaboCoreUtils)   # Metabolomics utilities\n}\n\nif (requireNamespace(\"MsCoreUtils\", quietly = TRUE)) {\n  library(MsCoreUtils)       # MS data utilities\n}\n\n# Standard CRAN packages\nlibrary(ggplot2)           # Visualization\nlibrary(dplyr)             # Data manipulation\nlibrary(tidyr)             # Data tidying\n\n# Optional packages\nif (requireNamespace(\"pheatmap\", quietly = TRUE)) library(pheatmap)\nif (requireNamespace(\"patchwork\", quietly = TRUE)) library(patchwork)\nif (requireNamespace(\"CAMERA\", quietly = TRUE)) library(CAMERA)\nif (requireNamespace(\"CompoundDb\", quietly = TRUE)) library(CompoundDb)\nif (requireNamespace(\"mixOmics\", quietly = TRUE)) library(mixOmics)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-metabolomics-analysis.html#understanding-metabolomics-workflows",
    "href": "07-metabolomics-analysis.html#understanding-metabolomics-workflows",
    "title": "7  Metabolomics Data Analysis",
    "section": "7.2 Understanding Metabolomics Workflows",
    "text": "7.2 Understanding Metabolomics Workflows\n\n7.2.1 LC-MS Metabolomics Pipeline\nA typical untargeted metabolomics workflow consists of:\n\nPeak detection: Identifying features (m/z-RT pairs) across samples\nAlignment: Correcting RT variations between samples\nCorrespondence: Matching features across samples\nGap filling: Integrating missing peaks\nAnnotation: Identifying metabolites\nStatistical analysis: Finding differential metabolites\n\n\n# Display workflow overview\nworkflow_steps &lt;- data.frame(\n  Step = 1:6,\n  Process = c(\"Peak Detection\", \"Retention Time Correction\", \n              \"Correspondence\", \"Gap Filling\", \n              \"Annotation\", \"Statistical Analysis\"),\n  Package = c(\"xcms\", \"xcms\", \"xcms\", \"xcms\", \n              \"CAMERA/CompoundDb\", \"limma/mixOmics\"),\n  Output = c(\"Chromatographic peaks\", \"Aligned peaks\",\n             \"Feature matrix\", \"Complete matrix\",\n             \"Putative IDs\", \"Differential metabolites\")\n)\n\nprint(workflow_steps)\n\n  Step                   Process           Package                   Output\n1    1            Peak Detection              xcms    Chromatographic peaks\n2    2 Retention Time Correction              xcms            Aligned peaks\n3    3            Correspondence              xcms           Feature matrix\n4    4               Gap Filling              xcms          Complete matrix\n5    5                Annotation CAMERA/CompoundDb             Putative IDs\n6    6      Statistical Analysis    limma/mixOmics Differential metabolites",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-metabolomics-analysis.html#xcms-based-peak-detection",
    "href": "07-metabolomics-analysis.html#xcms-based-peak-detection",
    "title": "7  Metabolomics Data Analysis",
    "section": "7.3 XCMS-Based Peak Detection",
    "text": "7.3 XCMS-Based Peak Detection\n\n7.3.1 Loading Raw Data\n\n# Example workflow with real LC-MS data\n# Load raw files (typically multiple mzML files)\nraw_files &lt;- c(\"sample1.mzML\", \"sample2.mzML\", \"sample3.mzML\")\n\n# Create phenotype data\npd &lt;- data.frame(\n  sample_name = basename(raw_files),\n  sample_group = c(\"Control\", \"Control\", \"Treatment\"),\n  sample_type = \"Sample\",\n  stringsAsFactors = FALSE\n)\n\n# Read data into an XCMSnExp object\nraw_data &lt;- readMSData(\n  files = raw_files,\n  pdata = new(\"NAnnotatedDataFrame\", pd),\n  mode = \"onDisk\"\n)\n\n# Display data summary\nraw_data\n\n\n\n7.3.2 Chromatographic Peak Detection\n\n# Define peak detection parameters (CentWave for high-resolution data)\ncwp &lt;- CentWaveParam(\n  ppm = 20,                  # m/z tolerance in ppm\n  peakwidth = c(5, 20),      # Expected peak width range (seconds)\n  snthresh = 10,             # Signal-to-noise threshold\n  prefilter = c(3, 5000),   # Prefilter: min peaks, min intensity\n  mzCenterFun = \"wMean\",     # Weighted mean for m/z\n  integrate = 1,             # Integration method\n  mzdiff = 0.01,            # Minimum m/z difference\n  fitgauss = FALSE,          # Gaussian peak fitting\n  noise = 1000              # Noise threshold\n)\n\n# Perform peak detection\ndata_peaks &lt;- findChromPeaks(raw_data, param = cwp)\n\n# Summary of detected peaks\ncat(\"Total chromatographic peaks:\", sum(chromPeaks(data_peaks)[, \"into\"] &gt; 0), \"\\n\")\n\n\n\n7.3.3 Simulated XCMS Workflow\nFor demonstration, let’s create a simulated dataset:\n\n# Create synthetic metabolomics data for demonstration\nset.seed(42)\n\n# Simulate retention times and m/z values for metabolites\nn_metabolites &lt;- 50\nmetabolite_data &lt;- data.frame(\n  metabolite_id = paste0(\"Met_\", 1:n_metabolites),\n  mz = runif(n_metabolites, 100, 800),\n  rt = runif(n_metabolites, 60, 1200),  # RT in seconds\n  intensity_mean = rlnorm(n_metabolites, meanlog = 6, sdlog = 1)\n)\n\n# Create sample information\nsample_info &lt;- data.frame(\n  sample_name = paste0(\"Sample_\", 1:20),\n  group = rep(c(\"Control\", \"Treatment\"), each = 10),\n  batch = rep(1:2, each = 10),\n  injection_order = 1:20\n)\n\n# Simulate peak intensity matrix\nintensity_matrix &lt;- matrix(nrow = nrow(sample_info), ncol = nrow(metabolite_data))\nrownames(intensity_matrix) &lt;- sample_info$sample_name\ncolnames(intensity_matrix) &lt;- metabolite_data$metabolite_id\n\nfor (i in 1:nrow(sample_info)) {\n  for (j in 1:nrow(metabolite_data)) {\n    # Add group effect for some metabolites\n    group_effect &lt;- ifelse(sample_info$group[i] == \"Treatment\" & j &lt;= 15, 1.5, 1.0)\n    # Add some biological and technical variation\n    intensity_matrix[i, j] &lt;- metabolite_data$intensity_mean[j] * group_effect * \n                              rlnorm(1, 0, 0.3)  # 30% CV\n  }\n}\n\ncat(\"Created synthetic metabolomics dataset with\", nrow(sample_info), \"samples and\", \n    nrow(metabolite_data), \"metabolites\\n\")\n\nCreated synthetic metabolomics dataset with 20 samples and 50 metabolites\n\n\n\n\n7.3.4 Peak Quality Assessment\n\n# Function to assess peak quality\nassess_peak_quality &lt;- function(intensity_matrix, sample_info) {\n  # Calculate QC metrics\n  quality_metrics &lt;- data.frame(\n    metabolite_id = colnames(intensity_matrix),\n    mean_intensity = apply(intensity_matrix, 2, mean, na.rm = TRUE),\n    median_intensity = apply(intensity_matrix, 2, median, na.rm = TRUE),\n    cv_percent = apply(intensity_matrix, 2, function(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) * 100),\n    missing_percent = apply(intensity_matrix, 2, function(x) sum(is.na(x)) / length(x) * 100),\n    detection_rate = apply(intensity_matrix, 2, function(x) sum(x &gt; 0, na.rm = TRUE) / length(x) * 100)\n  )\n  \n  return(quality_metrics)\n}\n\npeak_quality &lt;- assess_peak_quality(intensity_matrix, sample_info)\n\n# Visualize peak quality metrics\nggplot(peak_quality, aes(x = cv_percent)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", alpha = 0.7) +\n  geom_vline(xintercept = 30, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Distribution of Coefficient of Variation\",\n       subtitle = \"Red line indicates 30% CV threshold\",\n       x = \"CV (%)\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n7.3.5 Peak Filtering\n\n# Apply quality filters\nfilter_peaks &lt;- function(intensity_matrix, quality_metrics, \n                        cv_threshold = 30, \n                        detection_threshold = 80,\n                        min_intensity = 1000) {\n  \n  # Define filters\n  cv_filter &lt;- quality_metrics$cv_percent &lt; cv_threshold\n  detection_filter &lt;- quality_metrics$detection_rate &gt;= detection_threshold\n  intensity_filter &lt;- quality_metrics$mean_intensity &gt;= min_intensity\n  \n  # Combine filters\n  pass_filter &lt;- cv_filter & detection_filter & intensity_filter\n  \n  cat(\"Filtering results:\\n\")\n  cat(\"  CV filter:\", sum(cv_filter, na.rm = TRUE), \"passed\\n\")\n  cat(\"  Detection rate filter:\", sum(detection_filter, na.rm = TRUE), \"passed\\n\")\n  cat(\"  Intensity filter:\", sum(intensity_filter, na.rm = TRUE), \"passed\\n\")\n  cat(\"  Combined filter:\", sum(pass_filter, na.rm = TRUE), \"passed\\n\")\n  \n  # Apply filter\n  filtered_matrix &lt;- intensity_matrix[, pass_filter]\n  filtered_metabolites &lt;- metabolite_data[pass_filter, ]\n  \n  return(list(\n    intensity_matrix = filtered_matrix,\n    metabolite_data = filtered_metabolites,\n    filter_summary = data.frame(\n      total_features = ncol(intensity_matrix),\n      passed_filter = sum(pass_filter, na.rm = TRUE),\n      filter_rate = sum(pass_filter, na.rm = TRUE) / ncol(intensity_matrix) * 100\n    )\n  ))\n}\n\nfiltered_data &lt;- filter_peaks(intensity_matrix, peak_quality)\n\nFiltering results:\n  CV filter: 21 passed\n  Detection rate filter: 50 passed\n  Intensity filter: 9 passed\n  Combined filter: 4 passed\n\nprint(filtered_data$filter_summary)\n\n  total_features passed_filter filter_rate\n1             50             4           8",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-metabolomics-analysis.html#data-normalization-and-scaling",
    "href": "07-metabolomics-analysis.html#data-normalization-and-scaling",
    "title": "7  Metabolomics Data Analysis",
    "section": "7.4 Data Normalization and Scaling",
    "text": "7.4 Data Normalization and Scaling\n\n7.4.1 Different Normalization Methods\n\n# Implement various normalization methods\nnormalize_data &lt;- function(intensity_matrix, method = \"median\") {\n  normalized_matrix &lt;- intensity_matrix\n  \n  if (method == \"median\") {\n    # Median normalization\n    sample_medians &lt;- apply(intensity_matrix, 1, median, na.rm = TRUE)\n    global_median &lt;- median(sample_medians, na.rm = TRUE)\n    normalization_factors &lt;- global_median / sample_medians\n    \n    for (i in 1:nrow(intensity_matrix)) {\n      normalized_matrix[i, ] &lt;- intensity_matrix[i, ] * normalization_factors[i]\n    }\n    \n  } else if (method == \"tic\") {\n    # Total ion current normalization\n    sample_sums &lt;- apply(intensity_matrix, 1, sum, na.rm = TRUE)\n    global_sum &lt;- median(sample_sums, na.rm = TRUE)\n    normalization_factors &lt;- global_sum / sample_sums\n    \n    for (i in 1:nrow(intensity_matrix)) {\n      normalized_matrix[i, ] &lt;- intensity_matrix[i, ] * normalization_factors[i]\n    }\n    \n  } else if (method == \"quantile\") {\n    # Quantile normalization (simplified)\n    ranked_data &lt;- apply(intensity_matrix, 2, rank, na.rm = TRUE, ties.method = \"average\")\n    sorted_means &lt;- apply(apply(intensity_matrix, 2, sort, na.rm = TRUE), 1, mean, na.rm = TRUE)\n    \n    for (i in 1:ncol(intensity_matrix)) {\n      normalized_matrix[, i] &lt;- sorted_means[ranked_data[, i]]\n    }\n  }\n  \n  return(normalized_matrix)\n}\n\n# Apply different normalization methods\nmedian_normalized &lt;- normalize_data(filtered_data$intensity_matrix, \"median\")\ntic_normalized &lt;- normalize_data(filtered_data$intensity_matrix, \"tic\")\n\n# Compare normalization effects\ncompare_normalization &lt;- function(original, normalized, method_name) {\n  original_cv &lt;- apply(original, 1, function(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) * 100)\n  normalized_cv &lt;- apply(normalized, 1, function(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) * 100)\n  \n  comparison_df &lt;- data.frame(\n    sample = rownames(original),\n    original_cv = original_cv,\n    normalized_cv = normalized_cv,\n    method = method_name\n  )\n  \n  return(comparison_df)\n}\n\nnormalization_comparison &lt;- rbind(\n  compare_normalization(filtered_data$intensity_matrix, median_normalized, \"Median\"),\n  compare_normalization(filtered_data$intensity_matrix, tic_normalized, \"TIC\")\n)\n\n# Visualize normalization effects\nggplot(normalization_comparison, aes(x = original_cv, y = normalized_cv, color = method)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\") +\n  facet_wrap(~method) +\n  labs(title = \"Effect of Normalization on Sample CV\",\n       x = \"Original CV (%)\", y = \"Normalized CV (%)\",\n       color = \"Method\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n7.4.2 Data Scaling\n\n# Different scaling methods\nscale_data &lt;- function(normalized_matrix, method = \"auto\") {\n  scaled_matrix &lt;- normalized_matrix\n  \n  if (method == \"auto\") {\n    # Auto-scaling (mean-centering + unit variance)\n    scaled_matrix &lt;- scale(normalized_matrix)\n    \n  } else if (method == \"pareto\") {\n    # Pareto scaling (mean-centering + square root of standard deviation)\n    means &lt;- apply(normalized_matrix, 2, mean, na.rm = TRUE)\n    sds &lt;- apply(normalized_matrix, 2, sd, na.rm = TRUE)\n    \n    for (i in 1:ncol(normalized_matrix)) {\n      scaled_matrix[, i] &lt;- (normalized_matrix[, i] - means[i]) / sqrt(sds[i])\n    }\n    \n  } else if (method == \"range\") {\n    # Range scaling (0-1 normalization)\n    for (i in 1:ncol(normalized_matrix)) {\n      min_val &lt;- min(normalized_matrix[, i], na.rm = TRUE)\n      max_val &lt;- max(normalized_matrix[, i], na.rm = TRUE)\n      scaled_matrix[, i] &lt;- (normalized_matrix[, i] - min_val) / (max_val - min_val)\n    }\n  }\n  \n  return(scaled_matrix)\n}\n\n# Apply different scaling methods\nauto_scaled &lt;- scale_data(median_normalized, \"auto\")\npareto_scaled &lt;- scale_data(median_normalized, \"pareto\")\nrange_scaled &lt;- scale_data(median_normalized, \"range\")\n\n# Visualize scaling effects\nscaling_comparison &lt;- data.frame(\n  feature = rep(colnames(median_normalized), 3),\n  variance = c(\n    apply(auto_scaled, 2, var, na.rm = TRUE),\n    apply(pareto_scaled, 2, var, na.rm = TRUE),\n    apply(range_scaled, 2, var, na.rm = TRUE)\n  ),\n  method = rep(c(\"Auto\", \"Pareto\", \"Range\"), each = ncol(median_normalized))\n)\n\nggplot(scaling_comparison, aes(x = method, y = variance, fill = method)) +\n  geom_boxplot(alpha = 0.7) +\n  scale_y_log10() +\n  labs(title = \"Variance Distribution by Scaling Method\",\n       x = \"Scaling Method\", y = \"Variance (log10 scale)\",\n       fill = \"Method\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-metabolomics-analysis.html#multivariate-analysis-for-metabolomics",
    "href": "07-metabolomics-analysis.html#multivariate-analysis-for-metabolomics",
    "title": "7  Metabolomics Data Analysis",
    "section": "7.5 Multivariate Analysis for Metabolomics",
    "text": "7.5 Multivariate Analysis for Metabolomics\n\n7.5.1 Principal Component Analysis (PCA)\n\n# Perform PCA on scaled data\nperform_metabolomics_pca &lt;- function(scaled_matrix, sample_info) {\n  # Remove any NA values\n  clean_matrix &lt;- scaled_matrix[complete.cases(scaled_matrix), ]\n  \n  # Perform PCA\n  pca_result &lt;- prcomp(clean_matrix, center = FALSE, scale. = FALSE)\n  \n  # Extract scores\n  pca_scores &lt;- data.frame(pca_result$x) %&gt;%\n    mutate(\n      sample_name = rownames(clean_matrix),\n      group = sample_info$group[match(rownames(clean_matrix), sample_info$sample_name)],\n      batch = factor(sample_info$batch[match(rownames(clean_matrix), sample_info$sample_name)])\n    )\n  \n  # Calculate variance explained\n  var_explained &lt;- (pca_result$sdev^2 / sum(pca_result$sdev^2)) * 100\n  \n  return(list(\n    pca_result = pca_result,\n    scores = pca_scores,\n    variance_explained = var_explained\n  ))\n}\n\npca_analysis &lt;- perform_metabolomics_pca(auto_scaled, sample_info)\n\n# PCA scores plot\nggplot(pca_analysis$scores, aes(x = PC1, y = PC2, color = group, shape = batch)) +\n  geom_point(size = 3, alpha = 0.8) +\n  stat_ellipse(aes(group = group), alpha = 0.3) +\n  labs(title = \"PCA Scores Plot - Metabolomics Data\",\n       x = paste0(\"PC1 (\", round(pca_analysis$variance_explained[1], 1), \"%)\"),\n       y = paste0(\"PC2 (\", round(pca_analysis$variance_explained[2], 1), \"%)\"),\n       color = \"Group\", shape = \"Batch\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n7.5.2 Partial Least Squares Discriminant Analysis (PLS-DA)\n\n# Perform PLS-DA using mixOmics (if available)\nif (requireNamespace(\"mixOmics\", quietly = TRUE)) {\n  perform_plsda &lt;- function(scaled_matrix, groups) {\n    # Clean data\n    clean_matrix &lt;- scaled_matrix[complete.cases(scaled_matrix), ]\n    clean_groups &lt;- groups[complete.cases(scaled_matrix)]\n    \n    # Perform PLS-DA\n    plsda_result &lt;- mixOmics::plsda(clean_matrix, clean_groups, ncomp = 3)\n    \n    return(plsda_result)\n  }\n  \n  plsda_analysis &lt;- perform_plsda(auto_scaled, sample_info$group)\n  \n  # Extract PLS-DA scores\n  plsda_scores &lt;- data.frame(plsda_analysis$variates$X) %&gt;%\n    mutate(\n      sample_name = rownames(auto_scaled)[complete.cases(auto_scaled)],\n      group = sample_info$group[complete.cases(auto_scaled)]\n    )\n  \n  # PLS-DA scores plot\n  p &lt;- ggplot(plsda_scores, aes(x = comp1, y = comp2, color = group)) +\n    geom_point(size = 3, alpha = 0.8) +\n    stat_ellipse(alpha = 0.3) +\n    labs(title = \"PLS-DA Scores Plot\",\n         x = \"Component 1\", y = \"Component 2\",\n         color = \"Group\") +\n    theme_minimal()\n  print(p)\n} else {\n  cat(\"Note: mixOmics package not available. Skipping PLS-DA analysis.\\n\")\n  cat(\"Install with: BiocManager::install('mixOmics')\\n\")\n}\n\nNote: mixOmics package not available. Skipping PLS-DA analysis.\nInstall with: BiocManager::install('mixOmics')\n\n\n\n\n7.5.3 Variable Importance Analysis\n\n# Calculate VIP scores (if mixOmics is available and PLS-DA was run)\nif (requireNamespace(\"mixOmics\", quietly = TRUE) && exists(\"plsda_analysis\")) {\n  calculate_vip &lt;- function(plsda_result) {\n    vip_scores &lt;- mixOmics::vip(plsda_result)\n    \n    vip_data &lt;- data.frame(\n      feature = rownames(vip_scores),\n      vip_comp1 = vip_scores[, 1],\n      vip_comp2 = if(ncol(vip_scores) &gt; 1) vip_scores[, 2] else NA\n    )\n    \n    return(vip_data)\n  }\n  \n  vip_scores &lt;- calculate_vip(plsda_analysis)\n  \n  # Plot VIP scores\n  p &lt;- ggplot(vip_scores, aes(x = reorder(feature, vip_comp1), y = vip_comp1)) +\n    geom_bar(stat = \"identity\", fill = \"steelblue\", alpha = 0.7) +\n    geom_hline(yintercept = 1, color = \"red\", linetype = \"dashed\") +\n    coord_flip() +\n    labs(title = \"Variable Importance in Projection (VIP) Scores\",\n         subtitle = \"Red line indicates VIP &gt; 1 threshold\",\n         x = \"Features\", y = \"VIP Score (Component 1)\") +\n    theme_minimal() +\n    theme(axis.text.y = element_text(size = 6))\n  print(p)\n} else {\n  cat(\"Note: VIP scores require mixOmics package and successful PLS-DA analysis.\\n\")\n  # Create empty vip_scores object for downstream code\n  vip_scores &lt;- NULL\n}\n\nNote: VIP scores require mixOmics package and successful PLS-DA analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-metabolomics-analysis.html#metabolite-identification",
    "href": "07-metabolomics-analysis.html#metabolite-identification",
    "title": "7  Metabolomics Data Analysis",
    "section": "7.6 Metabolite Identification",
    "text": "7.6 Metabolite Identification\n\n7.6.1 Accurate Mass Matching\n\n# Function for accurate mass matching\naccurate_mass_search &lt;- function(query_mz, mass_database, tolerance_ppm = 5) {\n  # Create a simple mass database (normally you'd use real databases)\n  if (missing(mass_database)) {\n    mass_database &lt;- data.frame(\n      compound_name = c(\"Glucose\", \"Fructose\", \"Sucrose\", \"Lactose\", \"Maltose\",\n                       \"Alanine\", \"Glycine\", \"Serine\", \"Threonine\", \"Valine\"),\n      exact_mass = c(180.0634, 180.0634, 342.1162, 342.1162, 342.1162,\n                    89.0477, 75.0320, 105.0426, 119.0582, 117.0790),\n      formula = c(\"C6H12O6\", \"C6H12O6\", \"C12H22O11\", \"C12H22O11\", \"C12H22O11\",\n                 \"C3H7NO2\", \"C2H5NO2\", \"C3H7NO3\", \"C4H9NO3\", \"C5H11NO2\")\n    )\n  }\n  \n  matches &lt;- data.frame()\n  \n  for (i in seq_along(query_mz)) {\n    # Calculate mass differences\n    mass_diff_ppm &lt;- abs(mass_database$exact_mass - query_mz[i]) / query_mz[i] * 1e6\n    \n    # Find matches within tolerance\n    match_idx &lt;- which(mass_diff_ppm &lt;= tolerance_ppm)\n    \n    if (length(match_idx) &gt; 0) {\n      for (j in match_idx) {\n        matches &lt;- rbind(matches, data.frame(\n          query_mz = query_mz[i],\n          matched_compound = mass_database$compound_name[j],\n          exact_mass = mass_database$exact_mass[j],\n          formula = mass_database$formula[j],\n          mass_error_ppm = mass_diff_ppm[j]\n        ))\n      }\n    }\n  }\n  \n  return(matches)\n}\n\n# Perform mass matching for significant features\nif (!is.null(vip_scores)) {\n  significant_features &lt;- vip_scores$feature[vip_scores$vip_comp1 &gt; 1]\n} else {\n  # Use alternative method to identify significant features\n  # Based on highest variance across samples\n  feature_variance &lt;- apply(filtered_data$metabolite_data[, -1], 1, var, na.rm = TRUE)\n  top_features_idx &lt;- order(feature_variance, decreasing = TRUE)[1:min(10, length(feature_variance))]\n  significant_features &lt;- filtered_data$metabolite_data$metabolite_id[top_features_idx]\n}\n\nif (length(significant_features) &gt; 0) {\n  # Extract m/z values for significant features\n  significant_mz &lt;- filtered_data$metabolite_data$mz[\n    filtered_data$metabolite_data$metabolite_id %in% significant_features\n  ]\n  \n  mass_matches &lt;- accurate_mass_search(significant_mz)\n  \n  if (nrow(mass_matches) &gt; 0) {\n    cat(\"Potential metabolite identifications:\\n\")\n    print(mass_matches)\n  } else {\n    cat(\"No matches found in database\\n\")\n  }\n}\n\nNo matches found in database\n\n\n\n\n7.6.2 MS/MS Spectrum Matching\n\n# Simulate MS/MS data for demonstration\nsimulate_msms_spectrum &lt;- function(precursor_mz, intensity = 1000) {\n  # Generate realistic fragment ions\n  n_fragments &lt;- sample(5:15, 1)\n  \n  # Common neutral losses for metabolites\n  neutral_losses &lt;- c(18.01, 28.01, 44.00, 46.00, 60.02, 62.00)\n  \n  fragment_mz &lt;- numeric(n_fragments)\n  fragment_intensity &lt;- numeric(n_fragments)\n  \n  for (i in 1:n_fragments) {\n    if (i &lt;= length(neutral_losses) && runif(1) &gt; 0.3) {\n      # Use neutral loss\n      fragment_mz[i] &lt;- precursor_mz - neutral_losses[i]\n    } else {\n      # Random fragment\n      fragment_mz[i] &lt;- runif(1, 50, precursor_mz - 1)\n    }\n    \n    # Random intensity\n    fragment_intensity[i] &lt;- runif(1, 0.1, 1) * intensity\n  }\n  \n  # Sort by m/z\n  order_idx &lt;- order(fragment_mz)\n  \n  return(data.frame(\n    mz = fragment_mz[order_idx],\n    intensity = fragment_intensity[order_idx]\n  ))\n}\n\n# Create example MS/MS spectra\nexample_msms &lt;- simulate_msms_spectrum(180.0634)  # Glucose-like spectrum\n\n# Plot MS/MS spectrum\nggplot(example_msms, aes(x = mz, y = intensity)) +\n  geom_segment(aes(xend = mz, yend = 0), color = \"blue\", size = 1) +\n  geom_point(color = \"red\", size = 2) +\n  labs(title = \"Simulated MS/MS Spectrum\",\n       x = \"m/z\", y = \"Relative Intensity\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-metabolomics-analysis.html#pathway-analysis",
    "href": "07-metabolomics-analysis.html#pathway-analysis",
    "title": "7  Metabolomics Data Analysis",
    "section": "7.7 Pathway Analysis",
    "text": "7.7 Pathway Analysis\n\n7.7.1 Metabolite Set Enrichment\n\n# Simulate pathway database\ncreate_pathway_database &lt;- function() {\n  pathways &lt;- list(\n    \"Glycolysis\" = c(\"Met_1\", \"Met_3\", \"Met_5\", \"Met_7\", \"Met_9\"),\n    \"TCA_Cycle\" = c(\"Met_2\", \"Met_4\", \"Met_6\", \"Met_8\", \"Met_10\"),\n    \"Amino_Acid_Metabolism\" = c(\"Met_11\", \"Met_12\", \"Met_13\", \"Met_14\", \"Met_15\"),\n    \"Lipid_Metabolism\" = c(\"Met_16\", \"Met_17\", \"Met_18\", \"Met_19\", \"Met_20\"),\n    \"Nucleotide_Metabolism\" = c(\"Met_21\", \"Met_22\", \"Met_23\", \"Met_24\", \"Met_25\")\n  )\n  \n  return(pathways)\n}\n\n# Perform pathway enrichment\npathway_enrichment &lt;- function(significant_metabolites, pathway_db, total_metabolites) {\n  enrichment_results &lt;- data.frame()\n  \n  for (pathway_name in names(pathway_db)) {\n    pathway_metabolites &lt;- pathway_db[[pathway_name]]\n    \n    # Calculate overlap\n    overlap &lt;- intersect(significant_metabolites, pathway_metabolites)\n    \n    # Calculate contingency table values\n    a &lt;- length(overlap)  # significant & in pathway\n    b &lt;- length(significant_metabolites) - length(overlap)  # significant & not in pathway\n    c &lt;- length(pathway_metabolites) - length(overlap)  # not significant & in pathway\n    d &lt;- total_metabolites - length(significant_metabolites) - \n         length(pathway_metabolites) + length(overlap)  # not significant & not in pathway\n    \n    # Ensure all values are non-negative\n    if (d &lt; 0 || any(c(a, b, c, d) &lt; 0)) {\n      # Skip this pathway if contingency table is invalid\n      next\n    }\n    \n    # Fisher's exact test\n    contingency_table &lt;- matrix(c(a, b, c, d), nrow = 2)\n    \n    fisher_test &lt;- tryCatch({\n      fisher.test(contingency_table, alternative = \"greater\")\n    }, error = function(e) {\n      list(p.value = 1, estimate = 1)\n    })\n    \n    enrichment_results &lt;- rbind(enrichment_results, data.frame(\n      pathway = pathway_name,\n      overlap_size = length(overlap),\n      pathway_size = length(pathway_metabolites),\n      significant_size = length(significant_metabolites),\n      p_value = fisher_test$p.value,\n      odds_ratio = as.numeric(fisher_test$estimate)\n    ))\n  }\n  \n  # Multiple testing correction\n  if (nrow(enrichment_results) &gt; 0) {\n    enrichment_results$p_adjusted &lt;- p.adjust(enrichment_results$p_value, method = \"fdr\")\n    return(enrichment_results[order(enrichment_results$p_value), ])\n  } else {\n    return(data.frame(\n      pathway = character(0),\n      overlap_size = integer(0),\n      pathway_size = integer(0),\n      significant_size = integer(0),\n      p_value = numeric(0),\n      odds_ratio = numeric(0),\n      p_adjusted = numeric(0)\n    ))\n  }\n}\n\n# Perform enrichment analysis\npathway_db &lt;- create_pathway_database()\nsignificant_metabolites &lt;- significant_features\n\nenrichment_results &lt;- pathway_enrichment(\n  significant_metabolites, \n  pathway_db, \n  ncol(filtered_data$intensity_matrix)\n)\n\nif (nrow(enrichment_results) &gt; 0) {\n  print(enrichment_results)\n  \n  # Visualize enrichment results\n  p &lt;- ggplot(enrichment_results, aes(x = reorder(pathway, -log10(p_value)), \n                                y = -log10(p_value))) +\n    geom_bar(stat = \"identity\", fill = \"coral\", alpha = 0.7) +\n    geom_hline(yintercept = -log10(0.05), linetype = \"dashed\", color = \"red\") +\n    coord_flip() +\n    labs(title = \"Pathway Enrichment Analysis\",\n         x = \"Pathway\", y = \"-log10(P-value)\") +\n    theme_minimal()\n  print(p)\n} else {\n  cat(\"No significant pathway enrichment found.\\n\")\n}\n\nNo significant pathway enrichment found.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-metabolomics-analysis.html#quality-control-and-batch-correction",
    "href": "07-metabolomics-analysis.html#quality-control-and-batch-correction",
    "title": "7  Metabolomics Data Analysis",
    "section": "7.8 Quality Control and Batch Correction",
    "text": "7.8 Quality Control and Batch Correction\n\n7.8.1 QC Sample Analysis\n\n# Simulate QC samples\nsimulate_qc_samples &lt;- function(original_matrix, n_qc = 5) {\n  # QC samples are typically pooled samples with intermediate values\n  qc_matrix &lt;- matrix(nrow = n_qc, ncol = ncol(original_matrix))\n  rownames(qc_matrix) &lt;- paste0(\"QC_\", 1:n_qc)\n  colnames(qc_matrix) &lt;- colnames(original_matrix)\n  \n  for (i in 1:n_qc) {\n    for (j in 1:ncol(original_matrix)) {\n      # QC intensity as mean of original samples with some variation\n      qc_matrix[i, j] &lt;- mean(original_matrix[, j], na.rm = TRUE) * \n                         rlnorm(1, 0, 0.1)  # 10% variation\n    }\n  }\n  \n  return(qc_matrix)\n}\n\nqc_matrix &lt;- simulate_qc_samples(filtered_data$intensity_matrix)\n\n# Calculate QC stability\ncalculate_qc_stability &lt;- function(qc_matrix) {\n  qc_cv &lt;- apply(qc_matrix, 2, function(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) * 100)\n  \n  stability_summary &lt;- data.frame(\n    feature = names(qc_cv),\n    qc_cv = qc_cv,\n    stable = qc_cv &lt; 20  # 20% CV threshold for stability\n  )\n  \n  return(stability_summary)\n}\n\nqc_stability &lt;- calculate_qc_stability(qc_matrix)\n\n# Visualize QC stability\nggplot(qc_stability, aes(x = qc_cv, fill = stable)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  scale_fill_manual(values = c(\"TRUE\" = \"green\", \"FALSE\" = \"red\")) +\n  geom_vline(xintercept = 20, linetype = \"dashed\", color = \"black\") +\n  labs(title = \"QC Sample Stability Assessment\",\n       x = \"QC CV (%)\", y = \"Count\",\n       fill = \"Stable (CV &lt; 20%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\ncat(\"Percentage of stable features:\", \n    round(sum(qc_stability$stable) / nrow(qc_stability) * 100, 1), \"%\\n\")\n\nPercentage of stable features: 100 %",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-metabolomics-analysis.html#exercises",
    "href": "07-metabolomics-analysis.html#exercises",
    "title": "7  Metabolomics Data Analysis",
    "section": "7.9 Exercises",
    "text": "7.9 Exercises\n\nProcess real metabolomics data using XCMS\nCompare different normalization methods on your dataset\nImplement a complete identification workflow using spectral databases\nPerform time-course metabolomics analysis\nBuild a metabolomics data processing pipeline with quality control",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-metabolomics-analysis.html#summary",
    "href": "07-metabolomics-analysis.html#summary",
    "title": "7  Metabolomics Data Analysis",
    "section": "7.10 Summary",
    "text": "7.10 Summary\nThis chapter covered comprehensive metabolomics data analysis workflows, including peak detection, data normalization, multivariate analysis, metabolite identification, and pathway analysis. These methods form the foundation for extracting biological insights from metabolomics experiments.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-proteomics-analysis.html",
    "href": "08-proteomics-analysis.html",
    "title": "8  Proteomics Data Analysis",
    "section": "",
    "text": "8.1 Setting Up Proteomics Environment\nProteomics focuses on the large-scale study of proteins, including their identification, quantification, and functional analysis. This chapter covers computational methods for bottom-up proteomics data analysis using the R for Mass Spectrometry ecosystem.\nThe R for Mass Spectrometry ecosystem provides specialized packages for proteomics analysis:\nlibrary(Spectra)          # Core MS data structures\nlibrary(PSMatch)          # Peptide-spectrum matching\nlibrary(ProtGenerics)     # Generic functions for proteomics\nlibrary(QFeatures)        # Quantitative features handling\nlibrary(msdata)           # Example MS data\nlibrary(mzR)             # Reading raw MS data\nlibrary(dplyr)           # Data manipulation\nlibrary(ggplot2)         # Visualization\nlibrary(pheatmap)        # Heatmaps\nlibrary(limma)           # Statistical analysis\nlibrary(tidyverse)       # Data science tools\n# Load proteomics test data from msdata\nproteomics_files &lt;- msdata::proteomics(full.names = TRUE)\ncat(\"Available proteomics files:\\n\")\n\nAvailable proteomics files:\n\nfor (i in seq_along(proteomics_files)) {\n  cat(i, \":\", basename(proteomics_files[i]), \"\\n\")\n}\n\n1 : MRM-standmix-5.mzML.gz \n2 : MS3TMT10_01022016_32917-33481.mzML.gz \n3 : MS3TMT11.mzML \n4 : TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz \n5 : TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01.mzML.gz \n\n# Select a file for analysis\nselected_file &lt;- proteomics_files[1]\ncat(\"\\nSelected file:\", basename(selected_file), \"\\n\")\n\n\nSelected file: MRM-standmix-5.mzML.gz",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-proteomics-analysis.html#understanding-proteomics-workflows",
    "href": "08-proteomics-analysis.html#understanding-proteomics-workflows",
    "title": "8  Proteomics Data Analysis",
    "section": "8.2 Understanding Proteomics Workflows",
    "text": "8.2 Understanding Proteomics Workflows\n\n8.2.1 Bottom-up Proteomics Pipeline\nThe typical bottom-up proteomics workflow involves:\n\nSample preparation: Protein extraction, digestion (usually with trypsin)\nLC-MS/MS analysis: Liquid chromatography coupled to tandem mass spectrometry\nDatabase searching: Matching MS/MS spectra to peptide sequences\nProtein inference: Assembling peptides into protein identifications\nQuantitative analysis: Comparing protein abundances across samples\n\n\n\n8.2.2 Data Structures in Proteomics\nProteomics data has a hierarchical structure: - Spectra: Raw MS and MS/MS data - PSMs: Peptide-Spectrum Matches from database search - Peptides: Unique peptide sequences - Proteins: Protein groups inferred from peptides",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-proteomics-analysis.html#msms-spectral-data-processing",
    "href": "08-proteomics-analysis.html#msms-spectral-data-processing",
    "title": "8  Proteomics Data Analysis",
    "section": "8.3 MS/MS Spectral Data Processing",
    "text": "8.3 MS/MS Spectral Data Processing\n\n8.3.1 Loading and Examining MS/MS Data\n\n# Load MS/MS data with error handling\ntryCatch({\n  ms_data &lt;- Spectra(selected_file, backend = MsBackendMzR())\n  ms_data &lt;- setBackend(ms_data, backend = MsBackendDataFrame())\n  cat(\"Successfully loaded real MS data\\n\")\n}, error = function(e) {\n  cat(\"Note: Using synthetic data due to mzR compatibility issues\\n\")\n  cat(\"Error details:\", conditionMessage(e), \"\\n\\n\")\n  \n  # Create synthetic MS/MS data\n  set.seed(456)\n  n_spectra &lt;- 200\n  \n  library(S4Vectors)\n  library(IRanges)\n  \n  # Generate peak data\n  mz_list &lt;- lapply(1:n_spectra, function(i) {\n    sort(runif(sample(40:120, 1), 200, 1800))\n  })\n  \n  intensity_list &lt;- lapply(mz_list, function(mz_vals) {\n    rlnorm(length(mz_vals), meanlog = 7, sdlog = 2)\n  })\n  \n  # Generate MS levels (80% MS2, 20% MS1)\n  ms_levels &lt;- sample(1:2, n_spectra, replace = TRUE, prob = c(0.2, 0.8))\n  \n  # Create metadata DataFrame\n  spd &lt;- DataFrame(\n    msLevel = ms_levels,\n    rtime = seq(100, 4500, length.out = n_spectra),\n    acquisitionNum = 1:n_spectra,\n    precursorMz = ifelse(ms_levels == 1, NA_real_, runif(n_spectra, 400, 1600)),\n    precursorCharge = ifelse(ms_levels == 1, NA_integer_, sample(2:4, n_spectra, replace = TRUE)),\n    precursorIntensity = ifelse(ms_levels == 1, NA_real_, rlnorm(n_spectra, meanlog = 10, sdlog = 1.5)),\n    collisionEnergy = ifelse(ms_levels == 1, NA_real_, runif(n_spectra, 25, 45)),\n    polarity = rep(1L, n_spectra)\n  )\n  \n  # Add peak data as list columns\n  spd$mz &lt;- NumericList(mz_list)\n  spd$intensity &lt;- NumericList(intensity_list)\n  \n  # Initialize backend and create Spectra object\n  backend &lt;- MsBackendDataFrame()\n  backend &lt;- backendInitialize(backend, spd)\n  ms_data &lt;&lt;- Spectra(backend)\n})\n\nNote: Using synthetic data due to mzR compatibility issues\nError details: BiocParallel errors\n  1 remote errors, element index: 1\n  0 unevaluated and other errors\n  first remote error:\nError in DataFrame(..., check.names = FALSE): different row counts implied by arguments\n \n\n# Basic information about the data\ncat(\"\\nDataset summary:\\n\")\n\n\nDataset summary:\n\ncat(\"Total spectra:\", length(ms_data), \"\\n\")\n\nTotal spectra: 200 \n\ncat(\"MS levels:\", paste(unique(msLevel(ms_data)), collapse = \", \"), \"\\n\")\n\nMS levels: 2, 1 \n\ncat(\"Scan range:\", range(acquisitionNum(ms_data)), \"\\n\")\n\nScan range: 1 200 \n\ncat(\"RT range:\", round(range(rtime(ms_data)), 2), \"seconds\\n\")\n\nRT range: 100 4500 seconds\n\n# MS2 spectra information\nms2_data &lt;- filterMsLevel(ms_data, msLevel = 2)\ncat(\"\\nMS2 spectra:\", length(ms2_data), \"\\n\")\n\n\nMS2 spectra: 164 \n\nif (length(ms2_data) &gt; 0) {\n  cat(\"Precursor m/z range:\", round(range(precursorMz(ms2_data), na.rm = TRUE), 2), \"\\n\")\n  cat(\"Charge state distribution:\\n\")\n  print(table(precursorCharge(ms2_data)))\n}\n\nPrecursor m/z range: 400.4 1595.69 \nCharge state distribution:\n\n 2  3  4 \n49 58 57 \n\n\n\n\n8.3.2 MS/MS Spectrum Quality Assessment\n\n# Function to assess MS/MS spectrum quality\nassess_ms2_quality &lt;- function(ms2_spectra) {\n  quality_metrics &lt;- data.frame(\n    spectrum_id = seq_along(ms2_spectra),\n    precursor_mz = precursorMz(ms2_spectra),\n    precursor_charge = precursorCharge(ms2_spectra),\n    precursor_intensity = precursorIntensity(ms2_spectra),\n    retention_time = rtime(ms2_spectra),\n    total_ion_current = sapply(seq_along(ms2_spectra), function(i) {\n      sum(intensity(ms2_spectra[i])[[1]], na.rm = TRUE)\n    }),\n    peak_count = sapply(seq_along(ms2_spectra), function(i) {\n      length(intensity(ms2_spectra[i])[[1]])\n    }),\n    base_peak_intensity = sapply(seq_along(ms2_spectra), function(i) {\n      max(intensity(ms2_spectra[i])[[1]], na.rm = TRUE)\n    })\n  )\n  \n  # Calculate signal-to-noise metrics\n  quality_metrics$snr_estimate &lt;- quality_metrics$base_peak_intensity / \n                                  (quality_metrics$total_ion_current / quality_metrics$peak_count)\n  \n  return(quality_metrics)\n}\n\n# Assess quality for first 100 MS2 spectra\nms2_quality &lt;- assess_ms2_quality(ms2_data[1:min(100, length(ms2_data))])\n\n# Visualize quality metrics\nquality_plots &lt;- list()\n\nquality_plots[[1]] &lt;- ggplot(ms2_quality, aes(x = peak_count)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", alpha = 0.7) +\n  labs(title = \"Distribution of Peak Counts\", x = \"Peak Count\", y = \"Frequency\") +\n  theme_minimal()\n\nquality_plots[[2]] &lt;- ggplot(ms2_quality, aes(x = precursor_charge, y = peak_count)) +\n  geom_boxplot(aes(group = precursor_charge), fill = \"lightcoral\", alpha = 0.7) +\n  labs(title = \"Peak Count vs Charge State\", x = \"Charge State\", y = \"Peak Count\") +\n  theme_minimal()\n\n# Print plots\nprint(quality_plots[[1]])\n\n\n\n\n\n\n\nprint(quality_plots[[2]])\n\n\n\n\n\n\n\n\n\n\n8.3.3 Spectrum Preprocessing\n\n# Function to preprocess MS/MS spectra\npreprocess_ms2_spectrum &lt;- function(spectrum_obj, \n                                   min_intensity = 100,\n                                   top_n_peaks = 150,\n                                   remove_precursor = TRUE,\n                                   precursor_tolerance = 1.5) {\n  \n  processed_spectra &lt;- list()\n  \n  for (i in seq_along(spectrum_obj)) {\n    mz_vals &lt;- mz(spectrum_obj[i])[[1]]\n    int_vals &lt;- intensity(spectrum_obj[i])[[1]]\n    precursor_mz_val &lt;- precursorMz(spectrum_obj[i])\n    \n    if (length(mz_vals) == 0 || length(int_vals) == 0) {\n      next\n    }\n    \n    # Remove low-intensity peaks\n    intensity_filter &lt;- int_vals &gt;= min_intensity\n    mz_vals &lt;- mz_vals[intensity_filter]\n    int_vals &lt;- int_vals[intensity_filter]\n    \n    # Remove precursor ion if requested\n    if (remove_precursor && !is.na(precursor_mz_val)) {\n      precursor_filter &lt;- abs(mz_vals - precursor_mz_val) &gt; precursor_tolerance\n      mz_vals &lt;- mz_vals[precursor_filter]\n      int_vals &lt;- int_vals[precursor_filter]\n    }\n    \n    # Keep only top N peaks\n    if (length(int_vals) &gt; top_n_peaks) {\n      top_indices &lt;- order(int_vals, decreasing = TRUE)[1:top_n_peaks]\n      mz_vals &lt;- mz_vals[top_indices]\n      int_vals &lt;- int_vals[top_indices]\n      \n      # Re-order by m/z\n      order_indices &lt;- order(mz_vals)\n      mz_vals &lt;- mz_vals[order_indices]\n      int_vals &lt;- int_vals[order_indices]\n    }\n    \n    # Normalize intensities\n    int_vals &lt;- int_vals / max(int_vals) * 100\n    \n    processed_spectra[[i]] &lt;- list(\n      spectrum_index = i,\n      mz = mz_vals,\n      intensity = int_vals,\n      precursor_mz = precursor_mz_val,\n      precursor_charge = precursorCharge(spectrum_obj[i]),\n      retention_time = rtime(spectrum_obj[i]),\n      original_peak_count = length(intensity(spectrum_obj[i])[[1]]),\n      processed_peak_count = length(int_vals)\n    )\n  }\n  \n  return(processed_spectra)\n}\n\n# Preprocess first 50 MS2 spectra\nprocessed_ms2 &lt;- preprocess_ms2_spectrum(ms2_data[1:50])\n\n# Remove NULL entries\nprocessed_ms2 &lt;- processed_ms2[!sapply(processed_ms2, is.null)]\n\ncat(\"Processed\", length(processed_ms2), \"MS/MS spectra\\n\")\n\nProcessed 50 MS/MS spectra\n\n# Example: visualize a processed spectrum\nif (length(processed_ms2) &gt; 0) {\n  example_spectrum &lt;- processed_ms2[[1]]\n  \n  spectrum_df &lt;- data.frame(\n    mz = example_spectrum$mz,\n    intensity = example_spectrum$intensity\n  )\n  \n  ggplot(spectrum_df, aes(x = mz, y = intensity)) +\n    geom_segment(aes(xend = mz, yend = 0), color = \"blue\", alpha = 0.7) +\n    labs(title = paste(\"Processed MS/MS Spectrum - Precursor m/z:\", \n                      round(example_spectrum$precursor_mz, 2)),\n         x = \"m/z\", y = \"Relative Intensity (%)\") +\n    theme_minimal()\n}",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-proteomics-analysis.html#protein-identification",
    "href": "08-proteomics-analysis.html#protein-identification",
    "title": "8  Proteomics Data Analysis",
    "section": "8.4 Protein Identification",
    "text": "8.4 Protein Identification\n\n8.4.1 Peptide Spectral Matching\n\n# Simulate protein database and peptide identification results\ncreate_protein_database &lt;- function() {\n  # Create a simplified protein database\n  proteins &lt;- data.frame(\n    protein_id = paste0(\"PROT_\", 1:100),\n    protein_name = paste0(\"Protein_\", 1:100),\n    gene_name = paste0(\"GENE_\", 1:100),\n    organism = \"Homo sapiens\",\n    sequence_length = sample(100:2000, 100),\n    stringsAsFactors = FALSE\n  )\n  \n  # Generate theoretical peptides for each protein\n  peptide_db &lt;- data.frame()\n  peptide_counter &lt;- 1\n  \n  for (i in 1:nrow(proteins)) {\n    # Simulate 5-15 peptides per protein\n    n_peptides &lt;- sample(5:15, 1)\n    \n    for (j in 1:n_peptides) {\n      # Generate random peptide sequence (simplified)\n      aa_codes &lt;- c(\"A\", \"R\", \"N\", \"D\", \"C\", \"E\", \"Q\", \"G\", \"H\", \"I\", \n                   \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\")\n      peptide_length &lt;- sample(7:25, 1)\n      sequence &lt;- paste(sample(aa_codes, peptide_length, replace = TRUE), collapse = \"\")\n      \n      # Calculate theoretical m/z (simplified calculation)\n      theoretical_mass &lt;- peptide_length * 110  # Rough average AA mass\n      charge &lt;- sample(2:4, 1)\n      theoretical_mz &lt;- (theoretical_mass + charge * 1.007276) / charge\n      \n      peptide_db &lt;- rbind(peptide_db, data.frame(\n        peptide_id = paste0(\"PEP_\", sprintf(\"%04d\", peptide_counter)),\n        protein_id = proteins$protein_id[i],\n        sequence = sequence,\n        theoretical_mz = theoretical_mz,\n        charge = charge,\n        peptide_counter = peptide_counter\n      ))\n      \n      peptide_counter &lt;- peptide_counter + 1\n    }\n  }\n  \n  return(list(proteins = proteins, peptides = peptide_db))\n}\n\ndb_info &lt;- create_protein_database()\ncat(\"Created database with\", nrow(db_info$proteins), \"proteins and\", \n    nrow(db_info$peptides), \"peptides\\n\")\n\nCreated database with 100 proteins and 995 peptides\n\n\n\n\n8.4.2 Simulate Peptide-Spectrum Matches (PSMs)\n\n# Simulate PSM results\nsimulate_psm_results &lt;- function(processed_spectra, peptide_db, match_probability = 0.3) {\n  psm_results &lt;- data.frame()\n  \n  for (i in seq_along(processed_spectra)) {\n    spectrum &lt;- processed_spectra[[i]]\n    \n    # Simulate whether this spectrum gets identified\n    if (runif(1) &lt; match_probability) {\n      # Find potential peptide matches based on precursor m/z\n      mz_tolerance &lt;- 0.01  # 10 ppm at m/z 1000\n      \n      potential_matches &lt;- which(\n        abs(peptide_db$theoretical_mz - spectrum$precursor_mz) &lt; mz_tolerance &\n        peptide_db$charge == spectrum$precursor_charge\n      )\n      \n      if (length(potential_matches) &gt; 0) {\n        # Select best match (random for simulation)\n        best_match &lt;- sample(potential_matches, 1)\n        \n        # Simulate scoring metrics\n        xcorr_score &lt;- runif(1, 1.5, 4.5)\n        delta_cn &lt;- runif(1, 0.1, 0.8)\n        sp_score &lt;- sample(200:800, 1)\n        mass_error_ppm &lt;- runif(1, -5, 5)\n        \n        # Calculate q-value based on score (simplified)\n        q_value &lt;- 1 / (1 + exp((xcorr_score - 2) * 3))  # Sigmoid function\n        \n        psm_results &lt;- rbind(psm_results, data.frame(\n          spectrum_index = i,\n          scan_number = i,\n          peptide_id = peptide_db$peptide_id[best_match],\n          protein_id = peptide_db$protein_id[best_match],\n          sequence = peptide_db$sequence[best_match],\n          charge = spectrum$precursor_charge,\n          theoretical_mz = peptide_db$theoretical_mz[best_match],\n          observed_mz = spectrum$precursor_mz,\n          mass_error_ppm = mass_error_ppm,\n          retention_time = spectrum$retention_time,\n          xcorr = xcorr_score,\n          delta_cn = delta_cn,\n          sp_score = sp_score,\n          q_value = q_value\n        ))\n      }\n    }\n  }\n  \n  return(psm_results)\n}\n\n# Generate PSM results\npsm_results &lt;- simulate_psm_results(processed_ms2, db_info$peptides)\ncat(\"Generated\", nrow(psm_results), \"PSMs\\n\")\n\nGenerated 0 PSMs\n\nif (nrow(psm_results) &gt; 0) {\n  head(psm_results)\n}\n\n\n\n8.4.3 PSM Quality Assessment and Filtering\n\n# Assess PSM quality\nassess_psm_quality &lt;- function(psm_data) {\n  # Quality distribution plots\n  quality_plots &lt;- list()\n  \n  # XCorr distribution\n  quality_plots[[1]] &lt;- ggplot(psm_data, aes(x = xcorr)) +\n    geom_histogram(bins = 30, fill = \"lightblue\", alpha = 0.7) +\n    labs(title = \"XCorr Score Distribution\", x = \"XCorr\", y = \"Count\") +\n    theme_minimal()\n  \n  # q-value distribution\n  quality_plots[[2]] &lt;- ggplot(psm_data, aes(x = q_value)) +\n    geom_histogram(bins = 30, fill = \"lightcoral\", alpha = 0.7) +\n    scale_x_log10() +\n    labs(title = \"Q-value Distribution\", x = \"Q-value (log10)\", y = \"Count\") +\n    theme_minimal()\n  \n  # Mass error distribution\n  quality_plots[[3]] &lt;- ggplot(psm_data, aes(x = mass_error_ppm)) +\n    geom_histogram(bins = 30, fill = \"lightgreen\", alpha = 0.7) +\n    labs(title = \"Mass Error Distribution\", x = \"Mass Error (ppm)\", y = \"Count\") +\n    theme_minimal()\n  \n  return(quality_plots)\n}\n\nif (nrow(psm_results) &gt; 0) {\n  quality_plots &lt;- assess_psm_quality(psm_results)\n  print(quality_plots[[1]])\n  print(quality_plots[[2]])\n  print(quality_plots[[3]])\n}\n\n\n\n8.4.4 PSM Filtering and FDR Control\n\n# Apply PSM filters\nfilter_psms &lt;- function(psm_data, \n                       xcorr_threshold = 2.0,\n                       qvalue_threshold = 0.01,\n                       mass_error_threshold = 10) {\n  \n  # Apply filters\n  filtered_psms &lt;- psm_data %&gt;%\n    filter(\n      xcorr &gt;= xcorr_threshold,\n      q_value &lt;= qvalue_threshold,\n      abs(mass_error_ppm) &lt;= mass_error_threshold\n    )\n  \n  cat(\"PSM filtering results:\\n\")\n  cat(\"  Original PSMs:\", nrow(psm_data), \"\\n\")\n  cat(\"  XCorr filter (&gt;=\", xcorr_threshold, \"):\", \n      sum(psm_data$xcorr &gt;= xcorr_threshold), \"\\n\")\n  cat(\"  Q-value filter (&lt;=\", qvalue_threshold, \"):\", \n      sum(psm_data$q_value &lt;= qvalue_threshold), \"\\n\")\n  cat(\"  Mass error filter (&lt;=\", mass_error_threshold, \"ppm):\", \n      sum(abs(psm_data$mass_error_ppm) &lt;= mass_error_threshold), \"\\n\")\n  cat(\"  Final filtered PSMs:\", nrow(filtered_psms), \"\\n\")\n  cat(\"  PSM-level FDR:\", round(mean(filtered_psms$q_value) * 100, 2), \"%\\n\")\n  \n  return(filtered_psms)\n}\n\nif (nrow(psm_results) &gt; 0) {\n  filtered_psms &lt;- filter_psms(psm_results)\n}",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-proteomics-analysis.html#protein-inference-and-quantification",
    "href": "08-proteomics-analysis.html#protein-inference-and-quantification",
    "title": "8  Proteomics Data Analysis",
    "section": "8.5 Protein Inference and Quantification",
    "text": "8.5 Protein Inference and Quantification\n\n8.5.1 Protein Grouping\n\n# Perform protein inference\nprotein_inference &lt;- function(filtered_psms, protein_db) {\n  if (nrow(filtered_psms) == 0) {\n    return(data.frame())\n  }\n  \n  # Group PSMs by protein\n  protein_groups &lt;- filtered_psms %&gt;%\n    group_by(protein_id) %&gt;%\n    summarise(\n      peptide_count = n_distinct(sequence),\n      psm_count = n(),\n      unique_peptide_count = n_distinct(sequence),  # Simplified - assume all peptides are unique\n      sequence_coverage = peptide_count * 10,  # Rough estimate\n      best_xcorr = max(xcorr),\n      mean_mass_error = mean(mass_error_ppm),\n      .groups = 'drop'\n    ) %&gt;%\n    filter(peptide_count &gt;= 2)  # Require at least 2 peptides\n  \n  # Add protein information\n  protein_groups &lt;- protein_groups %&gt;%\n    left_join(protein_db, by = \"protein_id\")\n  \n  return(protein_groups)\n}\n\nif (exists(\"filtered_psms\") && nrow(filtered_psms) &gt; 0) {\n  protein_groups &lt;- protein_inference(filtered_psms, db_info$proteins)\n  cat(\"Identified\", nrow(protein_groups), \"protein groups\\n\")\n  \n  if (nrow(protein_groups) &gt; 0) {\n    head(protein_groups)\n  }\n}\n\n\n\n8.5.2 Label-Free Quantification\n\n# Simulate label-free quantification data\nsimulate_lfq_data &lt;- function(protein_groups, n_samples = 12) {\n  if (nrow(protein_groups) == 0) {\n    return(list())\n  }\n  \n  # Create sample information\n  sample_info &lt;- data.frame(\n    sample_id = paste0(\"Sample_\", 1:n_samples),\n    condition = rep(c(\"Control\", \"Treatment\"), each = n_samples/2),\n    batch = rep(1:3, each = n_samples/3),\n    injection_order = 1:n_samples\n  )\n  \n  # Create intensity matrix\n  intensity_matrix &lt;- matrix(0, nrow = nrow(protein_groups), ncol = n_samples)\n  rownames(intensity_matrix) &lt;- protein_groups$protein_id\n  colnames(intensity_matrix) &lt;- sample_info$sample_id\n  \n  # Simulate protein abundances\n  for (i in 1:nrow(protein_groups)) {\n    base_abundance &lt;- rlnorm(1, meanlog = 20, sdlog = 2)\n    \n    for (j in 1:n_samples) {\n      # Add condition effect for some proteins\n      condition_effect &lt;- ifelse(sample_info$condition[j] == \"Treatment\" & \n                                i &lt;= nrow(protein_groups) * 0.2, \n                                log2(1.5), 0)  # 1.5-fold change for 20% of proteins\n      \n      # Add batch effect\n      batch_effect &lt;- rnorm(1, 0, 0.1) * sample_info$batch[j]\n      \n      # Add biological and technical variation\n      log_intensity &lt;- log2(base_abundance) + condition_effect + batch_effect + rnorm(1, 0, 0.3)\n      \n      # Convert back to linear scale with some probability of missing values\n      if (runif(1) &gt; 0.1) {  # 90% detection rate\n        intensity_matrix[i, j] &lt;- 2^log_intensity\n      }\n    }\n  }\n  \n  # Convert zero values to NA\n  intensity_matrix[intensity_matrix == 0] &lt;- NA\n  \n  return(list(\n    intensity_matrix = intensity_matrix,\n    sample_info = sample_info,\n    protein_info = protein_groups\n  ))\n}\n\nif (exists(\"protein_groups\") && nrow(protein_groups) &gt; 0) {\n  lfq_data &lt;- simulate_lfq_data(protein_groups)\n  \n  cat(\"Created LFQ dataset:\\n\")\n  cat(\"  Proteins:\", nrow(lfq_data$intensity_matrix), \"\\n\")\n  cat(\"  Samples:\", ncol(lfq_data$intensity_matrix), \"\\n\")\n  cat(\"  Missing values:\", \n      round(sum(is.na(lfq_data$intensity_matrix)) / length(lfq_data$intensity_matrix) * 100, 1), \"%\\n\")\n}\n\n\n\n8.5.3 Data Normalization and Preprocessing\n\n# Normalize proteomics data\nnormalize_proteomics_data &lt;- function(intensity_matrix, method = \"median\") {\n  log_matrix &lt;- log2(intensity_matrix)\n  \n  if (method == \"median\") {\n    # Median normalization\n    sample_medians &lt;- apply(log_matrix, 2, median, na.rm = TRUE)\n    global_median &lt;- median(sample_medians, na.rm = TRUE)\n    normalization_factors &lt;- global_median - sample_medians\n    \n    for (i in 1:ncol(log_matrix)) {\n      log_matrix[, i] &lt;- log_matrix[, i] + normalization_factors[i]\n    }\n    \n  } else if (method == \"quantile\") {\n    # Quantile normalization (simplified)\n    for (i in 1:ncol(log_matrix)) {\n      log_matrix[, i] &lt;- scale(log_matrix[, i])[, 1]\n    }\n  }\n  \n  return(2^log_matrix)  # Convert back to linear scale\n}\n\nif (exists(\"lfq_data\")) {\n  # Normalize data\n  normalized_intensities &lt;- normalize_proteomics_data(lfq_data$intensity_matrix)\n  \n  # Visualize normalization effect\n  # Before normalization\n  sample_medians_before &lt;- apply(log2(lfq_data$intensity_matrix), 2, median, na.rm = TRUE)\n  sample_medians_after &lt;- apply(log2(normalized_intensities), 2, median, na.rm = TRUE)\n  \n  normalization_df &lt;- data.frame(\n    sample = rep(colnames(lfq_data$intensity_matrix), 2),\n    median_intensity = c(sample_medians_before, sample_medians_after),\n    normalization = rep(c(\"Before\", \"After\"), each = length(sample_medians_before))\n  )\n  \n  ggplot(normalization_df, aes(x = sample, y = median_intensity, fill = normalization)) +\n    geom_bar(stat = \"identity\", position = \"dodge\", alpha = 0.7) +\n    labs(title = \"Effect of Median Normalization\",\n         x = \"Sample\", y = \"Median log2 Intensity\",\n         fill = \"Normalization\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n}",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-proteomics-analysis.html#differential-expression-analysis",
    "href": "08-proteomics-analysis.html#differential-expression-analysis",
    "title": "8  Proteomics Data Analysis",
    "section": "8.6 Differential Expression Analysis",
    "text": "8.6 Differential Expression Analysis\n\n8.6.1 Statistical Testing with limma\n\n# Perform differential expression analysis\nperform_limma_analysis &lt;- function(intensity_matrix, sample_info) {\n  # Convert to log2 scale\n  log_matrix &lt;- log2(intensity_matrix)\n  \n  # Create design matrix\n  condition &lt;- factor(sample_info$condition)\n  batch &lt;- factor(sample_info$batch)\n  design &lt;- model.matrix(~ 0 + condition + batch)\n  colnames(design)[1:2] &lt;- levels(condition)\n  \n  # Fit linear model\n  fit &lt;- lmFit(log_matrix, design)\n  \n  # Create contrast matrix\n  contrast_matrix &lt;- makeContrasts(\n    TreatmentVsControl = Treatment - Control,\n    levels = design\n  )\n  \n  # Apply contrasts\n  fit2 &lt;- contrasts.fit(fit, contrast_matrix)\n  fit2 &lt;- eBayes(fit2)\n  \n  # Extract results\n  results &lt;- topTable(fit2, coef = \"TreatmentVsControl\", \n                     number = Inf, adjust.method = \"BH\")\n  \n  return(list(fit = fit2, results = results))\n}\n\nif (exists(\"normalized_intensities\")) {\n  limma_results &lt;- perform_limma_analysis(normalized_intensities, lfq_data$sample_info)\n  \n  cat(\"Differential expression results:\\n\")\n  cat(\"  Significant proteins (p &lt; 0.05):\", \n      sum(limma_results$results$P.Value &lt; 0.05, na.rm = TRUE), \"\\n\")\n  cat(\"  Significant proteins (FDR &lt; 0.05):\", \n      sum(limma_results$results$adj.P.Val &lt; 0.05, na.rm = TRUE), \"\\n\")\n  \n  head(limma_results$results)\n}\n\n\n\n8.6.2 Volcano Plot\n\n# Create volcano plot\nif (exists(\"limma_results\")) {\n  volcano_data &lt;- limma_results$results %&gt;%\n    mutate(\n      protein_id = rownames(.),\n      significant = adj.P.Val &lt; 0.05 & abs(logFC) &gt; log2(1.2),\n      direction = case_when(\n        logFC &gt; log2(1.2) & adj.P.Val &lt; 0.05 ~ \"Up\",\n        logFC &lt; -log2(1.2) & adj.P.Val &lt; 0.05 ~ \"Down\",\n        TRUE ~ \"NS\"\n      )\n    )\n  \n  ggplot(volcano_data, aes(x = logFC, y = -log10(P.Value))) +\n    geom_point(aes(color = direction), alpha = 0.7) +\n    scale_color_manual(values = c(\"Up\" = \"red\", \"Down\" = \"blue\", \"NS\" = \"gray\")) +\n    geom_hline(yintercept = -log10(0.05), linetype = \"dashed\") +\n    geom_vline(xintercept = c(-log2(1.2), log2(1.2)), linetype = \"dashed\") +\n    labs(title = \"Volcano Plot - Proteomics Differential Expression\",\n         x = \"log2 Fold Change\", y = \"-log10 P-value\",\n         color = \"Regulation\") +\n    theme_minimal()\n}\n\n\n\n8.6.3 Protein Set Analysis\n\n# Simulate gene ontology enrichment\nsimulate_go_enrichment &lt;- function(significant_proteins, all_proteins) {\n  # Create mock GO terms\n  go_terms &lt;- c(\"Protein Binding\", \"Metabolic Process\", \"Transport\", \n                \"Cell Division\", \"DNA Repair\", \"Signal Transduction\")\n  \n  enrichment_results &lt;- data.frame()\n  \n  for (go_term in go_terms) {\n    # Randomly assign proteins to GO terms\n    go_proteins &lt;- sample(all_proteins, size = sample(20:100, 1))\n    \n    # Calculate overlap with significant proteins\n    overlap &lt;- intersect(significant_proteins, go_proteins)\n    \n    # Fisher's exact test\n    contingency &lt;- matrix(c(\n      length(overlap),\n      length(significant_proteins) - length(overlap),\n      length(go_proteins) - length(overlap),\n      length(all_proteins) - length(significant_proteins) - \n        length(go_proteins) + length(overlap)\n    ), nrow = 2)\n    \n    fisher_result &lt;- fisher.test(contingency, alternative = \"greater\")\n    \n    enrichment_results &lt;- rbind(enrichment_results, data.frame(\n      go_term = go_term,\n      overlap_size = length(overlap),\n      go_size = length(go_proteins),\n      p_value = fisher_result$p.value,\n      odds_ratio = fisher_result$estimate\n    ))\n  }\n  \n  enrichment_results$adj_p_value &lt;- p.adjust(enrichment_results$p_value, method = \"BH\")\n  return(enrichment_results[order(enrichment_results$p_value), ])\n}\n\nif (exists(\"volcano_data\")) {\n  significant_proteins &lt;- volcano_data$protein_id[volcano_data$significant]\n  all_proteins &lt;- volcano_data$protein_id\n  \n  if (length(significant_proteins) &gt; 0) {\n    go_results &lt;- simulate_go_enrichment(significant_proteins, all_proteins)\n    \n    cat(\"GO enrichment analysis:\\n\")\n    print(go_results)\n    \n    # Plot enrichment results\n    ggplot(go_results, aes(x = reorder(go_term, -log10(p_value)), \n                          y = -log10(p_value))) +\n      geom_bar(stat = \"identity\", fill = \"steelblue\", alpha = 0.7) +\n      geom_hline(yintercept = -log10(0.05), linetype = \"dashed\", color = \"red\") +\n      coord_flip() +\n      labs(title = \"GO Term Enrichment Analysis\",\n           x = \"GO Term\", y = \"-log10 P-value\") +\n      theme_minimal()\n  }\n}",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-proteomics-analysis.html#data-visualization-and-reporting",
    "href": "08-proteomics-analysis.html#data-visualization-and-reporting",
    "title": "8  Proteomics Data Analysis",
    "section": "8.7 Data Visualization and Reporting",
    "text": "8.7 Data Visualization and Reporting\n\n8.7.1 Heat Map of Significant Proteins\n\n# Create heat map for significant proteins\nif (exists(\"volcano_data\") && exists(\"normalized_intensities\")) {\n  significant_proteins &lt;- volcano_data$protein_id[volcano_data$significant]\n  \n  if (length(significant_proteins) &gt; 5) {  # Need at least 5 proteins for meaningful heatmap\n    # Select top significant proteins\n    top_proteins &lt;- head(significant_proteins, 20)\n    heatmap_data &lt;- log2(normalized_intensities[top_proteins, ])\n    \n    # Create sample annotation\n    annotation_col &lt;- data.frame(\n      Condition = lfq_data$sample_info$condition,\n      Batch = factor(lfq_data$sample_info$batch),\n      row.names = lfq_data$sample_info$sample_id\n    )\n    \n    # Generate heat map\n    pheatmap(heatmap_data,\n             annotation_col = annotation_col,\n             scale = \"row\",\n             clustering_distance_rows = \"euclidean\",\n             clustering_distance_cols = \"euclidean\",\n             show_rownames = TRUE,\n             show_colnames = TRUE,\n             main = \"Heat Map of Significantly Changed Proteins\")\n  }\n}",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-proteomics-analysis.html#exercises",
    "href": "08-proteomics-analysis.html#exercises",
    "title": "8  Proteomics Data Analysis",
    "section": "8.8 Exercises",
    "text": "8.8 Exercises\n\nAnalyze real proteomics data from a public repository\nImplement different protein inference algorithms\nCompare various normalization methods for label-free quantification\nPerform time-course proteomics analysis\nIntegrate proteomics with other omics data types",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-proteomics-analysis.html#summary",
    "href": "08-proteomics-analysis.html#summary",
    "title": "8  Proteomics Data Analysis",
    "section": "8.9 Summary",
    "text": "8.9 Summary\nThis chapter covered comprehensive proteomics data analysis workflows, including MS/MS data processing, protein identification, quantification, and differential expression analysis. These methods are essential for extracting biological insights from bottom-up proteomics experiments.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html",
    "href": "10-qfeatures-quantitative.html",
    "title": "9  Quantitative Proteomics with QFeatures",
    "section": "",
    "text": "9.1 Understanding Quantitative MS Data\nQuantitative proteomics involves the measurement and comparison of protein abundances across different conditions. This chapter introduces the QFeatures infrastructure for handling quantitative MS data.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#understanding-quantitative-ms-data",
    "href": "10-qfeatures-quantitative.html#understanding-quantitative-ms-data",
    "title": "9  Quantitative Proteomics with QFeatures",
    "section": "",
    "text": "9.1.1 Quantitation Methodologies\nThere are several approaches to quantitative proteomics, each with distinct advantages:\n\nlibrary(QFeatures)\nlibrary(tidyverse)\nlibrary(limma)\nlibrary(ggplot2)\nlibrary(pheatmap)\n\n\n9.1.1.1 Label-free MS1: Extracted Ion Chromatograms (XIC)\nIn label-free quantitation, precursor peaks matching identified peptides are integrated over retention time.\n\n\n9.1.1.2 Labelled MS2: Isobaric Tagging (TMT/iTRAQ)\nIsobaric tags allow multiplexed quantitation where peptides from different samples are chemically labeled and analyzed together.\n\n\n9.1.1.3 Label-free MS2: Spectral Counting\nSimple counting of peptide-spectrum matches assigned to each protein.\n\n\n9.1.1.4 Labelled MS1: SILAC\nStable isotope labeling allows direct comparison between heavy and light labeled samples.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#the-qfeatures-framework",
    "href": "10-qfeatures-quantitative.html#the-qfeatures-framework",
    "title": "9  Quantitative Proteomics with QFeatures",
    "section": "9.2 The QFeatures Framework",
    "text": "9.2 The QFeatures Framework\n\n9.2.1 QFeatures Class Structure\nQFeatures extends the MultiAssayExperiment class to handle the hierarchical nature of MS data (spectra → peptides → proteins).\n\n# Load example data\ndata(feat1)\nfeat1\n\nAn instance of class QFeatures containing 1 set(s):\n [1] psms: SummarizedExperiment with 10 rows and 2 columns \n\n\n\n# Examine the structure\ncolData(feat1)\n\nDataFrame with 2 rows and 1 column\n       Group\n   &lt;integer&gt;\nS1         1\nS2         2\n\n\n\n# Access the PSM-level assay\npsms_assay &lt;- feat1[[\"psms\"]]\npsms_assay\n\nclass: SummarizedExperiment \ndim: 10 2 \nmetadata(0):\nassays(1): ''\nrownames(10): PSM1 PSM2 ... PSM9 PSM10\nrowData names(5): Sequence Protein Var location pval\ncolnames(2): S1 S2\ncolData names(0):\n\n\n\n# View quantitative data\nassay(psms_assay)\n\n      S1 S2\nPSM1   1 11\nPSM2   2 12\nPSM3   3 13\nPSM4   4 14\nPSM5   5 15\nPSM6   6 16\nPSM7   7 17\nPSM8   8 18\nPSM9   9 19\nPSM10 10 20\n\n\n\n# Examine row annotations\nrowData(psms_assay)\n\nDataFrame with 10 rows and 5 columns\n           Sequence     Protein       Var      location      pval\n        &lt;character&gt; &lt;character&gt; &lt;integer&gt;   &lt;character&gt; &lt;numeric&gt;\nPSM1       SYGFNAAR       ProtA         1 Mitochondr...     0.084\nPSM2       SYGFNAAR       ProtA         2 Mitochondr...     0.077\nPSM3       SYGFNAAR       ProtA         3 Mitochondr...     0.063\nPSM4       ELGNDAYK       ProtA         4 Mitochondr...     0.073\nPSM5       ELGNDAYK       ProtA         5 Mitochondr...     0.012\nPSM6       ELGNDAYK       ProtA         6 Mitochondr...     0.011\nPSM7  IAEESNFPFI...       ProtB         7       unknown     0.075\nPSM8  IAEESNFPFI...       ProtB         8       unknown     0.038\nPSM9  IAEESNFPFI...       ProtB         9       unknown     0.028\nPSM10 IAEESNFPFI...       ProtB        10       unknown     0.097\n\n\n\n\n9.2.2 Feature Aggregation\nA key feature of QFeatures is the ability to aggregate features from lower to higher levels while maintaining traceability.\n\n# Aggregate PSMs to peptides based on sequence\nfeat1 &lt;- aggregateFeatures(feat1, \n                          i = \"psms\",\n                          fcol = \"Sequence\", \n                          name = \"peptides\",\n                          fun = colMeans)\nfeat1\n\nAn instance of class QFeatures containing 2 set(s):\n [1] psms: SummarizedExperiment with 10 rows and 2 columns \n [2] peptides: SummarizedExperiment with 3 rows and 2 columns \n\n\n\n# Examine the peptide-level data\nassay(feat1[[\"peptides\"]])\n\n             S1   S2\nELGNDAYK    5.0 15.0\nIAEESNFPFIK 8.5 18.5\nSYGFNAAR    2.0 12.0\n\n\n\n# Check aggregation statistics\nrowData(feat1[[\"peptides\"]])\n\nDataFrame with 3 rows and 4 columns\n                 Sequence     Protein      location        .n\n              &lt;character&gt; &lt;character&gt;   &lt;character&gt; &lt;integer&gt;\nELGNDAYK         ELGNDAYK       ProtA Mitochondr...         3\nIAEESNFPFIK IAEESNFPFI...       ProtB       unknown         4\nSYGFNAAR         SYGFNAAR       ProtA Mitochondr...         3\n\n\n\n# Aggregate peptides to proteins\nfeat1 &lt;- aggregateFeatures(feat1,\n                          i = \"peptides\", \n                          fcol = \"Protein\",\n                          name = \"proteins\",\n                          fun = colMedians)\nfeat1\n\nAn instance of class QFeatures containing 3 set(s):\n [1] psms: SummarizedExperiment with 10 rows and 2 columns \n [2] peptides: SummarizedExperiment with 3 rows and 2 columns \n [3] proteins: SummarizedExperiment with 2 rows and 2 columns \n\n\n\n# View final protein quantification\nassay(feat1[[\"proteins\"]])\n\n       S1   S2\nProtA 3.5 13.5\nProtB 8.5 18.5\n\n\n\n\n9.2.3 Subsetting and Filtering\nQFeatures maintains relationships between assays during subsetting operations.\n\n# Subset for a specific protein\nprotein_a &lt;- feat1[\"ProtA\", , ]\nprotein_a\n\nAn instance of class QFeatures containing 3 set(s):\n [1] psms: SummarizedExperiment with 6 rows and 2 columns \n [2] peptides: SummarizedExperiment with 2 rows and 2 columns \n [3] proteins: SummarizedExperiment with 1 rows and 2 columns \n\n\n\n# Filter features based on quality criteria\nfeat1_filtered &lt;- filterFeatures(feat1, ~ pval &lt; 0.05)\nfeat1_filtered\n\nAn instance of class QFeatures containing 3 set(s):\n [1] psms: SummarizedExperiment with 4 rows and 2 columns \n [2] peptides: SummarizedExperiment with 0 rows and 2 columns \n [3] proteins: SummarizedExperiment with 0 rows and 2 columns",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#working-with-real-data-cptac-dataset",
    "href": "10-qfeatures-quantitative.html#working-with-real-data-cptac-dataset",
    "title": "9  Quantitative Proteomics with QFeatures",
    "section": "9.3 Working with Real Data: CPTAC Dataset",
    "text": "9.3 Working with Real Data: CPTAC Dataset\n\n9.3.1 Data Import\n\nlibrary(MsDataHub)\n\n# Load CPTAC peptide data\n# Note: This is a simulated example based on the CPTAC study design\nset.seed(123)\n\n# Create sample metadata\nsample_info &lt;- data.frame(\n  sample = paste0(\"Sample_\", 1:6),\n  condition = rep(c(\"6A\", \"6B\"), each = 3),\n  replicate = rep(1:3, 2),\n  row.names = paste0(\"Sample_\", 1:6)\n)\n\n# Simulate peptide quantification data\nn_peptides &lt;- 1000\npeptide_data &lt;- matrix(\n  rlnorm(n_peptides * 6, meanlog = 10, sdlog = 1),\n  nrow = n_peptides,\n  ncol = 6,\n  dimnames = list(\n    paste0(\"Peptide_\", 1:n_peptides),\n    rownames(sample_info)\n  )\n)\n\n# Add differential expression signal\nde_peptides &lt;- 1:100  # First 100 peptides are differentially expressed\npeptide_data[de_peptides, 4:6] &lt;- peptide_data[de_peptides, 4:6] * 1.5\n\n# Create row annotations\npeptide_annotations &lt;- data.frame(\n  Sequence = paste0(\"SEQ\", 1:n_peptides),\n  Proteins = sample(paste0(\"PROT\", 1:200), n_peptides, replace = TRUE),\n  PEP = runif(n_peptides, 0, 0.1),\n  Score = runif(n_peptides, 20, 100),\n  row.names = rownames(peptide_data)\n)\n\n# Create SummarizedExperiment\nlibrary(SummarizedExperiment)\ncptac_se &lt;- SummarizedExperiment(\n  assays = list(peptides = peptide_data),\n  rowData = peptide_annotations,\n  colData = sample_info\n)\n\n# Create QFeatures object\ncptac_qf &lt;- QFeatures(list(peptides = cptac_se))\ncptac_qf\n\nAn instance of class QFeatures containing 1 set(s):\n [1] peptides: SummarizedExperiment with 1000 rows and 6 columns \n\n\n\n\n9.3.2 Data Preprocessing Pipeline\n\n# Log transformation\ncptac_qf &lt;- logTransform(cptac_qf, \n                        i = \"peptides\",\n                        name = \"log_peptides\")\n\n# Normalization (median centering)\ncptac_qf &lt;- normalize(cptac_qf,\n                     i = \"log_peptides\", \n                     name = \"norm_peptides\",\n                     method = \"center.median\")\n\ncptac_qf\n\nAn instance of class QFeatures containing 3 set(s):\n [1] peptides: SummarizedExperiment with 1000 rows and 6 columns \n [2] log_peptides: SummarizedExperiment with 1000 rows and 6 columns \n [3] norm_peptides: SummarizedExperiment with 1000 rows and 6 columns \n\n\n\n\n9.3.3 Missing Value Analysis\n\n# Introduce some missing values for demonstration\nassay_data &lt;- assay(cptac_qf[[\"norm_peptides\"]])\n# Set 10% of values to NA randomly\nmissing_indices &lt;- sample(length(assay_data), length(assay_data) * 0.1)\nassay_data[missing_indices] &lt;- NA\nassay(cptac_qf[[\"norm_peptides\"]]) &lt;- assay_data\n\n# Analyze missing value patterns\nna_stats &lt;- nNA(cptac_qf[[\"norm_peptides\"]])\ncat(\"Overall missing values:\", na_stats$nNA$pNA * 100, \"%\\n\")\n\nOverall missing values: 10 %\n\n\n\n# Visualize missing value patterns\nmissing_pattern &lt;- na_stats$nNArows\nhead(missing_pattern, 10)\n\nDataFrame with 10 rows and 3 columns\n          name       nNA       pNA\n   &lt;character&gt; &lt;integer&gt; &lt;numeric&gt;\n1    Peptide_1         1  0.166667\n2    Peptide_2         2  0.333333\n3    Peptide_3         1  0.166667\n4    Peptide_4         2  0.333333\n5    Peptide_5         2  0.333333\n6    Peptide_6         1  0.166667\n7    Peptide_7         1  0.166667\n8    Peptide_8         0  0.000000\n9    Peptide_9         1  0.166667\n10  Peptide_10         0  0.000000\n\n\n\n# Filter peptides with too many missing values\ncptac_qf_clean &lt;- filterNA(cptac_qf, i = \"norm_peptides\", pNA = 0.5)\ncat(\"Peptides after filtering:\", nrow(cptac_qf_clean[[\"norm_peptides\"]]), \"\\n\")\n\nPeptides after filtering: 1000 \n\n\n\n\n9.3.4 Protein Aggregation\n\n# Aggregate to protein level using median\ncptac_qf_clean &lt;- aggregateFeatures(cptac_qf_clean,\n                                   i = \"norm_peptides\",\n                                   fcol = \"Proteins\", \n                                   name = \"proteins\",\n                                   fun = colMedians,\n                                   na.rm = TRUE)\n\ncptac_qf_clean\n\nAn instance of class QFeatures containing 4 set(s):\n [1] peptides: SummarizedExperiment with 1000 rows and 6 columns \n [2] log_peptides: SummarizedExperiment with 1000 rows and 6 columns \n [3] norm_peptides: SummarizedExperiment with 1000 rows and 6 columns \n [4] proteins: SummarizedExperiment with 197 rows and 6 columns \n\n\n\n# Examine aggregation results\naggregation_stats &lt;- rowData(cptac_qf_clean[[\"proteins\"]])[\".n\"]\ntable(aggregation_stats)\n\n.n\n 1  2  3  4  5  6  7  8  9 10 11 13 14 \n 6 16 27 42 32 28 18 13  6  5  1  2  1",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#quality-control-and-visualization",
    "href": "10-qfeatures-quantitative.html#quality-control-and-visualization",
    "title": "9  Quantitative Proteomics with QFeatures",
    "section": "9.4 Quality Control and Visualization",
    "text": "9.4 Quality Control and Visualization\n\n9.4.1 Principal Component Analysis\n\nlibrary(factoextra)\n\n# PCA on peptide-level data\npeptide_pca &lt;- cptac_qf_clean[[\"norm_peptides\"]] %&gt;%\n  filterNA() %&gt;%\n  assay() %&gt;%\n  t() %&gt;%\n  prcomp(scale = TRUE, center = TRUE)\n\n# Create PCA plot\nfviz_pca_ind(peptide_pca, \n             habillage = colData(cptac_qf_clean)$condition,\n             title = \"Peptide-level PCA\")\n\n\n\n\n\n\n\n\n\n# PCA on protein-level data  \nprotein_pca &lt;- cptac_qf_clean[[\"proteins\"]] %&gt;%\n  filterNA() %&gt;%\n  assay() %&gt;%\n  t() %&gt;%\n  prcomp(scale = TRUE, center = TRUE)\n\nfviz_pca_ind(protein_pca,\n             habillage = colData(cptac_qf_clean)$condition, \n             title = \"Protein-level PCA\")\n\n\n\n\n\n\n\n\n\n\n9.4.2 Expression Profile Visualization\n\n# Extract data for a specific protein using longForm\nexample_protein &lt;- rownames(assay(cptac_qf_clean[[\"proteins\"]]))[1]\n\nprofile_data &lt;- longForm(cptac_qf_clean[example_protein, , \n                                       c(\"norm_peptides\", \"proteins\")]) %&gt;%\n  as_tibble()\n\n# Get column data and ensure unique column names\ncol_data &lt;- as_tibble(colData(cptac_qf_clean), rownames = \"sample_id\")\n\n# Join the data\nprofile_data &lt;- profile_data %&gt;%\n  left_join(col_data, by = c(\"colname\" = \"sample_id\"))\n\nggplot(profile_data, aes(x = colname, y = value, color = condition)) +\n  geom_point(size = 3) +\n  geom_line(aes(group = rowname)) +\n  facet_wrap(~assay, scales = \"free_y\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = paste(\"Expression Profile:\", example_protein),\n       x = \"Sample\", y = \"Normalized Intensity\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#statistical-analysis",
    "href": "10-qfeatures-quantitative.html#statistical-analysis",
    "title": "9  Quantitative Proteomics with QFeatures",
    "section": "9.5 Statistical Analysis",
    "text": "9.5 Statistical Analysis\n\n9.5.1 Differential Expression with limma\n\n# Extract protein data for statistical analysis\nprotein_data &lt;- getWithColData(cptac_qf_clean, \"proteins\")\n\n# Set up design matrix\ndesign &lt;- model.matrix(~ condition, data = colData(protein_data))\ncolnames(design) &lt;- c(\"Intercept\", \"Condition_6B_vs_6A\")\n\n# Fit linear model\nfit &lt;- lmFit(assay(protein_data), design)\nfit &lt;- eBayes(fit)\n\n# Extract results\nresults &lt;- topTable(fit, coef = \"Condition_6B_vs_6A\", number = Inf) %&gt;%\n  rownames_to_column(\"protein\") %&gt;%\n  as_tibble()\n\nhead(results)\n\n# A tibble: 6 × 7\n  protein logFC AveExpr     t P.Value adj.P.Val     B\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 PROT157 -2.27   0.226 -3.88 0.00153     0.301 -1.33\n2 PROT137 -1.83   0.372 -3.24 0.00558     0.549 -2.22\n3 PROT140  1.70   0.859  2.94 0.0102      0.650 -2.64\n4 PROT188 -1.84  -0.551 -2.77 0.0144      0.650 -2.88\n5 PROT78   1.51  -0.224  2.71 0.0165      0.650 -2.97\n6 PROT187  1.93   0.220  2.46 0.0277      0.824 -3.36\n\n\n\n# Volcano plot\nresults %&gt;%\n  ggplot(aes(x = logFC, y = -log10(P.Value))) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = -log10(0.05), linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = c(-1, 1), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Volcano Plot\",\n       x = \"log2 Fold Change\", \n       y = \"-log10 P-value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Summary of differential expression\nsignificant_proteins &lt;- results %&gt;%\n  filter(adj.P.Val &lt; 0.05, abs(logFC) &gt; 1)\n\ncat(\"Significantly changed proteins:\", nrow(significant_proteins), \"\\n\")\n\nSignificantly changed proteins: 0 \n\ncat(\"Up-regulated:\", sum(significant_proteins$logFC &gt; 1), \"\\n\")\n\nUp-regulated: 0 \n\ncat(\"Down-regulated:\", sum(significant_proteins$logFC &lt; -1), \"\\n\")\n\nDown-regulated: 0 \n\n\n\n\n9.5.2 Heatmap of Significant Proteins\n\nif (nrow(significant_proteins) &gt; 5) {\n  # Select top 20 most significant proteins\n  top_proteins &lt;- head(significant_proteins, 20)$protein\n  \n  # Create heatmap data\n  heatmap_data &lt;- assay(protein_data)[top_proteins, ]\n  \n  # Sample annotations\n  annotation_col &lt;- data.frame(\n    Condition = colData(protein_data)$condition,\n    row.names = colnames(heatmap_data)\n  )\n  \n  # Generate heatmap\n  pheatmap(heatmap_data,\n           annotation_col = annotation_col,\n           scale = \"row\",\n           clustering_distance_rows = \"euclidean\",\n           clustering_distance_cols = \"euclidean\",\n           main = \"Top Differentially Expressed Proteins\")\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#advanced-aggregation-methods",
    "href": "10-qfeatures-quantitative.html#advanced-aggregation-methods",
    "title": "9  Quantitative Proteomics with QFeatures",
    "section": "9.6 Advanced Aggregation Methods",
    "text": "9.6 Advanced Aggregation Methods\n\n9.6.1 Robust Summarization\n\nlibrary(MsCoreUtils)\n\n# Robust aggregation using robust summarization\ncptac_robust &lt;- aggregateFeatures(cptac_qf_clean,\n                                 i = \"norm_peptides\",\n                                 fcol = \"Proteins\",\n                                 name = \"proteins_robust\", \n                                 fun = MsCoreUtils::robustSummary,\n                                 na.rm = TRUE)\n\n# Compare standard vs robust aggregation\ncomparison_data &lt;- data.frame(\n  standard = assay(cptac_qf_clean[[\"proteins\"]])[, 1],\n  robust = assay(cptac_robust[[\"proteins_robust\"]])[, 1]\n) %&gt;%\n  na.omit()\n\nggplot(comparison_data, aes(x = standard, y = robust)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  labs(title = \"Standard vs Robust Aggregation\",\n       x = \"Standard (Median)\",\n       y = \"Robust Summarization\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#working-with-qfeatures-workflows",
    "href": "10-qfeatures-quantitative.html#working-with-qfeatures-workflows",
    "title": "9  Quantitative Proteomics with QFeatures",
    "section": "9.7 Working with QFeatures Workflows",
    "text": "9.7 Working with QFeatures Workflows\n\n9.7.1 Visualization of Data Relationships\n\n# Visualize the relationship between assays\nplot(cptac_robust)\n\n\n\n\n\n\n\n\n\n\n9.7.2 Custom Processing Functions\n\n# Example: Custom normalization using addAssay\nquantile_normalize &lt;- function(x) {\n  # Simple quantile normalization implementation\n  x_sorted &lt;- apply(x, 2, sort, na.last = TRUE)\n  x_mean &lt;- rowMeans(x_sorted, na.rm = TRUE)\n  \n  for (i in 1:ncol(x)) {\n    ranks &lt;- rank(x[, i], na.last = \"keep\")\n    x[, i] &lt;- x_mean[ranks]\n  }\n  return(x)\n}\n\n# Apply custom normalization\nlog_peptides_se &lt;- cptac_qf_clean[[\"log_peptides\"]]\nquantile_norm_assay &lt;- quantile_normalize(assay(log_peptides_se))\n\n# Create a new SummarizedExperiment with the normalized data\nlibrary(SummarizedExperiment)\nquantile_norm_se &lt;- SummarizedExperiment(\n  assays = list(quantile_norm = quantile_norm_assay),\n  rowData = rowData(log_peptides_se),\n  colData = colData(log_peptides_se)\n)\n\n# Add to QFeatures object\ncptac_custom &lt;- addAssay(cptac_qf_clean,\n                        quantile_norm_se,\n                        name = \"quantile_norm\")\n\n# Add assay link - for a transformation, use simple string-based linking\n# This indicates that all features in quantile_norm come from log_peptides\ncptac_custom &lt;- addAssayLink(cptac_custom,\n                            from = \"log_peptides\",\n                            to = \"quantile_norm\")\n\ncat(\"Custom normalization applied successfully\\n\")\n\nCustom normalization applied successfully\n\ncat(\"Available assays:\", names(cptac_custom), \"\\n\")\n\nAvailable assays: peptides log_peptides norm_peptides proteins quantile_norm",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#exercises",
    "href": "10-qfeatures-quantitative.html#exercises",
    "title": "9  Quantitative Proteomics with QFeatures",
    "section": "9.8 Exercises",
    "text": "9.8 Exercises\n\nLoad the CPTAC dataset and perform complete preprocessing pipeline\nCompare different aggregation methods (mean, median, robust)\nImplement missing value imputation strategies\nPerform differential expression analysis with multiple comparisons\nCreate custom visualization functions for QFeatures objects",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#summary",
    "href": "10-qfeatures-quantitative.html#summary",
    "title": "9  Quantitative Proteomics with QFeatures",
    "section": "9.9 Summary",
    "text": "9.9 Summary\nThis chapter introduced the QFeatures framework for quantitative proteomics analysis. Key concepts covered include:\n\nDifferent quantitation methodologies in proteomics\nThe hierarchical structure of MS quantitative data\nFeature aggregation strategies\nQuality control and missing value handling\nStatistical analysis workflows\nVisualization of quantitative proteomics data\n\nThe QFeatures infrastructure provides a robust foundation for reproducible quantitative proteomics analysis in R.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "09-advanced-topics.html",
    "href": "09-advanced-topics.html",
    "title": "10  Advanced Topics and Applications",
    "section": "",
    "text": "10.1 Advanced Spectra Backends\nThis chapter covers advanced techniques and specialized applications in mass spectrometry data analysis using R, including backend management, computational considerations, and specialized workflows inspired by the R for Mass Spectrometry ecosystem.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "09-advanced-topics.html#advanced-spectra-backends",
    "href": "09-advanced-topics.html#advanced-spectra-backends",
    "title": "10  Advanced Topics and Applications",
    "section": "",
    "text": "10.1.1 Understanding Backend Architecture\nThe Spectra package uses different backends to store and access MS data efficiently. Understanding these backends is crucial for handling large-scale datasets.\n\nlibrary(Spectra)\nlibrary(tidyverse)\nlibrary(BiocParallel)\n\n# Optional packages (load if available)\nif (requireNamespace(\"MsDataHub\", quietly = TRUE)) library(MsDataHub)\nif (requireNamespace(\"HDF5Array\", quietly = TRUE)) library(HDF5Array)\n\n\n# Load example data to demonstrate backends with error handling\nlibrary(msdata)\nms_file &lt;- msdata::proteomics(full.names = TRUE)[1]\n\n# Create Spectra with error handling for mzR compatibility\ntryCatch({\n  sps_mzr &lt;- Spectra(ms_file, backend = MsBackendMzR())\n  sps_mzr &lt;- setBackend(sps_mzr, backend = MsBackendDataFrame())\n  cat(\"Successfully loaded real MS data\\n\")\n}, error = function(e) {\n  cat(\"Note: Using synthetic data due to mzR compatibility issues\\n\\n\")\n  \n  # Create synthetic data\n  set.seed(123)\n  n_spectra &lt;- 100\n  \n  library(S4Vectors)\n  library(IRanges)\n  \n  mz_list &lt;- lapply(1:n_spectra, function(i) {\n    sort(runif(sample(50:150, 1), 150, 2000))\n  })\n  \n  intensity_list &lt;- lapply(mz_list, function(mz_vals) {\n    rlnorm(length(mz_vals), meanlog = 8, sdlog = 2)\n  })\n  \n  ms_levels &lt;- sample(1:2, n_spectra, replace = TRUE, prob = c(0.3, 0.7))\n  \n  spd &lt;- DataFrame(\n    msLevel = ms_levels,\n    rtime = seq(100, 3800, length.out = n_spectra),\n    acquisitionNum = 1:n_spectra,\n    precursorMz = ifelse(ms_levels == 1, NA_real_, runif(n_spectra, 400, 1500)),\n    precursorCharge = ifelse(ms_levels == 1, NA_integer_, sample(2:3, n_spectra, replace = TRUE)),\n    polarity = rep(1L, n_spectra),\n    dataOrigin = rep(\"synthetic_data.mzML\", n_spectra)\n  )\n  \n  spd$mz &lt;- NumericList(mz_list)\n  spd$intensity &lt;- NumericList(intensity_list)\n  \n  backend &lt;- MsBackendDataFrame()\n  backend &lt;- backendInitialize(backend, spd)\n  sps_mzr &lt;&lt;- Spectra(backend)\n})\n\nNote: Using synthetic data due to mzR compatibility issues\n\ncat(\"Backend class:\", class(sps_mzr@backend)[1], \"\\n\")\n\nBackend class: MsBackendDataFrame \n\ncat(\"Total spectra:\", length(sps_mzr), \"\\n\")\n\nTotal spectra: 100 \n\ncat(\"Data access type: In-memory\\n\")\n\nData access type: In-memory\n\n\n\n10.1.1.1 MsBackendMzR: On-disk Storage\n\n# Note: Demonstrating backend concepts with in-memory data\n# (MsBackendMzR requires compatible mzR version)\ncat(\"Backend information:\\n\")\n\nBackend information:\n\ncat(\"Backend class:\", class(sps_mzr@backend)[1], \"\\n\")\n\nBackend class: MsBackendDataFrame \n\ncat(\"Data origin:\", unique(dataOrigin(sps_mzr))[1], \"\\n\")\n\nData origin: synthetic_data.mzML \n\n# Data access demonstration\npeak_data &lt;- peaksData(sps_mzr[1:5])\ncat(\"\\nRetrieved peaks for 5 spectra\\n\")\n\n\nRetrieved peaks for 5 spectra\n\ncat(\"First spectrum has\", length(peak_data[[1]][,1]), \"peaks\\n\")\n\nFirst spectrum has 80 peaks\n\n\n\n\n10.1.1.2 MsBackendDataFrame: In-memory Storage\n\n# Our data is already in-memory using MsBackendDataFrame\ncat(\"Current backend class:\", class(sps_mzr@backend)[1], \"\\n\")\n\nCurrent backend class: MsBackendDataFrame \n\ncat(\"Data is in memory for fast access\\n\")\n\nData is in memory for fast access\n\n# Demonstrate fast access to data\nsystem.time({\n  for(i in 1:10) {\n    temp &lt;- intensity(sps_mzr[1:min(50, length(sps_mzr))])\n  }\n})\n\n   user  system elapsed \n   0.04    0.00    0.03 \n\ncat(\"\\nIn-memory backend provides fast repeated access\\n\")\n\n\nIn-memory backend provides fast repeated access\n\n\n\n\n10.1.1.3 MsBackendHdf5Peaks: HDF5 Storage\n\n# For very large datasets, HDF5 backend provides efficient storage\n# This is particularly useful for processed data\n# Note: MsBackendHdf5Peaks package not available in this environment\n\n# Conceptual demonstration (requires MsBackendHdf5Peaks package)\n# library(MsBackendHdf5Peaks)\n# hdf5_file &lt;- tempfile(fileext = \".h5\")\n# sps_hdf5 &lt;- setBackend(sps_mzr, \n#                       backend = MsBackendHdf5Peaks(),\n#                       hdf5path = hdf5_file)\n\ncat(\"HDF5 backend benefits:\\n\")\n\nHDF5 backend benefits:\n\ncat(\"- Efficient storage for large datasets\\n\")\n\n- Efficient storage for large datasets\n\ncat(\"- Fast partial data loading\\n\") \n\n- Fast partial data loading\n\ncat(\"- Cross-platform compatibility\\n\")\n\n- Cross-platform compatibility\n\ncat(\"- Reduced memory footprint\\n\")\n\n- Reduced memory footprint\n\ncat(\"\\nNote: Install with BiocManager::install('MsBackendHdf5Peaks')\\n\")\n\n\nNote: Install with BiocManager::install('MsBackendHdf5Peaks')",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "09-advanced-topics.html#computational-considerations",
    "href": "09-advanced-topics.html#computational-considerations",
    "title": "10  Advanced Topics and Applications",
    "section": "10.2 Computational Considerations",
    "text": "10.2 Computational Considerations\n\n10.2.1 Parallel Processing with BiocParallel\n\n# Configure parallel processing for large datasets\nlibrary(BiocParallel)\nlibrary(parallel)  # For detectCores()\n\n# Check available cores\ncat(\"Available cores:\", detectCores(), \"\\n\")\n\nAvailable cores: 24 \n\n# Set up parallel backend\nparam &lt;- MulticoreParam(workers = 2)  # Use 2 cores for demonstration\nregister(param)\n\n# Parallel processing example with spectral processing\nprocess_spectra_parallel &lt;- function(sps, param = bpparam()) {\n  # Example: Calculate TIC for all spectra in parallel\n  bplapply(seq_along(sps), function(i) {\n    sum(intensity(sps[i])[[1]], na.rm = TRUE)\n  }, BPPARAM = param)\n}\n\n# Use the available spectra data (sps_mzr)\nn_spectra &lt;- min(100, length(sps_mzr))\n\n# Time comparison - Sequential processing\nsystem.time({\n  tic_sequential &lt;- sapply(1:n_spectra, function(i) {\n    sum(intensity(sps_mzr[i])[[1]], na.rm = TRUE)\n  })\n})\n\n   user  system elapsed \n   0.26    0.00    0.26 \n\n# Parallel processing\nsystem.time({\n  tic_parallel &lt;- unlist(process_spectra_parallel(sps_mzr[1:n_spectra], param))\n})\n\n   user  system elapsed \n   0.27    0.00    0.29 \n\ncat(\"Parallel processing can significantly speed up large-scale operations\\n\")\n\nParallel processing can significantly speed up large-scale operations\n\n\n\n\n10.2.2 Memory Management Strategies\n\n# Memory monitoring functions\ncheck_memory_usage &lt;- function(label = \"\") {\n  if (label != \"\") cat(label, \":\\n\")\n  mem_used &lt;- sum(sapply(ls(envir = .GlobalEnv), function(x) {\n    object.size(get(x, envir = .GlobalEnv))\n  }))\n  cat(\"  Memory used:\", format(mem_used, units = \"MB\"), \"\\n\")\n}\n\n# Demonstrate memory efficient workflows\nlarge_dataset_workflow &lt;- function(sps_data) {\n  # Process spectra in batches without loading all into memory at once\n  batch_size &lt;- 20\n  n_spectra &lt;- length(sps_data)\n  n_batches &lt;- ceiling(n_spectra / batch_size)\n  \n  results &lt;- list()\n  \n  for (i in 1:n_batches) {\n    start_idx &lt;- (i - 1) * batch_size + 1\n    end_idx &lt;- min(i * batch_size, n_spectra)\n    \n    # Process batch\n    batch_sps &lt;- sps_data[start_idx:end_idx]\n    \n    # Extract summary statistics\n    results[[i]] &lt;- data.frame(\n      batch = i,\n      n_spectra = length(batch_sps),\n      rt_range = paste(round(range(rtime(batch_sps)), 2), collapse = \"-\"),\n      ms_levels = paste(unique(msLevel(batch_sps)), collapse = \",\"),\n      mean_peaks = round(mean(lengths(mz(batch_sps))))\n    )\n    \n    # Explicitly remove batch objects\n    rm(batch_sps)\n    gc()  # Force garbage collection\n  }\n  \n  do.call(rbind, results)\n}\n\n# Example usage with existing data\nresult &lt;- large_dataset_workflow(sps_mzr)\nprint(result)\n\n  batch n_spectra        rt_range ms_levels mean_peaks\n1     1        20       100-810.1       2,1         97\n2     2        20  847.47-1557.58       1,2        104\n3     3        20 1594.95-2305.05       2,1        103\n4     4        20 2342.42-3052.53       2,1        106\n5     5        20     3089.9-3800       2,1        106\n\ncat(\"\\nBatch processing helps manage memory for large datasets\\n\")\n\n\nBatch processing helps manage memory for large datasets",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "09-advanced-topics.html#advanced-spectral-processing",
    "href": "09-advanced-topics.html#advanced-spectral-processing",
    "title": "10  Advanced Topics and Applications",
    "section": "10.3 Advanced Spectral Processing",
    "text": "10.3 Advanced Spectral Processing\n\n10.3.1 Custom Backend Development\n\n# Example of extending Spectra with custom functionality\ncustom_spectral_processing &lt;- function(sps) {\n  # Custom processing pipeline\n  processed_sps &lt;- sps\n  \n  # 1. Noise reduction with Savitzky-Golay smoothing\n  processed_sps &lt;- smooth(processed_sps, method = \"SavitzkyGolay\")\n  \n  # 2. Peak picking with custom parameters\n  processed_sps &lt;- pickPeaks(processed_sps, \n                            method = \"MAD\",\n                            snr = 3)\n  \n  # 3. Normalize intensities by dividing by max intensity\n  processed_sps &lt;- addProcessing(processed_sps, function(x) {\n    x[, \"intensity\"] &lt;- x[, \"intensity\"] / max(x[, \"intensity\"], na.rm = TRUE)\n    x\n  })\n  \n  return(processed_sps)\n}\n\n# Apply custom processing\nprocessed_spectra &lt;- custom_spectral_processing(sps_mzr[1:10])\ncat(\"Applied custom processing pipeline to\", length(processed_spectra), \"spectra\\n\")\n\nApplied custom processing pipeline to 10 spectra\n\ncat(\"Processing steps include: smoothing, peak picking, and normalization\\n\")\n\nProcessing steps include: smoothing, peak picking, and normalization\n\n\n\n\n10.3.2 Spectral Similarity Networks\n\n# Advanced spectral comparison and networking\nlibrary(igraph)\n\ncalculate_spectral_similarity_matrix &lt;- function(sps, method = \"cosine\") {\n  n &lt;- length(sps)\n  similarity_matrix &lt;- matrix(0, nrow = n, ncol = n)\n  \n  for (i in 1:n) {\n    for (j in i:n) {\n      if (i == j) {\n        similarity_matrix[i, j] &lt;- 1.0\n      } else {\n        # Calculate pairwise similarity\n        sim &lt;- compareSpectra(sps[i], sps[j], \n                             FUN = MsCoreUtils::gnps,\n                             ppm = 20)\n        similarity_matrix[i, j] &lt;- sim\n        similarity_matrix[j, i] &lt;- sim\n      }\n    }\n  }\n  \n  return(similarity_matrix)\n}\n\n# Create similarity network (with subset for demonstration)\nsubset_spectra &lt;- sps_mzr[1:20]\nsim_matrix &lt;- calculate_spectral_similarity_matrix(subset_spectra)\n\n# Create network from similarity matrix\nthreshold &lt;- 0.6\nadjacency_matrix &lt;- ifelse(sim_matrix &gt;= threshold, 1, 0)\ndiag(adjacency_matrix) &lt;- 0  # Remove self-loops\n\nnetwork &lt;- graph_from_adjacency_matrix(adjacency_matrix, mode = \"undirected\")\n\n# Network statistics\ncat(\"Network statistics:\\n\")\n\nNetwork statistics:\n\ncat(\"  Nodes:\", vcount(network), \"\\n\")\n\n  Nodes: 20 \n\ncat(\"  Edges:\", ecount(network), \"\\n\")\n\n  Edges: 0 \n\ncat(\"  Connected components:\", components(network)$no, \"\\n\")\n\n  Connected components: 20 \n\n# Visualize network (if small enough)\nif (vcount(network) &lt;= 50) {\n  plot(network,\n       vertex.size = 8,\n       vertex.color = \"lightblue\",\n       edge.width = 2,\n       main = \"Spectral Similarity Network\")\n}",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "09-advanced-topics.html#integration-with-external-tools",
    "href": "09-advanced-topics.html#integration-with-external-tools",
    "title": "10  Advanced Topics and Applications",
    "section": "10.4 Integration with External Tools",
    "text": "10.4 Integration with External Tools\n\n10.4.1 Connecting to Online Resources\n\n# Example: Integration with online databases\naccess_online_resources &lt;- function() {\n  cat(\"Strategies for integrating external resources:\\n\\n\")\n  \n  cat(\"1. GNPS (Global Natural Products Social Molecular Networking):\\n\")\n  cat(\"   - Use GNPS REST API for spectral library matching\\n\")\n  cat(\"   - Export data in GNPS-compatible formats\\n\\n\")\n  \n  cat(\"2. MassBank:\\n\") \n  cat(\"   - Access curated reference spectra\\n\")\n  cat(\"   - Use RMassBank for compound identification\\n\\n\")\n  \n  cat(\"3. ChemSpider/PubChem:\\n\")\n  cat(\"   - Retrieve compound information\\n\")\n  cat(\"   - Use webchem package for programmatic access\\n\\n\")\n  \n  cat(\"4. MetaboLights/PRIDE:\\n\")\n  cat(\"   - Access public datasets\\n\")\n  cat(\"   - Use appropriate R packages for data retrieval\\n\")\n}\n\naccess_online_resources()\n\nStrategies for integrating external resources:\n\n1. GNPS (Global Natural Products Social Molecular Networking):\n   - Use GNPS REST API for spectral library matching\n   - Export data in GNPS-compatible formats\n\n2. MassBank:\n   - Access curated reference spectra\n   - Use RMassBank for compound identification\n\n3. ChemSpider/PubChem:\n   - Retrieve compound information\n   - Use webchem package for programmatic access\n\n4. MetaboLights/PRIDE:\n   - Access public datasets\n   - Use appropriate R packages for data retrieval\n\n\n\n\n10.4.2 Export and Interoperability\n\n# Data export functions for different formats\nexport_spectra_formats &lt;- function(sps, base_name) {\n  # 1. Export to mzML (standard format)\n  # export(sps, file = paste0(base_name, \".mzML\"))\n  \n  # 2. Export to MGF (for GNPS)\n  # export(sps, file = paste0(base_name, \".mgf\"))\n  \n  # 3. Export metadata as CSV\n  metadata_df &lt;- spectraData(sps) %&gt;%\n    as.data.frame() %&gt;%\n    select(msLevel, rtime, precursorMz, precursorCharge, collisionEnergy)\n  \n  write.csv(metadata_df, file = paste0(base_name, \"_metadata.csv\"), row.names = FALSE)\n  \n  cat(\"Exported spectra in multiple formats:\\n\")\n  cat(\"  - Metadata: CSV format\\n\")\n  cat(\"  - Spectral data: mzML/MGF (commented out)\\n\")\n}\n\n# Example export (metadata only)\nexport_spectra_formats(subset_spectra, \"example_export\")\n\nExported spectra in multiple formats:\n  - Metadata: CSV format\n  - Spectral data: mzML/MGF (commented out)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "09-advanced-topics.html#machine-learning-integration",
    "href": "09-advanced-topics.html#machine-learning-integration",
    "title": "10  Advanced Topics and Applications",
    "section": "10.5 Machine Learning Integration",
    "text": "10.5 Machine Learning Integration\n\n10.5.1 Feature Engineering for ML\n\n# Create synthetic feature matrix and metadata for ML demonstration\nset.seed(123)\nn_samples &lt;- 50\nn_features &lt;- 100\n\n# Create synthetic feature matrix\nspectral_features &lt;- matrix(\n  rlnorm(n_samples * n_features, meanlog = 5, sdlog = 2),\n  nrow = n_samples,\n  ncol = n_features\n)\ncolnames(spectral_features) &lt;- paste0(\"feature_\", 1:n_features)\nrownames(spectral_features) &lt;- paste0(\"sample_\", 1:n_samples)\n\n# Create sample metadata\nsample_metadata &lt;- data.frame(\n  sample_id = paste0(\"sample_\", 1:n_samples),\n  class = factor(rep(c(\"Healthy\", \"Disease\"), each = n_samples/2)),\n  batch = factor(rep(1:5, length.out = n_samples))\n)\n\ncat(\"Created synthetic dataset:\\n\")\n\nCreated synthetic dataset:\n\ncat(\"  Samples:\", n_samples, \"\\n\")\n\n  Samples: 50 \n\ncat(\"  Features:\", n_features, \"\\n\")\n\n  Features: 100 \n\ncat(\"  Classes:\", paste(levels(sample_metadata$class), collapse = \", \"), \"\\n\")\n\n  Classes: Disease, Healthy \n\n\n\n# Feature preprocessing\npreprocess_features &lt;- function(features, metadata) {\n  # Log transform\n  log_features &lt;- log2(features + 1)\n  \n  # Normalize by sample (row-wise)\n  normalized_features &lt;- t(scale(t(log_features)))\n  \n  # Handle missing values\n  normalized_features[is.na(normalized_features)] &lt;- 0\n  \n  return(normalized_features)\n}\n\n# Apply preprocessing\nprocessed_features &lt;- preprocess_features(spectral_features, sample_metadata)\ncat(\"Preprocessed features: log2 transformation and normalization\\n\")\n\nPreprocessed features: log2 transformation and normalization\n\n\n\n# Feature selection using statistical tests\nfeature_selection_stats &lt;- function(features, classes) {\n  p_values &lt;- numeric(ncol(features))\n  fold_changes &lt;- numeric(ncol(features))\n  \n  for (i in 1:ncol(features)) {\n    healthy_vals &lt;- features[classes == \"Healthy\", i]\n    disease_vals &lt;- features[classes == \"Disease\", i]\n    \n    # t-test\n    test_result &lt;- t.test(disease_vals, healthy_vals)\n    p_values[i] &lt;- test_result$p.value\n    \n    # Fold change\n    fold_changes[i] &lt;- log2(mean(disease_vals) / mean(healthy_vals))\n  }\n  \n  # Multiple testing correction\n  adj_p_values &lt;- p.adjust(p_values, method = \"fdr\")\n  \n  feature_stats &lt;- data.frame(\n    feature = colnames(features),\n    p_value = p_values,\n    adj_p_value = adj_p_values,\n    log2_fc = fold_changes,\n    significant = adj_p_values &lt; 0.05\n  )\n  \n  return(feature_stats[order(feature_stats$adj_p_value), ])\n}\n\nfeature_stats &lt;- feature_selection_stats(processed_features, sample_metadata$class)\n\n# Visualize feature selection results\nggplot(feature_stats, aes(x = log2_fc, y = -log10(p_value))) +\n  geom_point(aes(color = significant), alpha = 0.7) +\n  scale_color_manual(values = c(\"FALSE\" = \"gray\", \"TRUE\" = \"red\")) +\n  labs(title = \"Feature Selection - Volcano Plot\",\n       x = \"log2 Fold Change\", y = \"-log10 P-value\",\n       color = \"Significant\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Select top features\ntop_features &lt;- head(feature_stats$feature[feature_stats$significant], 50)\n\n# If no significant features, use top 50 by p-value\nif (length(top_features) == 0) {\n  top_features &lt;- head(feature_stats$feature, 50)\n  cat(\"No significant features found. Using top 50 features by p-value.\\n\")\n}\n\nNo significant features found. Using top 50 features by p-value.\n\nselected_features &lt;- processed_features[, top_features, drop = FALSE]\n\ncat(\"Selected\", ncol(selected_features), \"features for ML\\n\")\n\nSelected 50 features for ML\n\n\n\n\n10.5.2 Classification Models\n\n# Load required ML packages\nif (!requireNamespace(\"randomForest\", quietly = TRUE)) {\n  cat(\"Note: randomForest package not installed. Install with: install.packages('randomForest')\\n\")\n} else {\n  library(randomForest)\n}\n\nif (!requireNamespace(\"e1071\", quietly = TRUE)) {\n  cat(\"Note: e1071 package not installed. Install with: install.packages('e1071')\\n\")\n} else {\n  library(e1071)\n}\n\n# Prepare data for machine learning\nprepare_ml_data &lt;- function(features, metadata) {\n  # Create training/testing split\n  set.seed(42)\n  train_indices &lt;- sample(1:nrow(features), size = 0.7 * nrow(features))\n  \n  ml_data &lt;- list(\n    train_x = features[train_indices, ],\n    train_y = factor(metadata$class[train_indices]),\n    test_x = features[-train_indices, ],\n    test_y = factor(metadata$class[-train_indices]),\n    train_indices = train_indices\n  )\n  \n  return(ml_data)\n}\n\nml_data &lt;- prepare_ml_data(selected_features, sample_metadata)\n\ncat(\"Training set:\", nrow(ml_data$train_x), \"samples\\n\")\n\nTraining set: 35 samples\n\ncat(\"Test set:\", nrow(ml_data$test_x), \"samples\\n\")\n\nTest set: 15 samples\n\n# Random Forest Classification\ntrain_random_forest &lt;- function(train_x, train_y, ntree = 500) {\n  if (!requireNamespace(\"randomForest\", quietly = TRUE)) {\n    cat(\"randomForest package not available, skipping...\\n\")\n    return(NULL)\n  }\n  rf_model &lt;- randomForest(\n    x = train_x,\n    y = train_y,\n    ntree = ntree,\n    importance = TRUE\n  )\n  return(rf_model)\n}\n\nrf_model &lt;- train_random_forest(ml_data$train_x, ml_data$train_y)\n\n# SVM Classification\ntrain_svm &lt;- function(train_x, train_y) {\n  if (!requireNamespace(\"e1071\", quietly = TRUE)) {\n    cat(\"e1071 package not available, skipping...\\n\")\n    return(NULL)\n  }\n  svm_model &lt;- svm(\n    x = train_x,\n    y = train_y,\n    kernel = \"radial\",\n    probability = TRUE\n  )\n  return(svm_model)\n}\n\nsvm_model &lt;- train_svm(ml_data$train_x, ml_data$train_y)\n\nif (!is.null(rf_model) && !is.null(svm_model)) {\n  cat(\"Trained Random Forest and SVM models\\n\")\n} else {\n  cat(\"Some models could not be trained due to missing packages\\n\")\n}\n\nTrained Random Forest and SVM models\n\n\n\n\n10.5.3 Model Evaluation\n\n# Load required packages for evaluation\nif (!requireNamespace(\"ROCR\", quietly = TRUE)) {\n  cat(\"Note: ROCR package not installed. Install with: install.packages('ROCR')\\n\")\n} else {\n  library(ROCR)\n}\n\nNote: ROCR package not installed. Install with: install.packages('ROCR')\n\n# Custom confusion matrix function if caret is not available\ncalculate_confusion_matrix &lt;- function(predictions, actual) {\n  conf_mat &lt;- table(Predicted = predictions, Actual = actual)\n  \n  # Calculate metrics\n  accuracy &lt;- sum(diag(conf_mat)) / sum(conf_mat)\n  \n  # For binary classification\n  if (nrow(conf_mat) == 2) {\n    tp &lt;- conf_mat[2, 2]\n    tn &lt;- conf_mat[1, 1]\n    fp &lt;- conf_mat[2, 1]\n    fn &lt;- conf_mat[1, 2]\n    \n    sensitivity &lt;- tp / (tp + fn)\n    specificity &lt;- tn / (tn + fp)\n    precision &lt;- tp / (tp + fp)\n  } else {\n    sensitivity &lt;- NA\n    specificity &lt;- NA\n    precision &lt;- NA\n  }\n  \n  return(list(\n    table = conf_mat,\n    overall = c(Accuracy = accuracy, Sensitivity = sensitivity, \n                Specificity = specificity, Precision = precision)\n  ))\n}\n\n# Evaluate model performance\nevaluate_model &lt;- function(model, test_x, test_y, model_name) {\n  if (is.null(model)) {\n    cat(\"Model\", model_name, \"is NULL, skipping evaluation\\n\")\n    return(NULL)\n  }\n  \n  if (model_name == \"RandomForest\") {\n    predictions &lt;- predict(model, test_x)\n    probabilities &lt;- predict(model, test_x, type = \"prob\")[, \"Disease\"]\n  } else if (model_name == \"SVM\") {\n    predictions &lt;- predict(model, test_x)\n    prob_matrix &lt;- attr(predict(model, test_x, probability = TRUE), \"probabilities\")\n    probabilities &lt;- prob_matrix[, \"Disease\"]\n  }\n  \n  # Confusion matrix\n  conf_matrix &lt;- calculate_confusion_matrix(predictions, test_y)\n  \n  # ROC analysis (if ROCR is available)\n  if (requireNamespace(\"ROCR\", quietly = TRUE)) {\n    pred_obj &lt;- ROCR::prediction(probabilities, test_y)\n    perf_obj &lt;- ROCR::performance(pred_obj, \"tpr\", \"fpr\")\n    auc_obj &lt;- ROCR::performance(pred_obj, \"auc\")\n    auc_value &lt;- auc_obj@y.values[[1]]\n    \n    roc_data &lt;- data.frame(\n      fpr = perf_obj@x.values[[1]],\n      tpr = perf_obj@y.values[[1]]\n    )\n  } else {\n    auc_value &lt;- NA\n    roc_data &lt;- data.frame(fpr = numeric(0), tpr = numeric(0))\n  }\n  \n  return(list(\n    model_name = model_name,\n    confusion_matrix = conf_matrix,\n    predictions = predictions,\n    probabilities = probabilities,\n    roc_data = roc_data,\n    auc = auc_value\n  ))\n}\n\n# Evaluate both models\nrf_eval &lt;- evaluate_model(rf_model, ml_data$test_x, ml_data$test_y, \"RandomForest\")\nsvm_eval &lt;- evaluate_model(svm_model, ml_data$test_x, ml_data$test_y, \"SVM\")\n\n# Print performance metrics\nif (!is.null(rf_eval)) {\n  cat(\"Random Forest Performance:\\n\")\n  print(rf_eval$confusion_matrix$overall)\n  cat(\"AUC:\", round(rf_eval$auc, 3), \"\\n\\n\")\n}\n\nRandom Forest Performance:\n   Accuracy Sensitivity Specificity   Precision \n  0.6000000   0.3333333   1.0000000   1.0000000 \nAUC: NA \n\nif (!is.null(svm_eval)) {\n  cat(\"SVM Performance:\\n\")\n  print(svm_eval$confusion_matrix$overall)\n  cat(\"AUC:\", round(svm_eval$auc, 3), \"\\n\")\n}\n\nSVM Performance:\n   Accuracy Sensitivity Specificity   Precision \n  0.7333333   0.5555556   1.0000000   1.0000000 \nAUC: NA \n\n# Plot ROC curves (if both models were evaluated)\nif (!is.null(rf_eval) && !is.null(svm_eval) && \n    nrow(rf_eval$roc_data) &gt; 0 && nrow(svm_eval$roc_data) &gt; 0) {\n  \n  roc_combined &lt;- rbind(\n    data.frame(rf_eval$roc_data, Model = \"Random Forest\"),\n    data.frame(svm_eval$roc_data, Model = \"SVM\")\n  )\n  \n  print(ggplot(roc_combined, aes(x = fpr, y = tpr, color = Model)) +\n    geom_line(size = 1) +\n    geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\") +\n    labs(title = \"ROC Curves Comparison\",\n         x = \"False Positive Rate\", y = \"True Positive Rate\") +\n    annotate(\"text\", x = 0.6, y = 0.3, \n             label = paste(\"RF AUC =\", round(rf_eval$auc, 3))) +\n    annotate(\"text\", x = 0.6, y = 0.2, \n             label = paste(\"SVM AUC =\", round(svm_eval$auc, 3))) +\n    theme_minimal())\n}\n\n\n\n10.5.4 Feature Importance Analysis\n\n# Analyze feature importance\nanalyze_feature_importance &lt;- function(rf_model, feature_names) {\n  importance_scores &lt;- importance(rf_model)\n  \n  importance_df &lt;- data.frame(\n    feature = feature_names,\n    mean_decrease_accuracy = importance_scores[, \"MeanDecreaseAccuracy\"],\n    mean_decrease_gini = importance_scores[, \"MeanDecreaseGini\"]\n  ) %&gt;%\n    arrange(desc(mean_decrease_accuracy))\n  \n  return(importance_df)\n}\n\nfeature_importance &lt;- analyze_feature_importance(rf_model, colnames(selected_features))\n\n# Plot feature importance\ntop_features_plot &lt;- head(feature_importance, 20)\n\nggplot(top_features_plot, aes(x = reorder(feature, mean_decrease_accuracy), \n                             y = mean_decrease_accuracy)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\", alpha = 0.7) +\n  coord_flip() +\n  labs(title = \"Top 20 Most Important Features (Random Forest)\",\n       x = \"Features\", y = \"Mean Decrease Accuracy\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "09-advanced-topics.html#ion-mobility-spectrometry-ms-ims-ms",
    "href": "09-advanced-topics.html#ion-mobility-spectrometry-ms-ims-ms",
    "title": "10  Advanced Topics and Applications",
    "section": "10.6 Ion Mobility Spectrometry-MS (IMS-MS)",
    "text": "10.6 Ion Mobility Spectrometry-MS (IMS-MS)\n\n10.6.1 IMS Data Simulation and Processing\n\n# Simulate IMS-MS data\nsimulate_ims_data &lt;- function(n_scans = 1000, n_drift_bins = 200, n_mz_bins = 500) {\n  # Create coordinate system\n  drift_times &lt;- seq(10, 50, length.out = n_drift_bins)  # ms\n  mz_values &lt;- seq(100, 1000, length.out = n_mz_bins)\n  retention_times &lt;- seq(60, 1800, length.out = n_scans)  # seconds\n  \n  # Simulate IMS-MS data structure\n  ims_data &lt;- list()\n  \n  for (scan in 1:min(100, n_scans)) {  # Limit for memory\n    # Create 2D IMS-MS spectrum (drift time x m/z)\n    intensity_matrix &lt;- matrix(0, nrow = n_drift_bins, ncol = n_mz_bins)\n    \n    # Add some peaks\n    n_peaks &lt;- sample(20:80, 1)\n    \n    for (peak in 1:n_peaks) {\n      # Random peak position\n      dt_center &lt;- sample(1:n_drift_bins, 1)\n      mz_center &lt;- sample(1:n_mz_bins, 1)\n      \n      # Peak intensity\n      peak_intensity &lt;- rlnorm(1, meanlog = 8, sdlog = 1)\n      \n      # Add Gaussian peak\n      for (dt in max(1, dt_center-5):min(n_drift_bins, dt_center+5)) {\n        for (mz in max(1, mz_center-3):min(n_mz_bins, mz_center+3)) {\n          distance &lt;- sqrt((dt - dt_center)^2 + (mz - mz_center)^2)\n          if (distance &lt; 5) {\n            intensity_matrix[dt, mz] &lt;- intensity_matrix[dt, mz] + \n              peak_intensity * exp(-distance^2 / 4)\n          }\n        }\n      }\n    }\n    \n    ims_data[[scan]] &lt;- list(\n      scan_number = scan,\n      retention_time = retention_times[scan],\n      drift_times = drift_times,\n      mz_values = mz_values,\n      intensity_matrix = intensity_matrix\n    )\n  }\n  \n  return(ims_data)\n}\n\n# Generate IMS data\nims_dataset &lt;- simulate_ims_data(n_scans = 50)\ncat(\"Created IMS dataset with\", length(ims_dataset), \"scans\\n\")\n\nCreated IMS dataset with 50 scans\n\n# Example: visualize one IMS-MS spectrum\nif (length(ims_dataset) &gt; 0) {\n  example_scan &lt;- ims_dataset[[25]]\n  \n  # Create heatmap data\n  heatmap_data &lt;- expand.grid(\n    drift_time = example_scan$drift_times,\n    mz = example_scan$mz_values\n  )\n  heatmap_data$intensity &lt;- as.vector(example_scan$intensity_matrix)\n  \n  # Filter to show only non-zero intensities\n  heatmap_data &lt;- heatmap_data[heatmap_data$intensity &gt; 0, ]\n  \n  if (nrow(heatmap_data) &gt; 0) {\n    ggplot(heatmap_data, aes(x = mz, y = drift_time, fill = intensity)) +\n      geom_raster() +\n      scale_fill_viridis_c(trans = \"sqrt\") +\n      labs(title = paste(\"IMS-MS Spectrum - Scan\", example_scan$scan_number),\n           subtitle = paste(\"RT =\", round(example_scan$retention_time, 1), \"s\"),\n           x = \"m/z\", y = \"Drift Time (ms)\") +\n      theme_minimal()\n  }\n}\n\n\n\n\n\n\n\n\n\n\n10.6.2 IMS Peak Detection\n\n# IMS peak detection algorithm\ndetect_ims_peaks &lt;- function(ims_scan, intensity_threshold = 1000) {\n  intensity_matrix &lt;- ims_scan$intensity_matrix\n  drift_times &lt;- ims_scan$drift_times\n  mz_values &lt;- ims_scan$mz_values\n  \n  # Find peaks above threshold\n  peak_positions &lt;- which(intensity_matrix &gt; intensity_threshold, arr.ind = TRUE)\n  \n  if (nrow(peak_positions) == 0) {\n    return(data.frame())\n  }\n  \n  # Extract peak information\n  peaks &lt;- data.frame(\n    drift_time_index = peak_positions[, 1],\n    mz_index = peak_positions[, 2],\n    drift_time = drift_times[peak_positions[, 1]],\n    mz = mz_values[peak_positions[, 2]],\n    intensity = intensity_matrix[peak_positions]\n  )\n  \n  # Calculate collision cross section (CCS) - simplified relationship\n  peaks$ccs &lt;- calculate_ccs(peaks$drift_time, peaks$mz)\n  \n  return(peaks)\n}\n\n# Simplified CCS calculation\ncalculate_ccs &lt;- function(drift_time, mz, temperature = 293, pressure = 760) {\n  # Simplified Mason-Schamp equation\n  # CCS proportional to drift_time / sqrt(mz)\n  ccs &lt;- drift_time * 100 / sqrt(mz)  # Arbitrary units for demonstration\n  return(ccs)\n}\n\n# Detect peaks in example scan\nif (length(ims_dataset) &gt; 0) {\n  example_peaks &lt;- detect_ims_peaks(ims_dataset[[25]], intensity_threshold = 500)\n  \n  if (nrow(example_peaks) &gt; 0) {\n    cat(\"Detected\", nrow(example_peaks), \"peaks in example scan\\n\")\n    \n    # Plot peaks in CCS vs m/z space\n    ggplot(example_peaks, aes(x = mz, y = ccs, size = intensity)) +\n      geom_point(alpha = 0.7, color = \"blue\") +\n      scale_size_continuous(range = c(1, 5)) +\n      labs(title = \"IMS Peaks in CCS vs m/z Space\",\n           x = \"m/z\", y = \"CCS (Arbitrary Units)\",\n           size = \"Intensity\") +\n      theme_minimal()\n  }\n}\n\nDetected 1653 peaks in example scan",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "09-advanced-topics.html#advanced-statistical-methods",
    "href": "09-advanced-topics.html#advanced-statistical-methods",
    "title": "10  Advanced Topics and Applications",
    "section": "10.7 Advanced Statistical Methods",
    "text": "10.7 Advanced Statistical Methods\n\n10.7.1 Survival Analysis for MS Data\n\nlibrary(survival)\nlibrary(survminer)\n\n# Simulate clinical data with MS measurements\nsimulate_survival_data &lt;- function(n_patients = 200) {\n  # Patient characteristics\n  patients &lt;- data.frame(\n    patient_id = paste0(\"P\", 1:n_patients),\n    age = rnorm(n_patients, 65, 10),\n    gender = sample(c(\"M\", \"F\"), n_patients, replace = TRUE),\n    stage = sample(c(\"I\", \"II\", \"III\", \"IV\"), n_patients, replace = TRUE, \n                  prob = c(0.2, 0.3, 0.3, 0.2))\n  )\n  \n  # Simulate biomarker levels (e.g., protein concentrations)\n  biomarker_data &lt;- data.frame(\n    patient_id = patients$patient_id,\n    protein_A = rlnorm(n_patients, 3, 0.5),\n    protein_B = rlnorm(n_patients, 4, 0.8),\n    protein_C = rlnorm(n_patients, 3.5, 0.6)\n  )\n  \n  # Merge data\n  combined_data &lt;- merge(patients, biomarker_data, by = \"patient_id\")\n  \n  # Simulate survival times\n  # Higher protein levels associated with better survival\n  hazard_ratio &lt;- exp(-0.5 * scale(combined_data$protein_A)[,1] + \n                     0.3 * (combined_data$stage == \"IV\") +\n                     0.2 * (combined_data$age &gt; 70))\n  \n  # Generate survival times using exponential distribution\n  survival_times &lt;- rexp(n_patients, rate = hazard_ratio * 0.1)\n  \n  # Generate censoring (assume some patients are still alive at end of study)\n  max_followup &lt;- 60  # months\n  event_status &lt;- ifelse(survival_times &lt; max_followup, 1, 0)\n  observed_times &lt;- pmin(survival_times, max_followup)\n  \n  combined_data$time &lt;- observed_times\n  combined_data$event = event_status\n  \n  # Categorize biomarker levels\n  combined_data$protein_A_high &lt;- combined_data$protein_A &gt; median(combined_data$protein_A)\n  \n  return(combined_data)\n}\n\nsurvival_data &lt;- simulate_survival_data()\n\n# Perform survival analysis\nsurv_object &lt;- Surv(time = survival_data$time, event = survival_data$event)\n\n# Kaplan-Meier analysis\nkm_fit &lt;- survfit(surv_object ~ protein_A_high, data = survival_data)\n\n# Plot survival curves\nggsurvplot(km_fit, \n          data = survival_data,\n          pval = TRUE,\n          conf.int = TRUE,\n          xlab = \"Time (months)\",\n          ylab = \"Overall Survival Probability\",\n          title = \"Survival Analysis by Protein A Level\",\n          legend.title = \"Protein A\",\n          legend.labs = c(\"Low\", \"High\"))\n\n\n\n\n\n\n\n# Cox proportional hazards model\ncox_model &lt;- coxph(surv_object ~ protein_A + protein_B + protein_C + \n                  age + gender + stage, data = survival_data)\n\nsummary(cox_model)\n\nCall:\ncoxph(formula = surv_object ~ protein_A + protein_B + protein_C + \n    age + gender + stage, data = survival_data)\n\n  n= 200, number of events= 195 \n\n                coef  exp(coef)   se(coef)      z Pr(&gt;|z|)    \nprotein_A -0.0381232  0.9625943  0.0065579 -5.813 6.12e-09 ***\nprotein_B -0.0009910  0.9990095  0.0009303 -1.065    0.287    \nprotein_C -0.0038282  0.9961791  0.0026701 -1.434    0.152    \nage        0.0011187  1.0011194  0.0073091  0.153    0.878    \ngenderM    0.0755537  1.0784811  0.1492284  0.506    0.613    \nstageII    0.1418220  1.1523715  0.2093917  0.677    0.498    \nstageIII  -0.0572136  0.9443923  0.2076276 -0.276    0.783    \nstageIV    0.4426659  1.5568521  0.2545010  1.739    0.082 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n          exp(coef) exp(-coef) lower .95 upper .95\nprotein_A    0.9626     1.0389    0.9503     0.975\nprotein_B    0.9990     1.0010    0.9972     1.001\nprotein_C    0.9962     1.0038    0.9910     1.001\nage          1.0011     0.9989    0.9869     1.016\ngenderM      1.0785     0.9272    0.8050     1.445\nstageII      1.1524     0.8678    0.7645     1.737\nstageIII     0.9444     1.0589    0.6287     1.419\nstageIV      1.5569     0.6423    0.9454     2.564\n\nConcordance= 0.606  (se = 0.024 )\nLikelihood ratio test= 52.71  on 8 df,   p=1e-08\nWald test            = 39.87  on 8 df,   p=3e-06\nScore (logrank) test = 40.17  on 8 df,   p=3e-06\n\n\n\n\n10.7.2 Network Analysis\n\nlibrary(igraph)\n\n# Create protein-protein interaction network from correlation data\ncreate_protein_network &lt;- function(correlation_matrix, threshold = 0.7) {\n  # Convert correlation matrix to adjacency matrix\n  adj_matrix &lt;- abs(correlation_matrix) &gt; threshold\n  diag(adj_matrix) &lt;- FALSE\n  \n  # Create igraph object\n  network &lt;- graph_from_adjacency_matrix(adj_matrix, mode = \"undirected\")\n  \n  # Add edge weights\n  edges &lt;- get.edgelist(network)\n  edge_weights &lt;- numeric(nrow(edges))\n  \n  for (i in 1:nrow(edges)) {\n    from_node &lt;- edges[i, 1]\n    to_node &lt;- edges[i, 2]\n    edge_weights[i] &lt;- abs(correlation_matrix[from_node, to_node])\n  }\n  \n  E(network)$weight &lt;- edge_weights\n  \n  return(network)\n}\n\n# Create example correlation matrix from earlier proteomics data\nif (exists(\"normalized_intensities\")) {\n  # Calculate correlation for subset of proteins\n  protein_subset &lt;- normalized_intensities[1:20, ]  # Use first 20 proteins\n  protein_cor &lt;- cor(t(log2(protein_subset)), use = \"complete.obs\")\n  \n  # Create network\n  protein_network &lt;- create_protein_network(protein_cor, threshold = 0.6)\n  \n  # Network analysis\n  cat(\"Network properties:\\n\")\n  cat(\"  Nodes:\", vcount(protein_network), \"\\n\")\n  cat(\"  Edges:\", ecount(protein_network), \"\\n\")\n  cat(\"  Density:\", edge_density(protein_network), \"\\n\")\n  \n  # Calculate centrality measures\n  betweenness_centrality &lt;- betweenness(protein_network)\n  degree_centrality &lt;- degree(protein_network)\n  \n  # Plot network\n  if (ecount(protein_network) &gt; 0) {\n    plot(protein_network, \n         vertex.size = degree_centrality * 3 + 5,\n         vertex.color = \"lightblue\",\n         vertex.label.cex = 0.8,\n         edge.width = E(protein_network)$weight * 3,\n         main = \"Protein Correlation Network\",\n         layout = layout_with_fr)\n  }\n}",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "09-advanced-topics.html#method-development-and-validation",
    "href": "09-advanced-topics.html#method-development-and-validation",
    "title": "10  Advanced Topics and Applications",
    "section": "10.8 Method Development and Validation",
    "text": "10.8 Method Development and Validation\n\n10.8.1 Analytical Method Validation\n\n# Simulate analytical validation data\nsimulate_validation_data &lt;- function() {\n  # Calibration standards\n  concentrations &lt;- c(0, 0.1, 0.5, 1, 5, 10, 50, 100)  # ng/mL\n  \n  validation_data &lt;- list()\n  \n  # Linearity assessment\n  linearity_data &lt;- data.frame()\n  for (conc in concentrations) {\n    for (replicate in 1:5) {\n      # Simulate instrument response with some noise\n      true_response &lt;- 1000 * conc + 500  # Linear relationship\n      observed_response &lt;- true_response + rnorm(1, 0, true_response * 0.05)  # 5% CV\n      \n      linearity_data &lt;- rbind(linearity_data, data.frame(\n        concentration = conc,\n        response = observed_response,\n        replicate = replicate\n      ))\n    }\n  }\n  \n  # Precision (repeatability and intermediate precision)\n  precision_data &lt;- data.frame()\n  test_concentrations &lt;- c(1, 10, 50)  # Low, medium, high\n  \n  for (day in 1:3) {\n    for (conc in test_concentrations) {\n      for (replicate in 1:6) {\n        true_response &lt;- 1000 * conc + 500\n        day_effect &lt;- rnorm(1, 0, true_response * 0.02)  # Day-to-day variation\n        observed_response &lt;- true_response + day_effect + \n                           rnorm(1, 0, true_response * 0.03)  # Within-day variation\n        \n        precision_data &lt;- rbind(precision_data, data.frame(\n          day = day,\n          concentration = conc,\n          response = observed_response,\n          replicate = replicate\n        ))\n      }\n    }\n  }\n  \n  # Accuracy (spike recovery)\n  accuracy_data &lt;- data.frame()\n  spike_levels &lt;- c(0.5, 5, 50)  # ng/mL\n  \n  for (spike in spike_levels) {\n    for (replicate in 1:5) {\n      expected_response &lt;- 1000 * spike + 500\n      recovery_rate &lt;- runif(1, 0.85, 1.15)  # 85-115% recovery\n      observed_response &lt;- expected_response * recovery_rate\n      \n      accuracy_data &lt;- rbind(accuracy_data, data.frame(\n        spiked_concentration = spike,\n        observed_response = observed_response,\n        expected_response = expected_response,\n        recovery_percent = recovery_rate * 100,\n        replicate = replicate\n      ))\n    }\n  }\n  \n  return(list(\n    linearity = linearity_data,\n    precision = precision_data,\n    accuracy = accuracy_data\n  ))\n}\n\nvalidation_results &lt;- simulate_validation_data()\n\n# Linearity assessment\nlinearity_summary &lt;- validation_results$linearity %&gt;%\n  group_by(concentration) %&gt;%\n  summarise(\n    mean_response = mean(response),\n    sd_response = sd(response),\n    cv_percent = sd(response) / mean(response) * 100,\n    .groups = 'drop'\n  )\n\n# Linear regression\nlm_model &lt;- lm(mean_response ~ concentration, data = linearity_summary)\nr_squared &lt;- summary(lm_model)$r.squared\n\n# Plot linearity\nggplot(validation_results$linearity, aes(x = concentration, y = response)) +\n  geom_point(alpha = 0.6, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(title = paste(\"Linearity Assessment (R² =\", round(r_squared, 4), \")\"),\n       x = \"Concentration (ng/mL)\", y = \"Instrument Response\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Precision assessment\nprecision_summary &lt;- validation_results$precision %&gt;%\n  group_by(concentration, day) %&gt;%\n  summarise(\n    mean_response = mean(response),\n    sd_response = sd(response),\n    cv_percent = sd(response) / mean(response) * 100,\n    .groups = 'drop'\n  )\n\n# Overall precision summary\noverall_precision &lt;- validation_results$precision %&gt;%\n  group_by(concentration) %&gt;%\n  summarise(\n    repeatability_cv = sd(response) / mean(response) * 100,\n    .groups = 'drop'\n  )\n\nprint(\"Precision Assessment:\")\n\n[1] \"Precision Assessment:\"\n\nprint(overall_precision)\n\n# A tibble: 3 × 2\n  concentration repeatability_cv\n          &lt;dbl&gt;            &lt;dbl&gt;\n1             1             3.53\n2            10             3.36\n3            50             2.81\n\n# Accuracy assessment\naccuracy_summary &lt;- validation_results$accuracy %&gt;%\n  group_by(spiked_concentration) %&gt;%\n  summarise(\n    mean_recovery = mean(recovery_percent),\n    sd_recovery = sd(recovery_percent),\n    .groups = 'drop'\n  )\n\nprint(\"Accuracy Assessment:\")\n\n[1] \"Accuracy Assessment:\"\n\nprint(accuracy_summary)\n\n# A tibble: 3 × 3\n  spiked_concentration mean_recovery sd_recovery\n                 &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1                  0.5          98.8       10.0 \n2                  5           102.         9.55\n3                 50            99.3       10.5 \n\n\n\n\n10.8.2 Quality Control Charts\n\n# Create QC charts for method monitoring\ncreate_qc_charts &lt;- function(qc_data) {\n  # Simulate QC data over time\n  n_days &lt;- 30\n  qc_measurements &lt;- data.frame()\n  \n  for (day in 1:n_days) {\n    # QC low level\n    qc_low &lt;- rnorm(1, 950, 50)  # Target = 1000, ±5%\n    \n    # QC medium level  \n    qc_medium &lt;- rnorm(1, 9500, 300)  # Target = 10000, ±3%\n    \n    # QC high level\n    qc_high &lt;- rnorm(1, 47500, 1200)  # Target = 50000, ±2.5%\n    \n    qc_measurements &lt;- rbind(qc_measurements, data.frame(\n      day = day,\n      qc_level = rep(c(\"Low\", \"Medium\", \"High\"), each = 1),\n      measurement = c(qc_low, qc_medium, qc_high),\n      target = c(1000, 10000, 50000),\n      ucl = c(1100, 10300, 51250),  # Upper control limit\n      lcl = c(900, 9700, 48750),    # Lower control limit\n      uwl = c(1050, 10150, 50625),  # Upper warning limit\n      lwl = c(950, 9850, 49375)     # Lower warning limit\n    ))\n  }\n  \n  return(qc_measurements)\n}\n\nqc_data &lt;- create_qc_charts()\n\n# Plot QC charts\nggplot(qc_data, aes(x = day, y = measurement)) +\n  geom_line(color = \"blue\") +\n  geom_point(aes(color = ifelse(measurement &gt; ucl | measurement &lt; lcl, \"Out of Control\", \"In Control\"))) +\n  geom_hline(aes(yintercept = target), color = \"green\", linetype = \"solid\") +\n  geom_hline(aes(yintercept = ucl), color = \"red\", linetype = \"dashed\") +\n  geom_hline(aes(yintercept = lcl), color = \"red\", linetype = \"dashed\") +\n  geom_hline(aes(yintercept = uwl), color = \"orange\", linetype = \"dotted\") +\n  geom_hline(aes(yintercept = lwl), color = \"orange\", linetype = \"dotted\") +\n  facet_wrap(~qc_level, scales = \"free_y\") +\n  scale_color_manual(values = c(\"In Control\" = \"blue\", \"Out of Control\" = \"red\")) +\n  labs(title = \"Quality Control Charts\",\n       x = \"Day\", y = \"QC Measurement\",\n       color = \"QC Status\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "09-advanced-topics.html#exercises",
    "href": "09-advanced-topics.html#exercises",
    "title": "10  Advanced Topics and Applications",
    "section": "10.9 Exercises",
    "text": "10.9 Exercises\n\nImplement a deep learning model for mass spectral classification\nDevelop an algorithm for automatic peak alignment across multiple samples\nCreate a method for isotope pattern recognition and deconvolution\nBuild a comprehensive data processing pipeline with quality control\nImplement real-time data analysis for online MS monitoring",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "09-advanced-topics.html#summary",
    "href": "09-advanced-topics.html#summary",
    "title": "10  Advanced Topics and Applications",
    "section": "10.10 Summary",
    "text": "10.10 Summary\nThis chapter covered advanced topics in mass spectrometry data analysis, including machine learning applications, ion mobility spectrometry, survival analysis, network analysis, and analytical method validation. These advanced techniques enable sophisticated analysis of complex MS datasets and support method development and validation efforts.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "11  Summary and Future Directions",
    "section": "",
    "text": "11.1 What We’ve Covered\nThis book has provided a comprehensive journey through mass spectrometry data analysis using R and the R for Mass Spectrometry ecosystem. Let’s review the key concepts and look toward future developments.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#what-weve-covered",
    "href": "summary.html#what-weve-covered",
    "title": "11  Summary and Future Directions",
    "section": "",
    "text": "11.1.1 Core Infrastructure (Chapters 1-2)\n\nR Fundamentals: Package installation, data structures, and the Bioconductor ecosystem\nData Formats: Working with mzML, MGF, and other MS file formats using Spectra\nBackend Architecture: Understanding MsBackendMzR, MsBackendDataFrame, and MsBackendHdf5Peaks for efficient data storage\n\n\n\n11.1.2 Data Processing (Chapters 3-4)\n\nPreprocessing: Baseline correction, smoothing (Savitzky-Golay), and noise reduction\nPeak Detection: MAD-based peak picking, noise estimation, and signal-to-noise calculations\nQuantification: Peak integration, area calculation, and quality metrics\n\n\n\n11.1.3 Analysis and Visualization (Chapters 5-6)\n\nVisualization: Spectral plots, chromatograms (TIC/BPC), mirror plots, and interactive graphics\nStatistical Methods: Descriptive statistics, PCA, clustering, differential analysis with limma\nQuality Control: CV analysis, missing value patterns, batch effect detection\n\n\n\n11.1.4 Application Areas (Chapters 7-8)\n\nMetabolomics: XCMS workflows, peak detection with CentWave, retention time correction, and correspondence\nProteomics: PSM handling, protein inference, database searching, and peptide-centric analysis\nQuantitative Proteomics: QFeatures framework for hierarchical data (PSMs → peptides → proteins)\n\n\n\n11.1.5 Advanced Topics (Chapters 9-10)\n\nBackend Management: Choosing appropriate backends for different data scales\nParallel Processing: BiocParallel for large-scale data processing\nQFeatures Workflows: Aggregation strategies, missing value handling, robust summarization\nIntegration: Connecting to online resources (GNPS, MassBank, MetaboLights)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#key-packages-in-the-r-for-mass-spectrometry-ecosystem",
    "href": "summary.html#key-packages-in-the-r-for-mass-spectrometry-ecosystem",
    "title": "11  Summary and Future Directions",
    "section": "11.2 Key Packages in the R for Mass Spectrometry Ecosystem",
    "text": "11.2 Key Packages in the R for Mass Spectrometry Ecosystem\n\n\n\n\n\n\n\n\n\n\nPackage\nPurpose\nKey Functions\n\n\n\n\nSpectra\nCore MS data infrastructure and spectral data handling\nSpectra(), filterMsLevel(), pickPeaks()\n\n\nQFeatures\nQuantitative features for proteomics workflows\nQFeatures(), aggregateFeatures(), filterNA()\n\n\nxcms\nLC-MS data processing and metabolomics\nfindChromPeaks(), adjustRtime(), groupChromPeaks()\n\n\nPSMatch\nPeptide-spectrum matching and protein identification\nPSM(), addFragments(), filterPSMs()\n\n\nMsCoreUtils\nCore utilities for MS data processing\nnoise(), compareSpectra(), robustSummary()\n\n\nMetaboCoreUtils\nUtilities specific to metabolomics analysis\nmass2mz(), calculateMass(), adductNames()\n\n\nProtGenerics\nGeneric functions for proteomics packages\nspectra(), peaks(), intensity()\n\n\nmsdata\nExample MS datasets for learning and testing\nproteomics(), sciex(), msdata()\n\n\nMsDataHub\nAccess to online MS data resources\nMsDataHub(), query(), recordTitle()",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#best-practices-for-ms-data-analysis-in-r",
    "href": "summary.html#best-practices-for-ms-data-analysis-in-r",
    "title": "11  Summary and Future Directions",
    "section": "11.3 Best Practices for MS Data Analysis in R",
    "text": "11.3 Best Practices for MS Data Analysis in R\n\n11.3.1 1. Choose the Right Backend\n\n# Small datasets: In-memory for speed\nsmall_data &lt;- Spectra(files, backend = MsBackendDataFrame())\n\n# Large datasets: On-disk for memory efficiency\nlarge_data &lt;- Spectra(files, backend = MsBackendMzR())\n\n# Very large or processed: HDF5 for balanced performance\nlibrary(MsBackendHdf5Peaks)\narchived_data &lt;- setBackend(data, MsBackendHdf5Peaks())\n\n\n\n11.3.2 2. Implement Quality Control\n\nCheck coefficient of variation (CV &lt; 30% for technical replicates)\nAssess missing value patterns\nMonitor batch effects with PCA\nValidate feature detection rates\n\n\n\n11.3.3 3. Use Appropriate Normalization\n\nMedian normalization: General purpose, robust to outliers\nTIC normalization: For consistent total signal across samples\nQuantile normalization: When distributions should be identical\nInternal standards: When available, most accurate\n\n\n\n11.3.4 4. Proper Statistical Testing\n\nUse limma for differential analysis (handles small sample sizes)\nApply multiple testing correction (FDR/Benjamini-Hochberg)\nCheck assumptions (normality, homoscedasticity)\nConsider batch effects in design matrix",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#reproducible-research-practices",
    "href": "summary.html#reproducible-research-practices",
    "title": "11  Summary and Future Directions",
    "section": "11.4 Reproducible Research Practices",
    "text": "11.4 Reproducible Research Practices\n\n# Document your analysis pipeline\n# 1. Record package versions\nsessionInfo()\n\n# 2. Use project-based workflows\nlibrary(here)\ndata_path &lt;- here(\"data\", \"raw\")\n\n# 3. Version control your analysis\n# Use Git for tracking changes\n\n# 4. Create R Markdown/Quarto reports\n# This entire book is an example!\n\n# 5. Share data and code\n# Deposit raw data in public repositories (PRIDE, MetaboLights)\n# Share analysis code on GitHub",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#future-directions-in-ms-data-analysis",
    "href": "summary.html#future-directions-in-ms-data-analysis",
    "title": "11  Summary and Future Directions",
    "section": "11.5 Future Directions in MS Data Analysis",
    "text": "11.5 Future Directions in MS Data Analysis\n\n11.5.1 Emerging Technologies\n\nIon Mobility MS: Additional separation dimension requiring new algorithms\nImaging MS: Spatial metabolomics and proteomics visualization\nTop-Down Proteomics: Intact protein analysis without digestion\nData-Independent Acquisition (DIA): Comprehensive MS/MS coverage\n\n\n\n11.5.2 Computational Advances\n\nDeep Learning: Neural networks for spectrum prediction and identification\nCloud Computing: Scalable processing of large cohort studies\nReal-Time Analysis: Online processing for quality control\nIntegration: Multi-omics data fusion (proteomics + metabolomics + genomics)\n\n\n\n11.5.3 Community Development\nThe R for Mass Spectrometry initiative continues to evolve:\n\nNew backends for emerging data formats\nEnhanced visualization capabilities\nImproved integration with online databases\nBetter support for non-standard MS applications",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#resources-for-continued-learning",
    "href": "summary.html#resources-for-continued-learning",
    "title": "11  Summary and Future Directions",
    "section": "11.6 Resources for Continued Learning",
    "text": "11.6 Resources for Continued Learning\n\n11.6.1 Official Documentation\n\nR for Mass Spectrometry Book: https://rformassspectrometry.github.io/book/\nSpectra Documentation: https://rformassspectrometry.github.io/Spectra/\nxcms Documentation: https://bioconductor.org/packages/xcms/\n\n\n\n11.6.2 Community\n\nBioconductor Support: https://support.bioconductor.org/\nR for Mass Spectrometry GitHub: https://github.com/RforMassSpectrometry\nMetabolomics Society: https://metabolomicssociety.org/\n\n\n\n11.6.3 Publications\nKey papers describing the R for Mass Spectrometry ecosystem provide deeper technical details and validation studies. Check package citations using citation(\"packagename\").",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#final-thoughts",
    "href": "summary.html#final-thoughts",
    "title": "11  Summary and Future Directions",
    "section": "11.7 Final Thoughts",
    "text": "11.7 Final Thoughts\nMass spectrometry data analysis is a rapidly evolving field. The R for Mass Spectrometry ecosystem provides a robust, flexible, and open-source foundation for tackling both routine and cutting-edge analytical challenges.\nThe skills you’ve developed through this book - from basic data import to advanced statistical analysis - will serve as a strong foundation for your research. Remember:\n\nStart simple: Use built-in functions before implementing custom solutions\nValidate thoroughly: Test your analysis pipeline with known standards\nDocument everything: Future you (and collaborators) will be grateful\nEngage with the community: Share code, ask questions, contribute improvements\n\nThank you for joining this journey through R for Mass Spectrometry. Now, go forth and analyze!\n\n\nHappy analyzing! 🔬📊",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  }
]