[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Mass Spectrometry",
    "section": "",
    "text": "Preface\nWelcome to “R for Mass Spectrometry” - a comprehensive guide to analyzing mass spectrometry data using the R programming language and its rich ecosystem of specialized packages.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "R for Mass Spectrometry",
    "section": "About This Book",
    "text": "About This Book\nMass spectrometry (MS) has become an indispensable tool in analytical chemistry, proteomics, metabolomics, and many other scientific disciplines. As the complexity and volume of MS data continue to grow, computational tools for data processing and analysis have become essential. R, with its extensive statistical capabilities and specialized packages for mass spectrometry, provides an excellent platform for comprehensive MS data analysis.\nThis book aims to bridge the gap between mass spectrometry theory and practical computational implementation, providing readers with both conceptual understanding and hands-on experience in MS data analysis using R.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-this-book-is-for",
    "href": "index.html#who-this-book-is-for",
    "title": "R for Mass Spectrometry",
    "section": "Who This Book Is For",
    "text": "Who This Book Is For\nThis book is designed for:\n\n\nGraduate students in analytical chemistry, biochemistry, or related fields\n\nResearchers working with mass spectrometry data\n\nData scientists entering the field of analytical chemistry\n\nBioinformaticians specializing in proteomics or metabolomics\n\nAnyone interested in learning computational approaches to MS data analysis",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "R for Mass Spectrometry",
    "section": "Prerequisites",
    "text": "Prerequisites\nReaders should have:\n\nBasic knowledge of R programming\nFamiliarity with fundamental mass spectrometry concepts\nUnderstanding of basic statistics\nExperience with data analysis workflows (helpful but not required)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-youll-learn",
    "href": "index.html#what-youll-learn",
    "title": "R for Mass Spectrometry",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\nBy the end of this book, you will be able to:\n\nSet up and configure R environments for MS data analysis\nImport, process, and visualize various MS data formats\nImplement spectral preprocessing and peak detection algorithms\nPerform statistical analysis of MS datasets\nConduct metabolomics and proteomics data analysis workflows\nApply machine learning techniques to MS data\nDevelop and validate analytical methods\nCreate reproducible analysis pipelines",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#book-structure",
    "href": "index.html#book-structure",
    "title": "R for Mass Spectrometry",
    "section": "Book Structure",
    "text": "Book Structure\nThe book is organized into progressively advanced topics:\n\n\nFundamentals - R basics and MS data structures\n\nData Handling - File formats and data import/export\n\nPreprocessing - Spectral cleaning and preparation\n\nPeak Analysis - Detection and quantification methods\n\nVisualization - Creating informative plots and graphics\n\nStatistics - Hypothesis testing and multivariate analysis\n\nMetabolomics - Specialized workflows for metabolite analysis\n\nProteomics - Protein identification and quantification\n\nAdvanced Topics - Machine learning and method development",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "R for Mass Spectrometry",
    "section": "Getting Started",
    "text": "Getting Started\nTo follow along with the examples in this book, you’ll need to install R and several specialized packages. Installation instructions and package setup are covered in the first chapter.\n\nCode# Example of loading key packages\nlibrary(Spectra)\nlibrary(xcms)\nlibrary(tidyverse)\nlibrary(ggplot2)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "R for Mass Spectrometry",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis book builds upon the excellent work of the R for Mass Spectrometry community and the developers of key packages including Spectra, xcms, MSnbase, and many others.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#feedback-and-updates",
    "href": "index.html#feedback-and-updates",
    "title": "R for Mass Spectrometry",
    "section": "Feedback and Updates",
    "text": "Feedback and Updates\nThis book is a living document. Please report errors, suggest improvements, or request additional topics through the book’s repository.\nLet’s begin our journey into the world of mass spectrometry data analysis with R!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition",
    "section": "",
    "text": "1.1 MS Data Analysis Workflow\nThe following diagram illustrates the comprehensive workflow for mass spectrometry data analysis in R:\nCode\nflowchart TB\n    subgraph Sample[\"Sample & Instrument\"]\n        A[Sample Preparation] --&gt; B[Chromatography&lt;br/&gt;LC/GC]\n        B --&gt; C[Ionization&lt;br/&gt;ESI/APCI/MALDI]\n        C --&gt; D[Mass Analyzer&lt;br/&gt;Orbitrap/TOF/Q]\n        D --&gt; E[Detector]\n    end\n    \n    subgraph RawData[\"Raw Data Files\"]\n        E --&gt; F[mzML/mzXML Files]\n    end\n    \n    subgraph RPackages[\"R for Mass Spectrometry\"]\n        F --&gt; G[Spectra Package&lt;br/&gt;Data Import]\n        G --&gt; H{Analysis Type?}\n        \n        H --&gt;|Proteomics| P1[PSM Matching&lt;br/&gt;PSMatch]\n        P1 --&gt; P2[Peptide ID&lt;br/&gt;Protein Inference]\n        P2 --&gt; P3[Quantification&lt;br/&gt;QFeatures]\n        \n        H --&gt;|Metabolomics| M1[Peak Detection&lt;br/&gt;xcms]\n        M1 --&gt; M2[Retention Time&lt;br/&gt;Correction]\n        M2 --&gt; M3[Feature Matching&lt;br/&gt;CAMERA]\n        \n        H --&gt;|Spectral Processing| S1[Preprocessing&lt;br/&gt;MsCoreUtils]\n        S1 --&gt; S2[Peak Picking]\n        S2 --&gt; S3[Baseline Correction]\n    end\n    \n    subgraph Analysis[\"Statistical Analysis\"]\n        P3 --&gt; SA[Feature Matrix]\n        M3 --&gt; SA\n        S3 --&gt; SA\n        SA --&gt; SB[Quality Control&lt;br/&gt;Missing Values]\n        SB --&gt; SC[Normalization]\n        SC --&gt; SD[Statistical Tests&lt;br/&gt;limma/DEqMS]\n    end\n    \n    subgraph Viz[\"Visualization & Results\"]\n        SD --&gt; V1[PCA/Clustering&lt;br/&gt;factoextra]\n        SD --&gt; V2[Volcano Plots&lt;br/&gt;ggplot2]\n        SD --&gt; V3[Heatmaps&lt;br/&gt;pheatmap]\n        V1 --&gt; OUT[Biological&lt;br/&gt;Interpretation]\n        V2 --&gt; OUT\n        V3 --&gt; OUT\n    end\n    \n    style Sample fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n    style RawData fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n    style RPackages fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n    style Analysis fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n    style Viz fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n\n\n\n\n\nflowchart TB\n    subgraph Sample[\"Sample & Instrument\"]\n        A[Sample Preparation] --&gt; B[Chromatography&lt;br/&gt;LC/GC]\n        B --&gt; C[Ionization&lt;br/&gt;ESI/APCI/MALDI]\n        C --&gt; D[Mass Analyzer&lt;br/&gt;Orbitrap/TOF/Q]\n        D --&gt; E[Detector]\n    end\n    \n    subgraph RawData[\"Raw Data Files\"]\n        E --&gt; F[mzML/mzXML Files]\n    end\n    \n    subgraph RPackages[\"R for Mass Spectrometry\"]\n        F --&gt; G[Spectra Package&lt;br/&gt;Data Import]\n        G --&gt; H{Analysis Type?}\n        \n        H --&gt;|Proteomics| P1[PSM Matching&lt;br/&gt;PSMatch]\n        P1 --&gt; P2[Peptide ID&lt;br/&gt;Protein Inference]\n        P2 --&gt; P3[Quantification&lt;br/&gt;QFeatures]\n        \n        H --&gt;|Metabolomics| M1[Peak Detection&lt;br/&gt;xcms]\n        M1 --&gt; M2[Retention Time&lt;br/&gt;Correction]\n        M2 --&gt; M3[Feature Matching&lt;br/&gt;CAMERA]\n        \n        H --&gt;|Spectral Processing| S1[Preprocessing&lt;br/&gt;MsCoreUtils]\n        S1 --&gt; S2[Peak Picking]\n        S2 --&gt; S3[Baseline Correction]\n    end\n    \n    subgraph Analysis[\"Statistical Analysis\"]\n        P3 --&gt; SA[Feature Matrix]\n        M3 --&gt; SA\n        S3 --&gt; SA\n        SA --&gt; SB[Quality Control&lt;br/&gt;Missing Values]\n        SB --&gt; SC[Normalization]\n        SC --&gt; SD[Statistical Tests&lt;br/&gt;limma/DEqMS]\n    end\n    \n    subgraph Viz[\"Visualization & Results\"]\n        SD --&gt; V1[PCA/Clustering&lt;br/&gt;factoextra]\n        SD --&gt; V2[Volcano Plots&lt;br/&gt;ggplot2]\n        SD --&gt; V3[Heatmaps&lt;br/&gt;pheatmap]\n        V1 --&gt; OUT[Biological&lt;br/&gt;Interpretation]\n        V2 --&gt; OUT\n        V3 --&gt; OUT\n    end\n    \n    style Sample fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n    style RawData fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n    style RPackages fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n    style Analysis fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n    style Viz fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition</span>"
    ]
  },
  {
    "objectID": "intro.html#ms-data-analysis-workflow",
    "href": "intro.html#ms-data-analysis-workflow",
    "title": "1  Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition",
    "section": "",
    "text": "Key Workflow Components\n\n\n\n\nSample & Instrument: Physical sample preparation through mass analyzer detection\nRaw Data: Standard MS data formats (mzML, mzXML, MGF)\nR Packages: Modular ecosystem for specific analysis types\nStatistical Analysis: QC, normalization, and hypothesis testing\nVisualization: Comprehensive plotting for interpretation",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition</span>"
    ]
  },
  {
    "objectID": "intro.html#the-fundamental-principle-measuring-molecular-mass",
    "href": "intro.html#the-fundamental-principle-measuring-molecular-mass",
    "title": "1  Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition",
    "section": "1.2 1. The Fundamental Principle: Measuring Molecular Mass",
    "text": "1.2 1. The Fundamental Principle: Measuring Molecular Mass\nMass spectrometry (MS) is a foundational technique for analyzing the mass-to-charge (m/z) ratio of ions, enabling precise identification and quantification of both small molecules and biomolecules.[7, 15, 11] The core process converts neutral analytes into gas-phase ions that can be manipulated by electromagnetic fields, providing a quantitative bridge between molecular structure and measured signal.\n\nKey Applications:\n\nIdentification of unknown compounds\nQuantification of target analytes\nStructural elucidation through fragmentation analysis\n\n\nReference: de Hoffmann & Stroobant, Mass Spectrometry: Principles and Applications, 3rd Ed. (Wiley, 2007).[7, 15]\nThe sections that follow expand this high-level view with practical considerations for instrumentation, sample handling, and data acquisition.\n\n1.2.1 1.1 Defining the Analyte: What is Mass Spectrometry?\nMass spectrometry (MS) is a powerful analytical technology used to measure the mass-to-charge ratio (m/z) of ions.[1, 2] This fundamental measurement, taken from a sample, allows for the precise calculation of the molecular weights of the constituent components.[1, 3]\nFrom this single, core capability, the primary applications of the technique are derived. Mass spectrometry is used to identify unknown compounds, to quantify known compounds, and to elucidate the chemical structure of molecules.[1, 2]\nThe defining principle of mass spectrometry is not “weighing” neutral molecules, but rather manipulating them. A neutral molecule is effectively invisible to the instrument. The entire process is contingent upon the successful conversion of a neutral analyte molecule, which may be in a solid, liquid, or gaseous state, into a gas-phase ion.[2, 4] It is only because ions possess a charge that they can be moved, accelerated, focused, and manipulated by external electric and magnetic fields.[1, 4, 5] Thus, “mass spectrometry” is, perhaps more accurately, “ion spectrometry.”\n\n\n1.2.2 1.2 The Language of Mass Spectrometry: Understanding the Mass-to-Charge (m/z) Ratio\nThe primary output of a mass spectrometer is the mass spectrum. This is a plot, or histogram, showing the relative abundance or signal intensity of detected ions (y-axis) as a function of their mass-to-charge ratio, or m/z (x-axis).[3, 6, 7]\nThe m/z value is a dimensionless quantity, formally defined by IUPAC as the mass of an ion (in Daltons, Da) divided by its charge number (z).[8] A common misconception, particularly for new users, is to equate the x-axis with “mass.” This is only true in cases where the charge number, z, is equal to 1.[6] While this is common for many ionization techniques (such as Electron Ionization or Matrix-Assisted Laser Desorption/Ionization), it is a critical error when interpreting data from other methods.\nSoft ionization techniques like Electrospray Ionization (ESI) are defined by their ability to create multiply-charged ions, such as [M + nH]^{n+}.[9, 10] A mass analyzer always measures the m/z ratio. For example, a 40,000 Da protein that has acquired 20 charges (a charge state of +20) will not appear at 40,000 on the x-axis; it will be detected at an m/z value of approximately 2,000. Understanding that m/z is the measured quantity and mass is the inferred property is the single most critical concept for interpreting spectra from modern soft ionization sources.\n\n\n1.2.3 1.3 Anatomy of a Mass Spectrometer: A Five-Part Journey\nWhile often simplified to three essential functions—ionization, sorting, and detection [1, 2, 4, 5]—a functional mass spectrometer relies on five distinct systems working in concert. The journey of an analyte from a sample vial to a data point involves:\n\nThe Sample Inlet: Introduces the sample to the instrument.\nThe Ion Source: Converts neutral analyte molecules into gas-phase ions.\nThe Mass Analyzer: Sorts the ions based on their m/z ratio.\nThe Detector: Measures the abundance of the sorted ions.\nThe Vacuum System: Maintains a low-pressure environment for ion manipulation.\n\nA central engineering challenge defines the architecture of all modern mass spectrometers: the atmospheric-vacuum conflict. A mass spectrometer must operate under a high vacuum (typically 10^{-5} to 10^{-8} torr).[4, 11, 12] This is a non-negotiable requirement. Ions are highly reactive and short-lived, and their “flight” path from the source to the detector must be free of collision with air molecules, which would otherwise scatter them or neutralize them.[4, 5]\nOur samples, however, exist at atmospheric pressure (760 torr).[4, 11] The core engineering marvel of modern MS, particularly when coupled with Liquid Chromatography (LC-MS), is the interface that bridges these two hostile worlds. The function of the inlet and ion source is not just to ionize, but to do so while successfully and efficiently transitioning the analyte from 760 torr to 10^{-8} torr—a pressure drop of over 100 million-fold.[13, 14]",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition</span>"
    ]
  },
  {
    "objectID": "intro.html#the-instrument-hardware-and-ion-physics",
    "href": "intro.html#the-instrument-hardware-and-ion-physics",
    "title": "1  Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition",
    "section": "1.3 2. The Instrument: Hardware and Ion Physics",
    "text": "1.3 2. The Instrument: Hardware and Ion Physics\nModern mass spectrometers integrate five principal systems that must operate in concert to deliver reliable data: the sample inlet, ion source, mass analyzer, detector, and vacuum infrastructure.[7] Each subsystem introduces specialized trade-offs—balancing robustness, sensitivity, resolution, and throughput—that dictate the suitability of an instrument for a particular application.\n\nSample Inlet: Transfers material from atmospheric pressure into the instrument without overwhelming the vacuum system.[7]\nIon Source: Converts neutral molecules to charged ions via hard (EI) or soft (ESI, APCI, MALDI) mechanisms.[8, 11]\nMass Analyzer: Separates ions by m/z, with quadrupole, TOF, and Orbitrap analyzers offering distinct combinations of resolution, scan speed, and mass range.[7, 9]\nDetector: Electron multipliers and Faraday cups transduce ion impacts into measurable electrical signals.[19]\nVacuum System: Maintains the mean-free path necessary to prevent ion-neutral collisions that would degrade sensitivity.[19]\n\nModern innovations—including high-field Orbitraps and hybrid Q-TOF or Q-Orbitrap designs—provide ultra-high resolution and accurate mass performance that underpin contemporary systems biology and omics research workflows.[9, 7, 15]\n\n1.3.1 2.1 Part 1: Sample Introduction – Bridging the World to the Vacuum\nThe inlet’s function is to introduce a small amount of the sample into the ion source with minimal disruption of the high vacuum.[13, 14] The choice of inlet represents the first major analytical decision, one that separates the analysis of pure or simple substances from that of complex mixtures.[12, 15, 16, 17]\n\n2.1.1 Direct Infusion and Direct Probes\nFor pure or simple, pre-purified samples, the inlet can be very simple. * Direct Infusion: The sample is dissolved in a solvent, placed in a syringe, and infused at a slow, continuous flow rate (e.g., 1-10 µL/min) directly into the ion source.[17] This is fast, simple, and provides an aggregate “snapshot” of the sample’s components without any prior separation. * Direct Probes: For non-volatile solids or liquids, the sample can be placed on the tip of a probe, which is then inserted through a vacuum lock directly into the ion source.[4, 12]\n\n\n2.1.2 Hyphenated Techniques (GC-MS and LC-MS)\nFor complex mixtures (e.g., blood plasma, environmental water, proteomic digests), direct infusion would result in a single, hopelessly complex mass spectrum. The solution is to “hyphenate” (couple) a chromatographic separation system to the mass spectrometer.[14, 15, 16] The gas chromatograph (GC) or liquid chromatograph (LC) acts as the inlet, providing a time-based separation of the mixture. The MS then acts as a highly sensitive and specific detector, analyzing the compounds as they elute from the column one by one.[16, 18] This adds a second dimension of data (retention time) to the analysis.\n\n\n2.1.3 The GC-MS Interface\nThis interface is relatively simple. A sample is injected into a hot GC inlet, where it is vaporized.[18, 19] An inert carrier gas (e.g., helium) carries the gaseous analytes through the GC column, where they are separated. The gas eluent from the column flows via a heated transfer line [19] directly into the ion source, which is located inside the high-vacuum chamber. The high-capacity turbomolecular pumps of the MS can easily handle the low gas flow (typically 1-2 mL/min) from the GC column, maintaining the high vacuum.[20]\n\n\n2.1.4 The LC-MS Interface and Atmospheric Pressure Ionization (API)\nThis interface is far more complex. LC uses a liquid mobile phase, often at flow rates of 0.2 to 1.0 mL/min.[21, 22] Injecting this volume of liquid directly into a 10^{-8} torr vacuum would instantly vaporize it, overwhelming the pumps and destroying the vacuum.\nThis problem was solved by the invention of Atmospheric Pressure Ionization (API).[21, 23, 24] Instead of attempting to force the liquid into the vacuum, ionization (e.g., ESI or APCI) occurs at atmospheric pressure (760 torr), outside the mass spectrometer’s vacuum chamber. The resulting cloud of gas-phase ions is then “sampled” into the instrument through a series of “skimmers” and “cones.” These are small orifices and ion lenses that separate chambers of progressively higher vacuum (e.g., 1 torr -&gt; 10^{-3} torr -&gt; 10^{-8} torr), each powered by its own set of pumps.[21, 25] This multi-stage pressure differential efficiently pulls the ions into the mass analyzer while pumping away the vast majority of neutral gas and solvent molecules. This API interface is the key technology that enables all modern LC-MS.\n\n\n\n1.3.2 2.2 Part 2: The Ion Source – Creating Gas-Phase Ions\nThe choice of ion source is the next critical experimental decision, as it dictates the type of data that will be obtained. Sources are broadly classified into two paradigms: “hard” and “soft” ionization.[26, 27, 28]\n\nHard Ionization: These techniques, such as Electron Ionization (EI), use a high amount of energy. This not only ionizes the molecule but also causes extensive and reproducible fragmentation. This fragmentation provides a rich “fingerprint” used for structural identification, but it may also be so energetic that the original, intact molecular ion is destroyed and not observed.[28, 29, 30, 31]\nSoft Ionization: These techniques, such as ESI, APCI, and MALDI, use a much gentler energy input. Their goal is to preserve the intact molecule, transferring just enough energy to create an ion (e.g., by adding a proton, [M+H]^{+}). This provides a clear, unambiguous signal for the molecular ion, allowing for easy determination of molecular weight, but it provides minimal fragmentation.[26, 29, 32]\n\n\n2.2.1 Hard Ionization: Electron Ionization (EI)\n\nPrinciple: EI is the classic ionization method and is used almost exclusively with GC-MS for volatile or semi-volatile compounds.[29, 33]\nMechanism: Inside the high-vacuum source, a filament (tungsten or rhenium) is heated to emit a steady stream of electrons. These electrons are accelerated by a voltage potential, creating a beam of high-energy electrons, standardized across all instruments at 70 electron volts (eV).[28, 34] As the neutral, gaseous analyte (M) from the GC passes through this beam, an electron collision ejects one of the analyte’s own electrons, creating a radical cation (M^{+\\bullet}).[28, 34, 35]\nFragmentation: The 70 eV of energy is far more than is needed for ionization (typically ~10 eV).[34] The excess energy is deposited into the newly formed M^{+\\bullet}, causing its bonds to shatter in a predictable, reproducible way.[29, 30] This fragmentation pattern is a unique “fingerprint” for that molecule’s structure.[28] This reproducibility allows the acquired spectrum to be matched against vast spectral libraries (e.g., NIST, Wiley) for positive compound identification.[29, 34]\nLimitations: The analyte must be volatile and thermally stable enough to be in the gas phase.[28, 35] It is generally limited to compounds below 600-1000 Da [35, 36], and the molecular ion is often weak or entirely absent.[30, 37]\n\n\n\n2.2.2 Soft Ionization (API): Electrospray Ionization (ESI)\n\nPrinciple: ESI is the most common and important LC-MS source.[33] It is ideal for analyzing polar, ionizable, and large biomolecules that are dissolved in a liquid mobile phase.\nMechanism: ESI is a liquid-phase ionization mechanism.[24] The liquid eluent from the LC (e.g., water and acetonitrile) is pumped through a fused-silica or stainless steel capillary. A high voltage (typically 4-5 kV) is applied to this capillary, creating a strong electric field at the tip.[9, 38, 39] This field forces the liquid to emerge as a fine “electrospray” of highly charged droplets.[29, 32] A counter-current of heated drying gas (e.g., nitrogen) and a heated capillary tube cause the solvent in the droplets to evaporate.[32] As the droplets shrink, the charge density on their surface increases. This continues until the “Rayleigh limit” is breached—the point at which the coulombic repulsion of the charges overcomes the droplet’s surface tension.[9] At this point, the droplet explodes, or, more commonly, ejects a stream of gas-phase ions that are then sampled into the mass spectrometer.[39]\nMultiple Charging: ESI’s defining characteristic and “superpower” is its ability to generate multiply-charged ions.[9, 10] For example, a large protein with many basic sites (like lysine and arginine) will acquire many protons, resulting in a series of ions like [M+10H]^{10+}, [M+11H]^{11+}, [M+12H]^{12+}, etc. This is a revolutionary advantage. A 50,000 Da protein with 50 positive charges ([M+50H]^{50+}) will have an m/z value of (50,000 + 50) / 50 ≈ 1001. This “cheat code” allows massive molecules—proteins, oligonucleotides, and polymers—to be analyzed by mass analyzers with a limited m/z range (e.g., a quadrupole that tops out at m/z 4000). This capability single-handedly enabled the field of proteomics and the analysis of large biomolecules.[32, 40]\n\n\n\n2.2.3 Soft Ionization (API): Atmospheric Pressure Chemical Ionization (APCI)\n\nPrinciple: APCI is a complementary LC-MS source, used for analytes that ESI struggles to ionize: namely, less polar, neutral, or non-polar small molecules.[29, 41, 42]\nMechanism: Unlike ESI, APCI is a gas-phase ionization mechanism.[24] The LC eluent first passes through a high-temperature nebulizer (a “vaporizer”), which converts the liquid stream into a hot aerosol of neutral analyte and solvent molecules.[23, 43] This aerosol then passes a corona discharge needle, which maintains a high voltage.[23, 24] This voltage is high enough to ionize the solvent and drying gas (e.g., N_2 + e^- \\rightarrow N_2^{+\\bullet}). These ionized solvent molecules (now a reagent gas) collide with the neutral analyte molecules (M) and ionize them via simple chemical reactions, most commonly proton transfer (e.g., ^+ + M \\rightarrow Solvent + [M+H]^+).[22, 29]\nESI vs. APCI Complementarity: ESI and APCI are not competitors; they are a complementary pair that together can analyze a vast range of compounds.[10, 26, 33] The operator chooses based on the analyte’s properties:\n\nUse ESI for: Polar, ionizable, thermally labile (cannot be heated), and large molecules (peptides, proteins, polar metabolites).[9, 32]\nUse APCI for: Less-polar, neutral, thermally stable (must survive vaporization) small molecules (steroids, lipids, pesticides, pharmaceuticals).[24, 41, 42]\n\n\n\n\n2.2.4 Soft Ionization (Vacuum): Matrix-Assisted Laser Desorption/Ionization (MALDI)\n\nPrinciple: MALDI is a solid-state, pulsed technique, distinct from the continuous-flow ESI and APCI.\nMechanism: The analyte is first mixed with a “matrix”—a small, organic, highly UV-absorbing compound (like sinapinic acid or 2,5-DHB).[29, 44] A droplet of this mixture is applied to a metal target plate and allowed to dry. The analyte and matrix co-crystallize.[45, 46] The plate is inserted into the MS vacuum. A pulsed laser (e.g., a nitrogen laser at 337 nm) [45, 46] is fired at the crystal spot. The matrix is chosen to “sacrificially” absorb this high-energy pulse, not the analyte.[29, 44, 47] This causes a “plume” of desorption and ablation, energetically carrying the intact, neutral analyte into the gas phase. In this hot, dense plume, proton transfer occurs from the ionized matrix molecules to the analyte, creating ions.[45]\nCharacteristics: MALDI almost exclusively produces singly-charged ions ([M+H]^{+}), even for very large molecules.[9, 26, 33] This makes its spectra very simple to interpret, as the m/z value is a direct measurement of the molecule’s mass (plus a proton).\nMALDI-TOF “Perfect Match”: MALDI is a pulsed technique (the laser fires in discrete shots).[33, 45] A Time-of-Flight (TOF) mass analyzer (see 2.3.2) is also a pulsed technique (it analyzes ions in “packets”). This makes them an ideal and ubiquitous instrumental pairing (MALDI-TOF).[33, 48, 49]\nApplication Highlight: MALDI Imaging (MSI): Because the laser can be precisely aimed, an operator can fire the laser in a rastering grid pattern (e.g., 50 µm x 50 µm) across an entire biological tissue section.[50] By acquiring a full mass spectrum at each x,y coordinate, the instrument can generate a label-free map showing the spatial distribution of any molecule (drugs, lipids, proteins) within the tissue.[26, 50, 51, 52, 53] This creates a “molecular picture,” or “molecular histology,” a powerful tool in pathology and pharmaceutical research.[53, 54]\n\n Table 1. Comparison of Common Ionization Techniques\n\n\n\n\n\n\n\n\n\n\n\nTechnique\nPrinciple\nIonization Phase\nType\nTypical Analytes\nKey Features & Common MS Pairing\n\n\n\n\nElectron Ionization (EI)\nHigh-energy electron bombardment (70 eV)\nGas-Phase (High Vacuum)\nHard\nVolatile/Semi-volatile small molecules (&lt;1000 Da)\nExtensive fragmentation (fingerprint); Molecular ion often absent. (GC-MS) [28, 29, 35]\n\n\nElectrospray Ionization (ESI)\nHigh-voltage electrospray of a liquid; solvent evaporation\nLiquid-Phase (Atmospheric)\nSoft\nPolar, ionizable, large molecules (peptides, proteins, metabolites)\nMultiple charging ([M+nH]^{n+}) is its key feature; analyzes massive molecules. (LC-MS) [9, 10, 38]\n\n\nAtmospheric Pressure Chemical Ionization (APCI)\nCorona discharge ionizes solvent, which transfers charge to analyte\nGas-Phase (Atmospheric)\nSoft\nLess-polar, neutral, thermally stable small molecules\nRequires analyte to be stable to vaporization; complements ESI. (LC-MS) [24, 29, 41, 43]\n\n\nMatrix-Assisted Laser Desorption/Ionization (MALDI)\nPulsed laser desorbs analyte and UV-absorbing matrix; proton transfer in plume\nSolid-State (Vacuum)\nSoft\nVery large molecules (proteins, polymers, oligonucleotides)\nProduces singly-charged ions ([M+H]^+); pulsed technique. (MALDI-TOF) [9, 26, 45]\n\n\n\n\n\n\n\n1.3.3 2.3 Part 3: The Mass Analyzer – Sorting Ions by m/z\nThe mass analyzer is the “heart” of the instrument.[4] It receives the continuous or pulsed stream of ions from the ion source and sorts them based on their m/z ratio.[1, 5] No single analyzer is “best.” A fundamental trade-off, an “iron triangle,” exists between three key performance metrics: Resolution (the ability to distinguish between two ions of very similar m/z), Scan Speed (how fast a full spectrum can be acquired), and Mass Range/Cost. The operator’s analytical goal dictates the correct tool.\n\n2.3.1 The Workhorse: Quadrupole Mass Filters\n\nPrinciple: A quadrupole consists of four parallel metal rods, precisely aligned.[55, 56]\nMechanism: A combination of an oscillating radio frequency (RF) voltage and a static DC voltage is applied to the rods.[55, 57] This creates a complex, oscillating electric field in the space between the rods. For a given RF/DC voltage ratio, only ions of a single, specific m/z value have a stable trajectory through the field and can pass through to the detector.[55, 56] All other ions—those with a slightly higher or lower m/z—have an “unstable” trajectory. Their oscillations grow exponentially until they collide with one of the rods and are neutralized.[58] It is, therefore, a “mass filter,” not a scanner.\nOperation: To acquire a full spectrum, the instrument does not scan in a single measurement. Instead, it rapidly sweeps the RF/DC voltages from low to high. This action allows m/z 100 to pass, then m/z 101, then m/z 102, etc., sequentially filtering each mass to the detector to build the spectrum over time.[58, 59] Quadrupoles are relatively inexpensive, robust, and fast, but are low-resolution analyzers.[48, 55]\n\n\n\n2.3.2 The Speed Demon: Time-of-Flight (TOF) Analyzers\n\nPrinciple: A TOF analyzer is an ion “race”.[11] The analyzer itself is a long (1-2 meter) “drift tube” or “flight tube,” which is kept at a high vacuum and is, crucially, free of any electric or magnetic fields.[60, 61, 62]\nMechanism: A “packet” of ions from the source is accelerated by a single, strong electric pulse. This pulse gives all ions, regardless of their mass, the same kinetic energy (KE).[3, 62, 63]\nThe KE = \\frac{1}{2}mv^2 Logic: This is the core principle. Since all ions have the same kinetic energy (KE), an ion’s velocity (v) must be inversely proportional to the square root of its mass (m).[62, 64] Therefore, lighter ions (low m/z) fly faster, and heavier ions (high m/z) fly slower.[3, 63, 65]\nOperation: The detector, at the end of the flight tube, measures the time of flight for each ion to travel the known distance.[60] This flight time is easily converted to an m/z value. Because it measures all ions from a single pulse “at once” (rather than scanning sequentially like a quadrupole), it is extremely fast and sensitive.[48]\nKey Feature: The Reflectron: A simple “linear” TOF has poor resolution because not all ions start at the exact same place or get the exact same KE.[65] All modern TOF instruments use a reflectron, or “ion mirror.” This is an electrostatic field at the end of the tube that repels the ions and sends them back (often at a slight angle) toward the detector.[65] Ions with slightly more KE penetrate deeper into this mirror, taking a longer path. Ions with less KE penetrate less, taking a shorter path. This “time-of-flight focusing” [60] ensures that all ions of the same m/z (despite small energy differences) hit the detector at the exact same time, dramatically increasing the instrument’s resolution.[60, 65]\n\n\n\n2.3.3 The High-Resolution Master: The Orbitrap\n\nPrinciple: The Orbitrap is the newest major mass analyzer.[66] It is an ion trap that uses only electrostatic fields (no magnets).[67] It consists of a central, spindle-shaped electrode fitted inside a split, barrel-shaped outer electrode.[55, 67]\nMechanism: Ions are injected tangentially into the trap. The electric field causes them to be “trapped” in a stable orbit around the central spindle.[68] As they orbit, they also oscillate back and forth along the axis of the spindle (axial oscillation).[55, 67]\nOperation: The frequency (\\omega) of this axial oscillation is independent of the ion’s energy and is related only to its mass-to-charge ratio (\\omega \\propto 1/\\sqrt{m/z}).[67] The oscillating packets of ions induce a tiny “image current” in the two halves of the outer electrode.[68, 69] This complex signal, called a “transient,” is a superposition of all the different frequencies from all the different ion packets trapped in the cell.\nData Processing: A mathematical operation, the Fourier Transform (FT), is used to deconvolute this complex time-domain transient signal into its component frequencies.[67, 69, 70] These frequencies are then converted into the final, high-resolution m/z spectrum.\nCharacteristics: The Orbitrap is the definition of HRAM (High-Resolution Accurate-Mass).[68, 71, 72] Resolution is routinely set between 60,000 and 500,000 FWHM (Full Width at Half Maximum) [68, 73], with mass accuracy better than 1 part-per-million (ppm).[67] This is so precise that it can often determine a molecule’s elemental formula (e.g., distinguish C_{10}H_{12}O from C_9H_8O_2) from the accurate mass alone.[74]\n\n\n\n2.3.4 Hybrid and Tandem Instruments (MS/MS)\nThe true power of modern mass spectrometry comes from combining these analyzers in sequence, a technique known as Tandem Mass Spectrometry (MS/MS or MS^n).[16, 55, 75, 76] This allows for complex experimental designs for structural elucidation and quantification.\n\nMechanism (Tandem-in-Space):\n\nMS1 (Analyzer 1): Selects an ion of interest (the “precursor ion”) and filters away all others.\nCollision Cell (q2): The selected precursor ion is passed into a cell (often a quadrupole or other ion guide) filled with a low pressure of an inert gas like argon or nitrogen. The ion collides with the gas, gains internal energy, and fragments. This is called Collision-Induced Dissociation (CID).[70, 75, 76]\nMS2 (Analyzer 2): The resulting “product ions” (the fragments) are passed to a second mass analyzer, which scans and detects them.\n\nCommon Hybrids:\n\nTriple Quadrupole (QqQ): Q1 (MS1, filter) \\rightarrow q2 (collision cell) \\rightarrow Q3 (MS2, filter). This is the gold standard instrument for targeted quantification.[71, 77, 78]\nQuadrupole-Time-of-Flight (Q-TOF): Q1 (MS1, filter) \\rightarrow q2 (collision cell) \\to TOF (MS2, high-resolution scanner). A workhorse for discovery proteomics and metabolomics, combining a filter with a fast, high-resolution analyzer.[60, 75, 79]\nQuadrupole-Orbitrap (e.g., Q-Exactive): Q1 (MS1, filter) \\rightarrow Collision Cell \\rightarrow Orbitrap (MS2, HRAM scanner). The premier HRAM discovery instrument, offering high-resolution analysis of product ions.[66, 70, 72]\n\nMechanism (Tandem-in-Time): Some traps, like an Orbitrap or Ion Trap, can perform MS/MS in time instead of in space. They trap all ions, electrostatically eject all but the precursor of interest, fragment that ion inside the trap using CID, and then analyze the resulting product ions in the same device.[76, 80]\n\n Table 2. Comparison of Primary Mass Analyzers\n\n\n\n\n\n\n\n\n\n\n\nAnalyzer\nPrinciple of Separation\nTypical Resolution\nMass Accuracy\nScan Speed\nKey Application\n\n\n\n\nQuadrupole\nIon trajectory stability in an oscillating RF/DC field (Mass Filter) [55, 56]\nLow (~2,000 FWHM)\nLow (0.1 Da)\nVery Fast (Scan or Filter)\nRoutine Quantification, Mass Filtering (MS1) [48, 55, 81]\n\n\nTime-of-Flight (TOF)\nTime to travel a fixed distance; all ions given same Kinetic Energy (KE = \\frac{1}{2}mv^2) [62, 63]\nHigh (~10,000 - 60,000 FWHM)\nGood (5-10 ppm)\nExtremely Fast (Pulsed)\nFast separations (GCxGC), MALDI, Discovery (Q-TOF) [48, 63, 77]\n\n\nOrbitrap\nFrequency of axial oscillation in an electrostatic field (Frequency \\propto 1/\\sqrt{m/z}) [67, 68]\nUltra-High (HRAM) (120,000 - 500,000+ FWHM)\nExcellent (HRAM) (&lt;1-3 ppm)\nModerate (FT required)\nHRAM Discovery, Metabolite ID, Proteomics [68, 71, 72]\n\n\n\n\n\n\n\n1.3.4 2.4 Part 4: The Detector – Counting the Ions\nThe detector is the final component of the mass spectrometer. It is positioned at the end of the mass analyzer and is responsible for converting the kinetic energy of each ion that strikes its surface into a measurable electrical signal.[3, 12, 82, 83] The two most common types are the Faraday Cup and the Electron Multiplier.\n\n2.4.1 The Faraday Cup (FC)\n\nPrinciple: The Faraday Cup (FC) is a simple, robust, and highly accurate detector. It is, at its core, a conductive metal cup designed to “catch” the charged particles.[84, 85, 86]\nMechanism: When a positive ion from the analyzer strikes the metal cup, it is neutralized by an electron from a connected circuit. This flow of electrons to the cup constitutes a tiny electrical current.[85, 86] This current is measured, and it is directly proportional to the number of ions hitting the cup (e.g., one nanoamp corresponds to ~6 billion singly-charged ions per second).[86]\nCharacteristics: The FC provides no signal gain (one ion strike results in one electron of current).[85] This makes it less sensitive than other detectors.[85, 87] However, it is extremely stable and “highly regarded for accuracy” because of the direct relationship between current and ion count.[86]\n\n\n\n2.4.2 The Electron Multiplier (EM)\n\nPrinciple: The Electron Multiplier (EM) is the most common detector in modern mass spectrometers, used for its exceptionally high internal gain and ability to detect single ion events.[82, 88]\nMechanism: The EM operates on the principle of secondary electron emission.[89, 90] It consists of a series of surfaces called “dynodes,” each held at a progressively higher voltage.\n\nA single ion from the analyzer strikes the first dynode surface.\nThis impact has enough energy to “splash” multiple electrons (secondary electrons) from the surface.[90, 91]\nThese new electrons are accelerated by the voltage gradient (~100-200V) and strike the second dynode.[88]\nEach of these electrons, in turn, splashes more electrons from the second dynode’s surface.\nThis “cascade” [82] continues down a series of discrete dynodes or along the wall of a continuous, horn-shaped dynode.[90, 91]\n\nCharacteristics: The result is a massive signal amplification. A single ion striking the front of the multiplier can result in a measurable pulse of 10^6 to 10^8 electrons exiting the back, allowing for single-ion counting.[82, 85, 88]\n\nThis leads to a classic detector trade-off: sensitivity versus stability.[87, 92] The EM is extremely sensitive and required for trace-level analysis. The FC is less sensitive but offers “excellent precision” and stability.[92] For this reason, EMs are standard for most quantitative and qualitative work, while FCs are preferred for high-precision isotope ratio mass spectrometry (IRMS), where the stability of the ratio measurement is more important than raw sensitivity.[2, 92]",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition</span>"
    ]
  },
  {
    "objectID": "intro.html#preparing-for-analysis-from-sample-to-instrument",
    "href": "intro.html#preparing-for-analysis-from-sample-to-instrument",
    "title": "1  Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition",
    "section": "1.4 3. Preparing for Analysis: From Sample to Instrument",
    "text": "1.4 3. Preparing for Analysis: From Sample to Instrument\nSample preparation is the essential precondition for high-quality MS data, directly influencing sensitivity, reproducibility, and instrument longevity.[8, 20, 16] Effective workflows combine purification, concentration, and—when appropriate—chemical derivatization to stabilize or volatilize analytes before they enter the mass spectrometer.\n\nPurification & concentration: Solid-phase extraction (SPE) and desalting protocols remove salts and detergents that otherwise suppress ionization efficiency.[8, 20]\nChemical derivatization: Strategic derivatization increases volatility for GC-MS analyses or enhances ionization efficiency for LC-MS approaches.[20]\nProteomics workflows: Bottom-up digestion with sequence-grade proteases remains the most reproducible route to comprehensive protein identification.[8, 12]\n\nBest practice: Employ MS-compatible solvents (acetonitrile, methanol) and volatile buffers (ammonium acetate or ammonium formate) to minimize ion suppression in both chromatographic and direct-infusion methods.[8, 20]\nThis is the most critical, and most often-failed, step for new users. Mass spectrometers are high-sensitivity, high-vacuum instruments, and “operating outside of this range normally results in severe degradation of performance”.[93] The “Garbage In, Garbage Out” (GIGO) principle is paramount. Impure samples, containing high concentrations of non-volatile salts, detergents, or contaminants, will not produce good mass spectra.[94] These non-volatile components will clog the instrument, contaminate the ion source, and, most damagingly, cause ion suppression—where the contaminant “steals” the ionizable charge from the analyte, making the analyte invisible to the detector.[95, 96]\nTherefore, sample preparation is a non-negotiable step focused on mitigating risk—the risk of instrument contamination, the risk of ion suppression, and the risk of acquiring ambiguous or false data.\n\n1.4.1 3.1 Sample Preparation: The Prerequisite for Quality Data\n\n3.1.1 General Considerations (Small Molecules)\n\nPurity & Concentration: Samples should be as pure as possible.[94] A typical starting concentration for a pure compound is ~1 mg/mL, which is then serially diluted into an appropriate solvent for analysis.[97]\nSolvents: Solvents must be compatible with the ionization source. ESI requires polar, volatile solvents (e.g., methanol, acetonitrile, water).[9, 97] If using organic solvents like chloroform or DCM, glass vials must be used, as these solvents will leach plasticizers from plastic tubes, leading to severe contamination.[95]\nThe Enemy: Non-Volatile Salts & Buffers: Non-volatile salts (e.g., NaCl, KCl, phosphate buffers) are the number one cause of ESI-MS failure. They do not evaporate in the ESI source but instead precipitate, rapidly clogging the capillary and source optics. They also form “adducts” (e.g., [M+Na]^{+}) that complicate the spectrum and, most importantly, they “steal” the charge from the analyte, causing severe ion suppression.[9, 95] If a buffer is absolutely required, it must be volatile (e.g., ammonium acetate, ammonium formate, or formic acid).[98]\n\n\n\n3.1.2 Clean-up and Concentration: Solid-Phase Extraction (SPE)\n\nPurpose: SPE is a powerful clean-up technique used to separate the analyte of interest from a complex matrix (like plasma or urine), remove interfering compounds (like salts), and concentrate the analyte.[96, 99]\nMechanism: SPE uses a small, disposable cartridge packed with a sorbent (e.g., C18-silica, reversed-phase). The process follows four steps, often abbreviated “CLWE” [100]:\n\nCondition: The sorbent is wetted (e.g., with methanol) and equilibrated (e.g., with water).\nLoad: The aqueous sample is passed through the sorbent. The analyte, being hydrophobic, sticks to the C18 sorbent, while the salts and polar interferences pass through to waste.\nWash: A weak solvent (e.g., 5% methanol/water) is passed through to wash away any remaining, weakly-bound interferences.\nElute: A strong organic solvent (e.g., 90% methanol) is used to elute the now-purified and concentrated analyte from the sorbent.\n\nKey Benefit for MS: The primary benefit of SPE is the reduction of ion suppression.[96] By removing the salts and other matrix components that compete for charge, the analyte’s signal is dramatically improved, leading to higher sensitivity and more accurate results.\n\n\n\n3.1.3 Chemical Modification: Derivatization for GC-MS\n\nPurpose: GC-MS requires all analytes to be volatile and thermally stable.[35, 101] Many biologically and pharmaceutically relevant molecules (e.g., sugars, amino acids, steroids) are not; they are highly polar and will decompose before they vaporize.[101]\nMechanism: Derivatization is a chemical reaction that solves this problem. It targets the polar functional groups (e.g., -OH, -NH, -SH) and replaces the active, polar hydrogens with non-polar, bulky groups. This masks the polarity, increases volatility, and makes the molecule “flyable” on a GC.[101, 102]\nCommon Methods: The three most widely used methods are [103]:\n\nSilylation: Replaces an active hydrogen with a silyl group, most commonly trimethylsilane (TMS).[101, 103]\nAcylation: Replaces an active hydrogen with an acyl group.[103, 104]\nAlkylation: Modifies compounds with acidic hydrogens, like carboxylic acids, to form esters.[101, 103]\n\n\n\n\n3.1.4 Protocols for Complex Biological Matrices (Proteomics)\nModern proteomics, the large-scale study of proteins [105, 106], does not typically analyze intact proteins. Instead, it uses a “bottom-up” approach.[105, 107, 108] The goal of this complex workflow is to turn a sample’s entire proteome (thousands of proteins) into a clean, separated mixture of peptides, which are more suitable for LC-MS analysis.[109]\nA standard “bottom-up” workflow involves the following steps [109, 110]:\n\nLysis: The cells or tissues are first physically or chemically disrupted (lysed) to release all the proteins, typically into a strong detergent solution (like SDS) or a chaotropic agent (like urea).[109, 110] Critically, protease inhibitors must be added to the lysis buffer to prevent the sample’s own endogenous enzymes from degrading the proteins upon cell rupture.[109]\nPrecipitation/Clean-up: Interfering compounds like detergents (which are incompatible with MS), lipids, and salts are removed. This is often done by precipitating the protein out of solution using cold acetone or a chloroform/methanol mixture.[111, 112] The “crashed out” protein pellet is retained, and the contaminants are washed away.\nReduction & Alkylation: The purified proteins are re-dissolved and unfolded (denatured). Their disulfide bonds (S-S) are chemically reduced to free thiols (-SH) using a reducing agent (e.g., DTT or TCEP). These new -SH groups are then alkylated (e.g., with iodoacetamide, IAM), which adds a “cap” to them. This irreversible step prevents the disulfide bonds from re-forming.[109, 110]\nDigestion: The protease Trypsin is added. Trypsin is the workhorse of proteomics because it is highly specific: it cleaves protein chains only on the C-terminal side of Lysine (K) and Arg_inine (R) residues. This digestion process breaks the thousands of proteins into millions of peptides, but in a highly predictable and reproducible way.\nPeptide Desalting: The final peptide mixture (“digest”) is still in a high-salt buffer. The last step is to “desalt” it using an SPE C18 “tip” (a small pipette tip packed with C18 resin). The peptides stick, the salt is washed away, and the clean peptides are eluted in a small volume of organic solvent, ready for LC-MS analysis.[109]\n\n\n\n\n1.4.2 3.2 Instrument Setup: Tuning and Calibration\nBefore acquiring data, the instrument must be prepared. This is a two-step process: tuning (optimizing the signal) and calibration (ensuring accuracy). A simple way to distinguish them is: Calibration is an X-axis (m/z) problem, while Tuning is a Y-axis (intensity) problem.[113]\n\n3.2.1 Tuning: Optimizing the Y-Axis (Sensitivity & Resolution)\n\nPurpose: To adjust the dozens of electronic voltages (on the ion lenses, reflectron, quadrupoles, etc.) to maximize the intensity (sensitivity) and quality (peak shape, resolution) of the ion signal.[57, 113, 114]\nProcess: A known tuning standard (a “calibrant”) is infused into the instrument. The operator, or more commonly an “autotune” routine, then systematically adjusts the voltages to find the “sweet spot” that produces the strongest and sharpest peak for that ion.[57, 114] This often involves balancing a direct trade-off between sensitivity and resolution; tuning for maximum resolution may slightly decrease sensitivity, and vice versa.[57]\n\n\n\n3.2.2 Calibration: Optimizing the X-Axis (Mass Accuracy)\n\nPurpose: To ensure that the m/z value reported on the x-axis is correct.[113, 115] This is “critical for the reliability, accuracy, and precision” of all measurements.[116, 117]\nProcess: An external calibrant solution—a mixture of compounds with well-known, highly accurate masses that span the instrument’s mass range—is infused.[115, 118] The software acquires a spectrum, measures the apparent m/z of these known compounds (e.g., their flight time in a TOF), and builds a calibration curve that maps the measured “apparent” values to the true, correct m/z values.[115, 119]\nFrequency: This must be done regularly (e.g., daily).[120] Instrument electronics drift with temperature and use, causing the mass assignment to “walk” over time.[120] For HRAM instruments (Orbitrap, Q-TOF), where high mass accuracy is the entire point, an internal calibrant or “lock mass” is often used. This involves a continuous, low-level spray of a known calibrant compound during the analytical run, allowing the software to correct for mass drift in real-time.[113, 121] Different calibrants are required for different sources and polarities (e.g., perfluorokerosene (PFK) for EI [122], or a specific ESI negative-ion solution [123]).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition</span>"
    ]
  },
  {
    "objectID": "intro.html#strategies-for-data-collection-designing-the-ms-experiment",
    "href": "intro.html#strategies-for-data-collection-designing-the-ms-experiment",
    "title": "1  Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition",
    "section": "1.5 4. Strategies for Data Collection: Designing the MS Experiment",
    "text": "1.5 4. Strategies for Data Collection: Designing the MS Experiment\nWith a prepared sample and a tuned, calibrated instrument, the operator must now make the most important decision: how to collect the data. This choice is dictated entirely by the analytical goal. The two governing philosophies are Untargeted (Discovery) analysis and Targeted (Quantitative) analysis.[124, 125, 126]\n\nUntargeted (Discovery): The goal is to “detect as many metabolites [or peptides] as possible”.[126] You are exploring the sample to see what is there, often without a priori knowledge. The preferred modes are Full Scan, Data-Dependent Acquisition (DDA), and Data-Independent Acquisition (DIA).\nTargeted (Quantification): The goal is the “multiplexed analysis of a set of defined metabolites”.[126] You are measuring the abundance of specific, known compounds. The preferred modes are Selected Ion Monitoring (SIM) and Selected/Multiple Reaction Monitoring (SRM/MRM).\n\n Table 3. Guide to Mass Spectrometry Data Acquisition Modes\n\n\n\n\n\n\n\n\n\n\n\nMode\nGoal\nInstrument(s) Used\nData Acquired\nKey Advantage\n\n\n\n\n\nFull Scan\nUntargeted\nAny MS (Q, TOF, Orbitrap)\nMS1 only\nComprehensive snapshot; simple; allows retrospective analysis [124, 127, 128]\n\n\n\nSelected Ion Monitoring (SIM)\nTargeted\nQuadrupole\nMS1 only (Specific m/z values)\nHigh Sensitivity (trace analysis); good for quantification [59, 128]\n\n\n\nProduct Ion Scan (PIS)\nTargeted\nTandem MS (QqQ, Q-TOF)\nMS/MS (of one precursor)\nQualitative; provides structural “fingerprint” of one known ion [78, 129]\n\n\n\nSelected Reaction Monitoring (SRM/MRM)\nTargeted\nTriple Quadrupole (QqQ)\nMS/MS (Specific “transition”)\nUnmatched Sensitivity & Selectivity; Gold standard for quantification [130, 131, 132]\n\n\n\nData-Dependent Acquisition (DDA)\nUntargeted\nTandem MS (Q-TOF, Orbitrap)\nMS1 + MS/MS (of “Top N”)\nGood for ID; generates clean MS/MS spectra from single precursors [133]\n\n\n\nData-Independent Acquisition (DIA)\nUntargeted\nTandem MS (Q-TOF, Orbitrap)\nMS1 + MS/MS (of everything)\nUnbiased; Highly Reproducible; good for quantification; de facto standard for discovery proteomics [133, 134, 135, 136]\n\n\n\n\n\n\n1.5.1 4.1 Untargeted Acquisition Modes (Discovery)\n\n4.1.1 Full Scan Mode\n\nWorkflow: This is the simplest acquisition mode. The mass analyzer is set to scan a wide m/z range (e.g., m/z 100-1000) and records only MS1 spectra (precursor ions).[124, 125, 127] No fragmentation is intentionally induced.[126]\nPros: It captures all precursor ions, providing a comprehensive “snapshot” of the sample.[127, 128] Because all data is recorded, it allows for “retrospective analysis”—a user can go back to the data file years later to look for a compound they didn’t know about at the time of acquisition.[127]\nCons: It provides no fragmentation data, which makes confident structural identification of unknowns extremely difficult.[126, 137] It also has lower sensitivity compared to targeted modes, as the detector’s time is divided across the entire m/z range.[128]\n\n\n\n4.1.2 Data-Dependent Acquisition (DDA)\n\nGoal: To automatically acquire fragmentation (MS/MS) spectra for the most interesting (i.e., most abundant) peaks in real-time, in order to identify them.[138, 139]\nWorkflow: DDA is a “smart” reflexive workflow [124, 133, 134, 140]:\n\nThe instrument performs a fast, high-resolution MS1 Full Scan.\nThe instrument’s computer in real-time identifies the “Top N” most abundant precursor ions in that scan (e.g., N=10).\nThe instrument sequentially performs a full MS/MS (Product Ion Scan) on each of those N ions: it isolates the precursor (MS1), fragments it (q2), and scans all of its product ions (MS2).\nThe instrument then repeats this entire cycle (e.g., 1 MS1 scan + 10 MS/MS scans), which typically takes 1-3 seconds.[135] To avoid analyzing the same peak over and over, it uses “dynamic exclusion,” ignoring an ion it has just fragmented for the next 30-60 seconds.\n\nPros: DDA generates clean, high-quality MS/MS spectra that originate from a single, isolated precursor ion. This makes the data simple to interpret and search against spectral libraries for identification.[126, 133]\nCons: DDA is biased and stochastic. It is biased toward the “Top N” most abundant ions [133, 136], meaning low-abundance ions are consistently ignored.[133, 136] It is stochastic (random) because the “Top N” list may be slightly different from run to run, leading to “under-sampling” and poor reproducibility, especially for quantitative comparisons.[133, 134]\n\n\n\n4.1.3 Data-Independent Acquisition (DIA)\n\nGoal: To solve the bias and reproducibility problems of DDA by acquiring fragmentation data for everything, regardless of abundance.[134, 141]\nWorkflow: DIA is an unbiased, systematic workflow [124, 134]:\n\nThe entire m/z range of interest (e.g., m/z 400-1000) is divided into wide, consecutive “isolation windows” (e.g., 20-25 Da wide).\nThe instrument systematically steps through these windows. In the first scan, it isolates all ions in the m/z 400-425 window.\nIt fragments all of those ions simultaneously in the collision cell.\nIt records a single, composite MS/MS spectrum containing the fragments from all precursors that were in the m/z 400-425 window.\nIn the next scan, it repeats this for m/z 425-450, and so on, until the full range is covered.\n\nPros: DIA is unbiased and highly reproducible. It fragments all ions, including low-abundance ones, in every single run.[124, 136] This provides “higher precision and better reproducibility than DDA” [133, 136, 142] and has been shown to “identify and quantify over 3 times as many proteins” as DDA.[143] It has become the new standard for large-scale discovery and quantitative proteomics.[133]\nCons: The data is extremely complex. The resulting MS/MS spectra are “chimeric” or “multiplexed” [135]—a jumble of fragments from dozens of different precursors that co-eluted. “Data analysis is challenging” [135] and requires sophisticated deconvolution software and pre-existing spectral libraries (often generated from DDA runs) to “mine” the data and assign fragments back to their correct precursors.[133, 141, 144]\n\n\n\n\n1.5.2 4.2 Targeted Acquisition Modes (Quantification)\n\n4.2.1 Selected Ion Monitoring (SIM)\n\nWorkflow: SIM is a targeted MS1 mode, typically performed on a quadrupole. Instead of scanning the full m/z range, the operator programs the analyzer to only monitor a few specific, pre-selected m/z values that correspond to the target analytes.[59, 128, 145]\n“Dwell Time”: The instrument “dwells” on m/z 254.1 for 100 milliseconds, collecting signal, then “hops” to m/z 301.2 for 100ms, then hops back. Because the detector “stares” at the ion of interest for a long time—rather than dividing its time across a 1000-Da scan—the signal-to-noise ratio is “dramatically” boosted.[59, 128]\nPros: High Sensitivity. SIM can be 10x to 100x more sensitive than a Full Scan, making it ideal for trace-level quantification.[59, 128, 146]\nCons: You must know the m/z of your target beforehand. You are “blind” to all other compounds in the sample.[128]\n\n\n\n4.2.2 Product Ion Scan (PIS)\n\nWorkflow: This is the basic qualitative MS/MS scan, used for structural elucidation.[147]\nOperation: [78, 129]\n\nMS1 (e.g., Q1) is FIXED to select one precursor ion of interest (e.g., m/z 300).\nThe m/z 300 ion is fragmented in the collision cell (q2).\nMS2 (e.g., Q3 or TOF) is SET TO SCAN the full m/z range to detect all resulting product ions (e.g., m/z 282, 254, 150…).\n\nGoal: To generate a full fragmentation “fingerprint” of one specific ion. This is used to confirm a compound’s identity by matching its fragment spectrum to a library, or to determine its chemical structure.[147, 148, 149]\n\n\n\n4.2.3 Selected Reaction Monitoring (SRM) / Multiple Reaction Monitoring (MRM)\n\nNomenclature: The terms are interchangeable.[150, 151] SRM refers to monitoring one single “transition” (precursor \\rightarrow product). MRM is simply the common practice of monitoring multiple SRM transitions in a single run.[78, 150]\nWorkflow: This is the gold standard for targeted quantification.[130, 152, 153] It requires a Triple Quadrupole (QqQ) instrument.\nOperation: [129, 131]\n\nMS1 (Q1) is FIXED to select a specific precursor m/z (e.g., m/z 300).\nThis ion is fragmented in the collision cell (q2).\nMS2 (Q3) is ALSO FIXED to select one specific, high-intensity product m/z that is characteristic of that precursor (e.g., m/z 282).\n\nPros: Unmatched Sensitivity and Selectivity. This mode is “non-scanning”.[131] The QqQ acts as a “double mass filter” [130], only allowing ions that pass the specific precursor \\rightarrow product “transition” to reach the detector.[78] This process virtually eliminates all chemical background and noise. The result is “exquisite sensitivity” [132], “up to 100-fold” better than a full Product Ion Scan [130], and a “wide dynamic range” (often 4-5 orders of magnitude).[131] It is the definitive method for quantitative proteomics and clinical assays (e.g., measuring drugs in blood plasma).[132, 151, 154]\nSRM Workflow Setup: An SRM assay must be rigorously developed.[151, 155, 156] The workflow is: (1) Select target peptides/compounds. (2) Select several unique precursor \\rightarrow product transitions for each target. (3) Optimize the collision energy for each transition to find the most intense signal. (4) Validate the method for absolute quantification using stable isotope-labeled internal standards.[154]",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition</span>"
    ]
  },
  {
    "objectID": "intro.html#initial-data-interpretation-understanding-the-output",
    "href": "intro.html#initial-data-interpretation-understanding-the-output",
    "title": "1  Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition",
    "section": "1.6 5. Initial Data Interpretation: Understanding the Output",
    "text": "1.6 5. Initial Data Interpretation: Understanding the Output\n\n1.6.1 5.1 The Chromatogram vs. The Spectrum\nData from a hyphenated technique (LC-MS or GC-MS) is two-dimensional, containing both m/z and retention time information.[19, 157] The instrument is “always on” [158], acquiring a full mass spectrum every second (or faster). The output is not a single spectrum, but thousands of spectra, one for each point in time.[157, 158] This dataset is visualized in two primary ways.\n\n5.1.1 Reading the Time Profile: Total Ion Chromatogram (TIC)\n\nWhat it is: The primary output. It is a 2D plot of Time (x-axis) vs. Total Ion Intensity (y-axis).[157, 159]\nHow it’s made: At each time point (e.g., 4.50 min), the instrument sums the intensities of all ions (across the entire m/z range) in the mass spectrum acquired at that instant. This single “total” value is plotted.[160, 161]\nWhat it shows: The TIC is a “map” of the analysis. Each peak in the TIC represents one or more compounds eluting from the chromatograph.[18, 159]\n\n\n\n5.1.2 The Mass Spectrum\n\nHow to get it: By clicking on a specific time point in the TIC (e.g., the apex of the peak at 4.50 min), the user can display the single mass spectrum that was acquired at that exact moment.[157]\nWhat it is: This is the plot of m/z (x-axis) vs. Relative Abundance (y-axis).[7, 8]\nY-Axis (Relative Abundance): This y-axis is not an absolute count. The instrument software finds the tallest peak in that single spectrum, sets its intensity to 100%, and then scales all other peaks in that same spectrum relative to it.[4, 7, 162, 163]\n\n\n\n\n1.6.2 5.2 Reading the Mass Spectrum\n\n5.2.1 Key Peaks: The Molecular Ion (M+) and the Base Peak\n\nMolecular Ion (M^{+\\bullet} or [M+H]^+): This is the peak that corresponds to the intact, unfragmented molecule.[162, 163, 164] It is (usually) the peak with the highest m/z value, excluding small isotope peaks. This peak is the most important, as it tells you the molecular weight of the compound.[164, 165]\nBase Peak: This is simply the tallest peak in the entire spectrum. It is assigned a relative abundance of 100%.[7, 166, 167, 168]\nDistinction: The Base Peak is not necessarily the Molecular Ion.[162, 168, 169] In hard ionization (EI), the molecular ion may be very unstable and fragment so completely that its peak (M+) is very small or absent. The Base Peak will instead be the most abundant fragment, which typically represents the most stable carbocation or fragment that can be formed.[166, 168, 169] The M+ peak tells you the size of the molecule; the Base Peak gives you a clue about its structure (e.g., a particularly stable sub-unit).\n\n\n\n5.2.2 Interpreting Patterns I: Isotopic Distributions\nMasses are not single lines; they are clusters of peaks that appear at M+1, M+2, etc..[164, 165] These “isotope peaks” exist because of the natural abundance of heavy stable isotopes (e.g., ^{13}C, ^{15}N, ^{18}O, ^{34}S, ^{37}Cl, ^{81}Br).[164, 170] These patterns are highly diagnostic.\n\nUsing the M+1 Peak: The natural abundance of ^{13}C is ~1.1%.[164, 170] Therefore, the relative intensity of the M+1 peak (compared to the M+ peak) is directly proportional to the number of carbon atoms in the molecule.[164] A molecule with an M+1 peak that is 6.6% as tall as the M+ peak suggests the presence of ~6 carbon atoms (6 \\times 1.1% = 6.6%).[164]\nUsing the M+2 Peak: This peak is a “dead giveaway” for two elements:\n\nChlorine: Chlorine exists as ^{35}Cl and ^{37}Cl in a natural ratio of ~3:1. A compound containing one chlorine atom will show a characteristic M+2 peak that is 1/3 the height of the M+ peak.[165, 170]\nBromine: Bromine exists as ^{79}Br and ^{81}Br in a natural ratio of ~1:1. A compound containing one bromine atom will show a characteristic M+2 peak that is equal in height to the M+ peak.[165, 170]\n\n\n\n\n5.2.3 Interpreting Patterns II: Fragmentation (EI)\n\nWhat they are: In an EI spectrum, all the peaks at m/z values lower than the molecular ion are “fragment ions”.[163] They are the “pieces” the molecule shattered into.[8, 163]\nHow to read them: The difference in mass between the molecular ion (M+) and a fragment peak represents the neutral piece that was lost during fragmentation.[34]\nCommon Losses: By looking for these “neutral losses,” a structure can be postulated.[30, 34, 164]\n\nLoss of 15 (a peak at m/z = M-15) indicates the loss of a methyl radical (\\bullet CH_3).[34]\nLoss of 29 (a peak at m/z = M-29) indicates the loss of an ethyl radical (\\bullet CH_2CH_3).[8]\nLoss of 18 (a peak at m/z = M-18) often indicates the loss of water (H_2O) from an alcohol.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition</span>"
    ]
  },
  {
    "objectID": "intro.html#a-horizon-of-applications",
    "href": "intro.html#a-horizon-of-applications",
    "title": "1  Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition",
    "section": "1.7 6. A Horizon of Applications",
    "text": "1.7 6. A Horizon of Applications\nBy mastering the principles of instrumentation, sample preparation, and data acquisition, the researcher gains access to one of the most versatile and powerful analytical tools in modern science.\n\nSystem-Wide Analysis: Proteomics and Metabolomics: MS is the engine of the “omics” revolution. In proteomics, it allows for the “system-wide characterization of the proteome” [105], enabling the identification and quantification of thousands of proteins [105], their isoforms [171], and their critical post-translational modifications (PTMs).[171, 172] In metabolomics, the “comprehensive study of small molecules” [173], the high sensitivity and resolution of MS characterizes the “wide range of metabolites” [173, 174] that define a biological state, identifying new biomarkers of disease and metabolic pathways.[71, 175] Recent advances in sensitivity have pushed these frontiers into single-cell proteomics and spatial profiling.[105, 176, 177]\nSpatially-Resolved Analysis: MALDI Mass Spectrometry Imaging (MSI): This “powerful label-free technique” [51, 52, 53] creates “molecular pictures” [53] by collecting thousands of individual MALDI spectra in a grid pattern across a thin tissue section.[50] This allows researchers to see the precise spatial distribution of drugs, lipids, or proteins directly in the tissue, without the need for antibodies or labels.[51, 54]\nClinical and Pharmaceutical Analysis: MS is a workhorse in drug development, used to assess the “performance of drugs in vivo” [54], analyze drug-excipient compositions [178], and perform high-sensitivity quantification of drugs in complex matrices like plasma.[71, 179] In clinical microbiology, MALDI-TOF has revolutionized the diagnostic lab. It is a “cost- and time-effective” [180] method for the rapid identification of bacterial and fungal species from a patient culture, delivering an answer in minutes instead of the days required for traditional biochemical tests.[180, 181]",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction - Principles of Mass Spectrometry and Strategies for Data Acquisition</span>"
    ]
  },
  {
    "objectID": "01-Introduction.html",
    "href": "01-Introduction.html",
    "title": "Introduction to Mass Spectrometry in R",
    "section": "",
    "text": "Loading Essential Packages\nThis introduction provides hands-on examples of working with mass spectrometry data in R using the R for Mass Spectrometry ecosystem. We’ll explore real datasets and demonstrate key functionalities.\nCode# Load core MS packages\nlibrary(Spectra)           # Core MS data infrastructure\nlibrary(msdata)            # Example MS datasets\nlibrary(ProtGenerics)      # Generic functions for proteomics\nlibrary(PSMatch)           # Peptide-spectrum matching\nlibrary(tidyverse)         # Data manipulation\nlibrary(MsDataHub)         # Access to online MS resources\n\ncat(\"Packages loaded successfully!\\n\")\n\nPackages loaded successfully!",
    "crumbs": [
      "Foundations",
      "Introduction to Mass Spectrometry in R"
    ]
  },
  {
    "objectID": "01-Introduction.html#exploring-example-datasets",
    "href": "01-Introduction.html#exploring-example-datasets",
    "title": "Introduction to Mass Spectrometry in R",
    "section": "Exploring Example Datasets",
    "text": "Exploring Example Datasets\nThe msdata package provides various example MS datasets for learning and testing.\nProteomics Data\n\nCode# List available proteomics files\nproteomics_files &lt;- msdata::proteomics(full.names = TRUE)\ncat(\"Available proteomics files:\\n\")\n\nAvailable proteomics files:\n\nCodefor (i in seq_along(proteomics_files)) {\n  cat(sprintf(\"%d. %s\\n\", i, basename(proteomics_files[i])))\n}\n\n1. MRM-standmix-5.mzML.gz\n2. MS3TMT10_01022016_32917-33481.mzML.gz\n3. MS3TMT11.mzML\n4. TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz\n5. TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01.mzML.gz\n\nCode# Load a specific proteomics dataset (TMT experiment)\nf &lt;- msdata::proteomics(pattern = \"2014\", full.names = TRUE)\ncat(\"\\nSelected file:\", basename(f), \"\\n\")\n\n\nSelected file: TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz \n\n\nLoading and Examining MS Data\n\nCode# Create Spectra object\nms &lt;- Spectra(f, backend = MsBackendMzR())\n\n# Display basic information\ncat(\"\\nDataset summary:\\n\")\n\n\nDataset summary:\n\nCodecat(\"  Total spectra:\", length(ms), \"\\n\")\n\n  Total spectra: 7534 \n\nCodecat(\"  MS levels:\", paste(unique(msLevel(ms)), collapse = \", \"), \"\\n\")\n\n  MS levels: 1, 2 \n\nCodecat(\"  RT range:\", round(range(rtime(ms)), 2), \"seconds\\n\")\n\n  RT range: 0.46 3601.98 seconds\n\nCodecat(\"  m/z range:\", round(range(unlist(mz(ms))), 2), \"\\n\")\n\n  m/z range: 100 2008.5 \n\nCode# Display the object\nms\n\nMSn data (Spectra) with 7534 spectra in a MsBackendMzR backend:\n       msLevel     rtime scanIndex\n     &lt;integer&gt; &lt;numeric&gt; &lt;integer&gt;\n1            1    0.4584         1\n2            1    0.9725         2\n3            1    1.8524         3\n4            1    2.7424         4\n5            1    3.6124         5\n...        ...       ...       ...\n7530         2   3600.47      7530\n7531         2   3600.83      7531\n7532         2   3601.18      7532\n7533         2   3601.57      7533\n7534         2   3601.98      7534\n ... 34 more variables/columns.\n\nfile(s):\nTMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz\n\n\nDifferent Types of MS Data\n\nCode# SWATH/DIA data example\nswath_file &lt;- PestMix1_SWATH.mzML()\nswath_data &lt;- Spectra(swath_file, backend = MsBackendMzR())\n\ncat(\"\\nSWATH/DIA Dataset:\\n\")\n\n\nSWATH/DIA Dataset:\n\nCodecat(\"  File:\", basename(swath_file), \"\\n\")\n\n  File: 95c051226acc_7862 \n\nCodecat(\"  Spectra count:\", length(swath_data), \"\\n\")\n\n  Spectra count: 8999 \n\nCodecat(\"  MS levels:\", paste(unique(msLevel(swath_data)), collapse = \", \"), \"\\n\")\n\n  MS levels: 2, 1 \n\n\n\nCode# Metabolomics data examples\nmetab_file1 &lt;- X20171016_POOL_POS_1_105.134.mzML()\nmetab_data1 &lt;- Spectra(metab_file1, backend = MsBackendMzR())\n\ncat(\"\\nMetabolomics Dataset 1:\\n\")\n\n\nMetabolomics Dataset 1:\n\nCodecat(\"  File:\", basename(metab_file1), \"\\n\")\n\n  File: 95c08883ab0_7859 \n\nCodecat(\"  Spectra count:\", length(metab_data1), \"\\n\")\n\n  Spectra count: 931 \n\nCodecat(\"  Polarity:\", unique(polarity(metab_data1)), \"\\n\")\n\n  Polarity: 1 \n\n\n\nCode# Another metabolomics example\nmetab_file2 &lt;- X20171016_POOL_POS_3_105.134.mzML()\nmetab_data2 &lt;- Spectra(metab_file2, backend = MsBackendMzR())\n\ncat(\"\\nMetabolomics Dataset 2:\\n\")\n\n\nMetabolomics Dataset 2:\n\nCodecat(\"  File:\", basename(metab_file2), \"\\n\")\n\n  File: 95c05604db2_7860 \n\nCodecat(\"  Spectra count:\", length(metab_data2), \"\\n\")\n\n  Spectra count: 931 \n\n\n\nCode# TMT labeled proteomics data\ntmt_file &lt;- TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01.20141210.mzML.gz()\ntmt_data &lt;- Spectra(tmt_file, backend = MsBackendMzR())\n\ncat(\"\\nTMT Proteomics Dataset:\\n\")\n\n\nTMT Proteomics Dataset:\n\nCodecat(\"  File:\", basename(tmt_file), \"\\n\")\n\n  File: 95c049704e13_7858 \n\nCodecat(\"  Spectra count:\", length(tmt_data), \"\\n\")\n\n  Spectra count: 7534 \n\nCodecat(\"  MS levels:\", paste(unique(msLevel(tmt_data)), collapse = \", \"), \"\\n\")\n\n  MS levels: 1, 2",
    "crumbs": [
      "Foundations",
      "Introduction to Mass Spectrometry in R"
    ]
  },
  {
    "objectID": "01-Introduction.html#peptide-fragment-calculation",
    "href": "01-Introduction.html#peptide-fragment-calculation",
    "title": "Introduction to Mass Spectrometry in R",
    "section": "Peptide Fragment Calculation",
    "text": "Peptide Fragment Calculation\nThe PSMatch package provides tools for theoretical peptide fragmentation.\n\nCode# Calculate theoretical fragments for a peptide\npeptide_seq &lt;- \"THSQEEMQHMQR\"\n\ncat(\"Calculating theoretical fragments for:\", peptide_seq, \"\\n\\n\")\n\nCalculating theoretical fragments for: THSQEEMQHMQR \n\nCodefragments &lt;- calculateFragments(peptide_seq)\n\n# Display fragment information\ncat(\"Theoretical fragments:\\n\")\n\nTheoretical fragments:\n\nCodeprint(head(fragments, 15))\n\n          mz ion type pos z         seq      peptide\n1   102.0550  b1    b   1 1           T THSQEEMQHMQR\n2   239.1139  b2    b   2 1          TH THSQEEMQHMQR\n3   326.1459  b3    b   3 1         THS THSQEEMQHMQR\n4   454.2045  b4    b   4 1        THSQ THSQEEMQHMQR\n5   583.2471  b5    b   5 1       THSQE THSQEEMQHMQR\n6   712.2897  b6    b   6 1      THSQEE THSQEEMQHMQR\n7   843.3301  b7    b   7 1     THSQEEM THSQEEMQHMQR\n8   971.3887  b8    b   8 1    THSQEEMQ THSQEEMQHMQR\n9  1108.4476  b9    b   9 1   THSQEEMQH THSQEEMQHMQR\n10 1239.4881 b10    b  10 1  THSQEEMQHM THSQEEMQHMQR\n11 1367.5467 b11    b  11 1 THSQEEMQHMQ THSQEEMQHMQR\n12  175.1190  y1    y   1 1           R THSQEEMQHMQR\n13  303.1775  y2    y   2 1          QR THSQEEMQHMQR\n14  434.2180  y3    y   3 1         MQR THSQEEMQHMQR\n15  571.2769  y4    y   4 1        HMQR THSQEEMQHMQR\n\nCodecat(\"\\nFragment types:\", paste(unique(fragments$type), collapse = \", \"), \"\\n\")\n\n\nFragment types: b, y, b_, y_, b*, y* \n\nCodecat(\"Total fragments:\", nrow(fragments), \"\\n\")\n\nTotal fragments: 58",
    "crumbs": [
      "Foundations",
      "Introduction to Mass Spectrometry in R"
    ]
  },
  {
    "objectID": "01-Introduction.html#peptide-spectrum-matching",
    "href": "01-Introduction.html#peptide-spectrum-matching",
    "title": "Introduction to Mass Spectrometry in R",
    "section": "Peptide-Spectrum Matching",
    "text": "Peptide-Spectrum Matching\nWorking with identification results from database searches.\n\nCode# Load identification files\nid_files &lt;- msdata::ident(full.names = TRUE)\ncat(\"Available identification files:\\n\")\n\nAvailable identification files:\n\nCodefor (i in seq_along(id_files)) {\n  cat(sprintf(\"%d. %s\\n\", i, basename(id_files[i])))\n}\n\n1. TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzid\n\nCode# Load PSM data\nid &lt;- PSM(id_files)\n\ncat(\"\\nPSM Summary:\\n\")\n\n\nPSM Summary:\n\nCodecat(\"  Total PSMs:\", nrow(id), \"\\n\")\n\n  Total PSMs: 5802 \n\nCodecat(\"  Unique peptides:\", length(unique(id$sequence)), \"\\n\")\n\n  Unique peptides: 4938 \n\nCodecat(\"  Unique proteins:\", length(unique(id$DatabaseAccess)), \"\\n\")\n\n  Unique proteins: 3148 \n\nCode# Display first few PSMs\ncat(\"\\nFirst few PSMs:\\n\")\n\n\nFirst few PSMs:\n\nCodeprint(head(id, 5))\n\nPSM with 5 rows and 35 columns.\nnames(35): sequence spectrumID ... subReplacementResidue subLocation",
    "crumbs": [
      "Foundations",
      "Introduction to Mass Spectrometry in R"
    ]
  },
  {
    "objectID": "01-Introduction.html#biological-annotation-and-enrichment",
    "href": "01-Introduction.html#biological-annotation-and-enrichment",
    "title": "Introduction to Mass Spectrometry in R",
    "section": "Biological Annotation and Enrichment",
    "text": "Biological Annotation and Enrichment\nConnecting MS data to biological databases for functional analysis.\n\nCode# Load Gene Ontology database\nlibrary(GO.db)\n\ncat(\"Gene Ontology database loaded\\n\")\n\nGene Ontology database loaded\n\nCodecat(\"Available columns:\", paste(columns(GO.db), collapse = \", \"), \"\\n\")\n\nAvailable columns: DEFINITION, GOID, ONTOLOGY, TERM \n\nCode# Query a specific GO term (focal adhesion)\ngo_term &lt;- \"GO:0005925\"\ngo_info &lt;- select(GO.db, \n                  keys = go_term,\n                  columns = columns(GO.db),\n                  keytype = \"GOID\")\n\ncat(\"\\nGO Term Information for\", go_term, \":\\n\")\n\n\nGO Term Information for GO:0005925 :\n\nCodeprint(go_info)\n\n        GOID\n1 GO:0005925\n                                                                                                                                                                                                                  DEFINITION\n1 A cell-substrate junction that anchors the cell to the extracellular matrix and that forms a point of termination of actin filaments. In insects focal adhesion has also been referred to as hemi-adherens junction (HAJ).\n  ONTOLOGY           TERM\n1       CC focal adhesion\n\n\n\nCode# Get human genes associated with focal adhesion\nlibrary(org.Hs.eg.db)\n\nGO_0005925 &lt;- AnnotationDbi::select(\n  org.Hs.eg.db,\n  keys = \"GO:0005925\",\n  columns = c(\"ENTREZID\", \"SYMBOL\"),\n  keytype = \"GO\"\n) %&gt;%\n  as_tibble() %&gt;%\n  filter(!duplicated(ENTREZID))\n\ncat(\"\\nGenes associated with Focal Adhesion (GO:0005925):\\n\")\n\n\nGenes associated with Focal Adhesion (GO:0005925):\n\nCodecat(\"  Total genes:\", nrow(GO_0005925), \"\\n\")\n\n  Total genes: 424 \n\nCodeprint(head(GO_0005925, 10))\n\n# A tibble: 10 × 5\n   GO         EVIDENCE ONTOLOGY ENTREZID SYMBOL\n   &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \n 1 GO:0005925 HDA      CC       60       ACTB  \n 2 GO:0005925 HDA      CC       70       ACTC1 \n 3 GO:0005925 ISS      CC       71       ACTG1 \n 4 GO:0005925 HDA      CC       81       ACTN4 \n 5 GO:0005925 HDA      CC       87       ACTN1 \n 6 GO:0005925 IMP      CC       88       ACTN2 \n 7 GO:0005925 IMP      CC       89       ACTN3 \n 8 GO:0005925 HDA      CC       102      ADAM10\n 9 GO:0005925 HDA      CC       118      ADD1  \n10 GO:0005925 HDA      CC       214      ALCAM \n\n\n\nCode# Get genes for another GO term (centrosome)\nGO_0005813 &lt;- AnnotationDbi::select(\n  org.Hs.eg.db,\n  keys = \"GO:0005813\",\n  columns = c(\"ENTREZID\", \"SYMBOL\"),\n  keytype = \"GO\"\n) %&gt;%\n  as_tibble() %&gt;%\n  filter(!duplicated(ENTREZID))\n\ncat(\"\\nGenes associated with Centrosome (GO:0005813):\\n\")\n\n\nGenes associated with Centrosome (GO:0005813):\n\nCodecat(\"  Total genes:\", nrow(GO_0005813), \"\\n\")\n\n  Total genes: 633 \n\nCodeprint(head(GO_0005813, 10))\n\n# A tibble: 10 × 5\n   GO         EVIDENCE ONTOLOGY ENTREZID SYMBOL\n   &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \n 1 GO:0005813 IDA      CC       35       ACADS \n 2 GO:0005813 IDA      CC       324      APC   \n 3 GO:0005813 IDA      CC       328      APEX1 \n 4 GO:0005813 IDA      CC       402      ARL2  \n 5 GO:0005813 IDA      CC       403      ARL3  \n 6 GO:0005813 IDA      CC       468      ATF4  \n 7 GO:0005813 ISS      CC       472      ATM   \n 8 GO:0005813 IBA      CC       582      BBS1  \n 9 GO:0005813 IDA      CC       585      BBS4  \n10 GO:0005813 IDA      CC       598      BCL2L1",
    "crumbs": [
      "Foundations",
      "Introduction to Mass Spectrometry in R"
    ]
  },
  {
    "objectID": "01-Introduction.html#summary",
    "href": "01-Introduction.html#summary",
    "title": "Introduction to Mass Spectrometry in R",
    "section": "Summary",
    "text": "Summary\nThis introduction demonstrated:\n\nLoading and examining various MS datasets (proteomics, metabolomics, DIA/SWATH)\nUsing the Spectra infrastructure with different backends\nCalculating theoretical peptide fragments with PSMatch\n\nWorking with peptide-spectrum match (PSM) data\nConnecting MS results to biological annotations (GO terms)\n\nThese examples provide a foundation for the detailed analyses covered in subsequent chapters.",
    "crumbs": [
      "Foundations",
      "Introduction to Mass Spectrometry in R"
    ]
  },
  {
    "objectID": "12-open-data.html",
    "href": "12-open-data.html",
    "title": "\n2  Accessing Open MS Data with MsDataHub\n",
    "section": "",
    "text": "2.1 What is MsDataHub?\nReproducibility and access to public datasets are cornerstones of modern computational biology. The Bioconductor project facilitates this through the ExperimentHub infrastructure, which provides a centralized way to access curated data from various experiments. For the mass spectrometry community, the MsDataHub package serves as a dedicated portal to a wide range of proteomics and metabolomics datasets.\nThis chapter introduces MsDataHub and demonstrates how to use it to find, download, and load example MS data for analysis within the R for Mass Spectrometry ecosystem.\nThe MsDataHub package provides a collection of mass spectrometry datasets, including: - Raw MS data (mzML, CDF) - Peptide-spectrum matching (PSM) results (mzid) - Quantitative proteomics and metabolomics data tables - Contaminant FASTA databases (cRAP)\nData is downloaded and cached locally on your machine, ensuring that you only need to download each file once.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Accessing Open MS Data with MsDataHub</span>"
    ]
  },
  {
    "objectID": "12-open-data.html#installation",
    "href": "12-open-data.html#installation",
    "title": "\n2  Accessing Open MS Data with MsDataHub\n",
    "section": "\n2.2 Installation",
    "text": "2.2 Installation\nMsDataHub is a Bioconductor package. To install it, use BiocManager:\n\nCodeif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\nBiocManager::install(\"MsDataHub\")",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Accessing Open MS Data with MsDataHub</span>"
    ]
  },
  {
    "objectID": "12-open-data.html#exploring-available-datasets",
    "href": "12-open-data.html#exploring-available-datasets",
    "title": "\n2  Accessing Open MS Data with MsDataHub\n",
    "section": "\n2.3 Exploring Available Datasets",
    "text": "2.3 Exploring Available Datasets\nTo see a complete list of available datasets, you can call the MsDataHub() function. This returns a data frame with metadata for each resource.\n\nCodelibrary(MsDataHub)\nhub &lt;- MsDataHub()\n\n\nWe can then display this as an interactive table using DT::datatable.\nThe table includes details such as the resource title, data type, species, and the function required to access the data.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Accessing Open MS Data with MsDataHub</span>"
    ]
  },
  {
    "objectID": "12-open-data.html#accessing-data-examples",
    "href": "12-open-data.html#accessing-data-examples",
    "title": "\n2  Accessing Open MS Data with MsDataHub\n",
    "section": "\n2.4 Accessing Data Examples",
    "text": "2.4 Accessing Data Examples\nMsDataHub creates accessor functions for each dataset. For example, a file named PestMix1_DDA.mzML can be accessed by calling a function of the same name, PestMix1_DDA.mzML(). Let’s explore a few examples.\n\n2.4.1 Example 1: Raw MS Data\nHere, we load a raw DDA (Data-Dependent Acquisition) file from a TripleTOF instrument. The accessor function returns a file path, which we can then load into a Spectra object.\n\nCodelibrary(Spectra)\n\n# Get the file path from MsDataHub\n# The first time, this will download and cache the file\ndda_file &lt;- PestMix1_DDA.mzML()\n\n# Load the data into a Spectra object\nsps_dda &lt;- Spectra(dda_file)\nsps_dda\n\nMSn data (Spectra) with 7602 spectra in a MsBackendMzR backend:\n       msLevel     rtime scanIndex\n     &lt;integer&gt; &lt;numeric&gt; &lt;integer&gt;\n1            1     0.231         1\n2            1     0.351         2\n3            1     0.471         3\n4            1     0.591         4\n5            1     0.711         5\n...        ...       ...       ...\n7598         1   899.491      7598\n7599         1   899.613      7599\n7600         1   899.747      7600\n7601         1   899.872      7601\n7602         1   899.993      7602\n ... 34 more variables/columns.\n\nfile(s):\n96281f6f2b3a_7861\n\n\n\n2.4.2 Example 2: Peptide-Spectrum Matches (PSM)\nThis example downloads peptide-spectrum matching results from the PRIDE repository (accession PXD000001). The .mzid file can be loaded using the PSMatch package.\n\nCodelibrary(PSMatch)\n\n# Get the path to the mzid file\nmzid_file &lt;- TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01.20141210.mzid()\n\n# Load the PSMs\npsms &lt;- PSM(mzid_file)\npsms\n\nPSM with 5802 rows and 35 columns.\nnames(35): sequence spectrumID ... subReplacementResidue subLocation\n\n\n\n2.4.3 Example 3: Quantitative Proteomics Data\nMsDataHub also provides processed quantitative data. Here, we access a CPTAC (Clinical Proteomic Tumor Analysis Consortium) dataset. The accessor returns the path to a tab-delimited text file, which can be read into a QFeatures object.\n\nCodelibrary(QFeatures)\n\n# Get the path to the quantitative data\ncptac_file &lt;- cptac_peptides.txt()\n\n# Define which columns contain the intensity data\necols &lt;- grep(\"Intensity\\\\.\", names(read.delim(cptac_file)))\n\n# Read the data into a SummarizedExperiment\nse &lt;- readSummarizedExperiment(cptac_file, quantCols = ecols, sep = \"\\t\")\nse\n\nclass: SummarizedExperiment \ndim: 11466 45 \nmetadata(0):\nassays(1): ''\nrownames(11466): 1 2 ... 11465 11466\nrowData names(143): Sequence N.term.cleavage.window ...\n  Oxidation..M..site.IDs MS.MS.Count\ncolnames(45): Intensity.6A_1 Intensity.6A_2 ... Intensity.6E_8\n  Intensity.6E_9\ncolData names(0):",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Accessing Open MS Data with MsDataHub</span>"
    ]
  },
  {
    "objectID": "12-open-data.html#contributing-to-msdatahub",
    "href": "12-open-data.html#contributing-to-msdatahub",
    "title": "\n2  Accessing Open MS Data with MsDataHub\n",
    "section": "\n2.5 Contributing to MsDataHub",
    "text": "2.5 Contributing to MsDataHub\nMsDataHub is an open-source project, and contributions are welcome. If you have a dataset that you believe would be a valuable addition, you can open an issue on the MsDataHub GitHub repository to start the process.\nBy providing a simple and unified interface to a diverse range of MS data, MsDataHub significantly lowers the barrier to entry for researchers looking to learn new analysis techniques or benchmark their methods on established datasets.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Accessing Open MS Data with MsDataHub</span>"
    ]
  },
  {
    "objectID": "02-r-fundamentals.html",
    "href": "02-r-fundamentals.html",
    "title": "\n3  R Fundamentals for Mass Spectrometry\n",
    "section": "",
    "text": "3.1 R Environment Setup\nThis chapter introduces R programming concepts specifically relevant to mass spectrometry data analysis.",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Fundamentals for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "02-r-fundamentals.html#r-environment-setup",
    "href": "02-r-fundamentals.html#r-environment-setup",
    "title": "\n3  R Fundamentals for Mass Spectrometry\n",
    "section": "",
    "text": "3.1.1 Installing Required Packages\nThe R for Mass Spectrometry ecosystem is built on Bioconductor, a collection of R packages for biological data analysis.\n\nCode# Step 1: Install BiocManager if not already installed\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\n\n# Step 2: Install core R for Mass Spectrometry packages\nBiocManager::install(c(\n  # Core MS data infrastructure\n  \"Spectra\",              # Modern MS data handling\n  \"MsBackendMzR\",        # Reading raw data files\n  \"MsBackendSql\",        # SQL-based backend\n  \"MsBackendHdf5Peaks\",  # HDF5 storage backend\n  \n  # MS data utilities\n  \"msdata\",              # Example MS datasets\n  \"MsDataHub\",           # Access to online MS resources\n  \"ProtGenerics\",        # Generic functions for proteomics\n  \"MsCoreUtils\",         # Core MS utilities\n  \n  # Proteomics packages\n  \"PSMatch\",             # Peptide-spectrum matching\n  \"QFeatures\",           # Quantitative features\n  \n  # Metabolomics packages\n  \"xcms\",                # LC/MS data processing\n  \"CAMERA\",              # Annotation of adducts and isotopes\n  \"MetaboCoreUtils\",     # Metabolomics utilities\n  \"CompoundDb\",          # Compound databases\n  \n  # Statistical analysis\n  \"limma\",               # Linear models for microarray/proteomics\n  \"DEqMS\"                # Differential expression\n))\n\n# Step 3: Install CRAN packages for data manipulation and visualization\ninstall.packages(c(\n  \"tidyverse\",           # Data science ecosystem\n  \"ggplot2\",             # Advanced plotting\n  \"plotly\",              # Interactive plots\n  \"pheatmap\",            # Heatmaps\n  \"corrplot\",            # Correlation plots\n  \"BiocParallel\"         # Parallel processing\n))\n\n\n\n3.1.2 Verifying Installation\n\nCode# Check if key packages are installed correctly\nrequired_packages &lt;- c(\"Spectra\", \"QFeatures\", \"xcms\", \"tidyverse\")\n\nfor (pkg in required_packages) {\n  if (requireNamespace(pkg, quietly = TRUE)) {\n    cat(\"✓\", pkg, \"is installed\\n\")\n  } else {\n    cat(\"✗\", pkg, \"is NOT installed\\n\")\n  }\n}\n\n\n\n3.1.3 Loading Essential Libraries\n\nCode# Core MS packages\nlibrary(Spectra)           # MS data structures\nlibrary(msdata)            # Example datasets\nlibrary(ProtGenerics)      # Generic functions\n\n# Data manipulation\nlibrary(tidyverse)         # Includes dplyr, ggplot2, tidyr, etc.\n\n# Check package versions\ncat(\"Spectra version:\", as.character(packageVersion(\"Spectra\")), \"\\n\")\n\nSpectra version: 1.18.2 \n\nCodecat(\"tidyverse version:\", as.character(packageVersion(\"tidyverse\")), \"\\n\")\n\ntidyverse version: 2.0.0",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Fundamentals for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "02-r-fundamentals.html#understanding-r-for-mass-spectrometry-ecosystem",
    "href": "02-r-fundamentals.html#understanding-r-for-mass-spectrometry-ecosystem",
    "title": "\n3  R Fundamentals for Mass Spectrometry\n",
    "section": "\n3.2 Understanding R for Mass Spectrometry Ecosystem",
    "text": "3.2 Understanding R for Mass Spectrometry Ecosystem\n\n3.2.1 Package Architecture\nThe R for Mass Spectrometry initiative provides a modular ecosystem:\n\n\nCore Infrastructure: Spectra, QFeatures for data structures\n\nData Access: MsBackendMzR, MsBackendSql for reading files\n\nProteomics: PSMatch, ProtGenerics for peptide/protein analysis\n\nMetabolomics: xcms, CAMERA for small molecule analysis\n\nUtilities: MsCoreUtils, MetaboCoreUtils for common operations\n\n\nCode# Display available backends for Spectra\navailable_backends &lt;- c(\n  \"MsBackendMzR\" = \"Read mzML/mzXML files\",\n  \"MsBackendDataFrame\" = \"In-memory storage\",\n  \"MsBackendHdf5Peaks\" = \"HDF5-based storage\",\n  \"MsBackendSql\" = \"SQL database storage\"\n)\n\ncat(\"Available Spectra Backends:\\n\")\n\nAvailable Spectra Backends:\n\nCodefor (backend in names(available_backends)) {\n  cat(\"  •\", backend, \"-\", available_backends[backend], \"\\n\")\n}\n\n  • MsBackendMzR - Read mzML/mzXML files \n  • MsBackendDataFrame - In-memory storage \n  • MsBackendHdf5Peaks - HDF5-based storage \n  • MsBackendSql - SQL database storage",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Fundamentals for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "02-r-fundamentals.html#data-structures-in-r-for-ms",
    "href": "02-r-fundamentals.html#data-structures-in-r-for-ms",
    "title": "\n3  R Fundamentals for Mass Spectrometry\n",
    "section": "\n3.3 Data Structures in R for MS",
    "text": "3.3 Data Structures in R for MS\n\n3.3.1 Vectors and Matrices\nMass spectra are fundamentally collections of m/z and intensity pairs, which map naturally to R’s vector and matrix structures.\n\nCode# Example: Creating a simple spectrum\nmz_values &lt;- c(100.1, 200.2, 300.3, 400.4)\nintensity_values &lt;- c(1000, 2500, 800, 1200)\n\n# Create a simple spectrum data frame\nspectrum_df &lt;- data.frame(\n  mz = mz_values,\n  intensity = intensity_values\n)\n\nprint(spectrum_df)\n\n     mz intensity\n1 100.1      1000\n2 200.2      2500\n3 300.3       800\n4 400.4      1200\n\n\n\n3.3.2 Lists for Complex Data\nMS experiments often contain metadata alongside spectral data.\n\nCode# Example: MS run metadata\nms_run &lt;- list(\n  instrument = \"Orbitrap Fusion\",\n  ionization = \"ESI\",\n  polarity = \"positive\",\n  acquisition_date = Sys.Date(),\n  spectra_count = 1000\n)\n\nprint(ms_run)\n\n$instrument\n[1] \"Orbitrap Fusion\"\n\n$ionization\n[1] \"ESI\"\n\n$polarity\n[1] \"positive\"\n\n$acquisition_date\n[1] \"2025-11-12\"\n\n$spectra_count\n[1] 1000",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Fundamentals for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "02-r-fundamentals.html#data-importexport-basics",
    "href": "02-r-fundamentals.html#data-importexport-basics",
    "title": "\n3  R Fundamentals for Mass Spectrometry\n",
    "section": "\n3.4 Data Import/Export Basics",
    "text": "3.4 Data Import/Export Basics\n\n3.4.1 Common File Formats in Mass Spectrometry\n\n\n\n\n\n\n\n\nFormat\nType\nDescription\nUse Case\n\n\n\nmzML\nXML\nVendor-neutral standard format\nRaw MS data storage\n\n\nmzXML\nXML\nOlder standard format\nLegacy data\n\n\nMGF\nText\nMascot Generic Format\nMS/MS for database search\n\n\nCDF\nBinary\nNetCDF format\nGC-MS data\n\n\nmzTab\nText\nTab-delimited results\nAnalysis results\n\n\n\n\nCode# Example: Reading mzML files using Spectra\nlibrary(msdata)\n\n# Get path to example file\nms_file &lt;- msdata::proteomics(full.names = TRUE)[1]\ncat(\"Example file:\", basename(ms_file), \"\\n\")\n\nExample file: MRM-standmix-5.mzML.gz \n\nCode# Load the data with error handling for mzR compatibility issues\ntryCatch({\n  ms_data &lt;- Spectra(ms_file, backend = MsBackendMzR())\n  # Convert to in-memory for better compatibility\n  ms_data &lt;- setBackend(ms_data, backend = MsBackendDataFrame())\n  cat(\"\\nSuccessfully loaded real MS data\\n\")\n}, error = function(e) {\n  cat(\"\\nNote: mzR compatibility issue detected, using synthetic data\\n\")\n  cat(\"Error:\", conditionMessage(e), \"\\n\\n\")\n  \n  # Create synthetic data as fallback\n  set.seed(42)\n  n_spectra &lt;- 50\n  \n  library(S4Vectors)\n  library(IRanges)\n  \n  mz_list &lt;- lapply(1:n_spectra, function(i) {\n    sort(runif(sample(50:150, 1), 100, 2000))\n  })\n  \n  intensity_list &lt;- lapply(mz_list, function(mz_vals) {\n    rlnorm(length(mz_vals), meanlog = 8, sdlog = 2)\n  })\n  \n  # Generate MS levels first\n  ms_levels &lt;- sample(1:2, n_spectra, replace = TRUE, prob = c(0.3, 0.7))\n  \n  spd &lt;- DataFrame(\n    msLevel = ms_levels,\n    rtime = seq(100, 3000, length.out = n_spectra),\n    precursorMz = ifelse(ms_levels == 1, NA_real_, runif(n_spectra, 400, 1500)),\n    polarity = rep(1L, n_spectra)\n  )\n  \n  spd$mz &lt;- NumericList(mz_list)\n  spd$intensity &lt;- NumericList(intensity_list)\n  \n  backend &lt;- MsBackendDataFrame()\n  backend &lt;- backendInitialize(backend, spd)\n  ms_data &lt;&lt;- Spectra(backend)\n})\n\n\nNote: mzR compatibility issue detected, using synthetic data\nError: BiocParallel errors\n  1 remote errors, element index: 1\n  0 unevaluated and other errors\n  first remote error:\nError in DataFrame(..., check.names = FALSE): different row counts implied by arguments\n \n\nCode# Display basic information\ncat(\"\\nDataset summary:\\n\")\n\n\nDataset summary:\n\nCodecat(\"  Total spectra:\", length(ms_data), \"\\n\")\n\n  Total spectra: 50 \n\nCodecat(\"  MS levels:\", paste(unique(msLevel(ms_data)), collapse = \", \"), \"\\n\")\n\n  MS levels: 2, 1 \n\nCodecat(\"  RT range:\", round(range(rtime(ms_data)), 2), \"seconds\\n\")\n\n  RT range: 100 3000 seconds\n\n\n\n3.4.2 Working with Spectral Data\n\nCode# Access spectral data\nfirst_spectrum &lt;- ms_data[1]\n\n# Extract m/z and intensity values\npeaks &lt;- peaksData(first_spectrum)[[1]]\ncat(\"First spectrum has\", nrow(peaks), \"peaks\\n\")\n\nFirst spectrum has 98 peaks\n\nCodecat(\"m/z range:\", round(range(peaks[, 1]), 2), \"\\n\")\n\nm/z range: 100.45 1978.89 \n\nCodecat(\"Intensity range:\", round(range(peaks[, 2]), 2), \"\\n\")\n\nIntensity range: 7.07 436079.1 \n\n\n\n3.4.3 Exporting Data\n\nCode# Export spectra to different formats\n\n# Export to mzML\nexport(ms_data[1:10], file = \"subset.mzML\")\n\n# Export to MGF (for MS2 spectra)\nms2_data &lt;- filterMsLevel(ms_data, 2)\nexport(ms2_data, file = \"ms2_spectra.mgf\")\n\n# Export metadata to CSV\nmetadata &lt;- spectraData(ms_data) %&gt;%\n  as.data.frame() %&gt;%\n  select(msLevel, rtime, precursorMz, precursorCharge)\n\nwrite.csv(metadata, \"spectra_metadata.csv\", row.names = FALSE)",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Fundamentals for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "02-r-fundamentals.html#exercises",
    "href": "02-r-fundamentals.html#exercises",
    "title": "\n3  R Fundamentals for Mass Spectrometry\n",
    "section": "\n3.5 Exercises",
    "text": "3.5 Exercises\n\n\nPackage Installation: Install the core R for Mass Spectrometry packages and verify the installation\n\nData Structures: Create vectors representing m/z and intensity values for a hypothetical spectrum with at least 10 peaks\n\nData Frames: Build a data frame combining multiple spectra with metadata (RT, precursor m/z, charge)\n\nFile I/O: Practice loading MS data from the msdata package and explore different file formats\n\nBackend Comparison: Load the same file using different backends and compare memory usage",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Fundamentals for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "02-r-fundamentals.html#summary",
    "href": "02-r-fundamentals.html#summary",
    "title": "\n3  R Fundamentals for Mass Spectrometry\n",
    "section": "\n3.6 Summary",
    "text": "3.6 Summary\nThis chapter covered the fundamental R concepts needed for MS data analysis:\n\n\nPackage ecosystem: Core Bioconductor packages for MS analysis (Spectra, QFeatures, xcms)\n\nData structures: Vectors, matrices, data frames, and lists for MS data\n\nR for MS architecture: Understanding backends and modular design\n\nFile formats: Common MS formats (mzML, MGF) and how to read/write them\n\nBasic operations: Loading MS data and accessing spectral information\n\nWith these fundamentals in place, you’re ready to proceed to more advanced MS data processing and analysis workflows in the following chapters.",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Fundamentals for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "03-data-formats-import.html",
    "href": "03-data-formats-import.html",
    "title": "\n4  Mass Spectrometry Data Formats and Import\n",
    "section": "",
    "text": "4.1 Overview of MS Data Formats\nUnderstanding and working with various MS data formats is crucial for effective data analysis. This chapter covers the main data structures and file formats used in R for Mass Spectrometry.",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "03-data-formats-import.html#overview-of-ms-data-formats",
    "href": "03-data-formats-import.html#overview-of-ms-data-formats",
    "title": "\n4  Mass Spectrometry Data Formats and Import\n",
    "section": "",
    "text": "4.1.1 mzML Format\nThe mzML format is the current standard for MS data exchange, defined by the Proteomics Standards Initiative (PSI).\n\nCodelibrary(tidyverse)\nlibrary(Spectra)\nlibrary(msdata)\nlibrary(mzR)\nlibrary(ProtGenerics)\n\n# Load example mzML file\nmzml_file &lt;- proteomics(full.names = TRUE, pattern = \"TMT\")\nbasename(mzml_file)\n\n[1] \"MS3TMT10_01022016_32917-33481.mzML.gz\"                                 \n[2] \"MS3TMT11.mzML\"                                                         \n[3] \"TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz\"\n[4] \"TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01.mzML.gz\"         \n\n\n\nCode# Read mzML data using Spectra\nms_data &lt;- Spectra(mzml_file)\nms_data\n\nMSn data (Spectra) with 9602 spectra in a MsBackendMzR backend:\n       msLevel     rtime scanIndex\n     &lt;integer&gt; &lt;numeric&gt; &lt;integer&gt;\n1            1   4422.62         1\n2            2   4422.65         2\n3            2   4422.67         3\n4            2   4422.74         4\n5            2   4422.80         5\n...        ...       ...       ...\n9598         2   1321.60       505\n9599         2   1321.89       506\n9600         2   1322.18       507\n9601         2   1322.47       508\n9602         1   1322.95       509\n ... 34 more variables/columns.\n\nfile(s):\nMS3TMT10_01022016_32917-33481.mzML.gz\nMS3TMT11.mzML\nTMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz\n ... 1 more files\n\n\n\n4.1.2 Understanding Spectra Objects\nThe Spectra object is the core data structure in R for Mass Spectrometry, providing an abstraction for MS data that allows efficient manipulation and analysis.\n\nCode# Basic information about the data\ncat(\"Number of spectra:\", length(ms_data), \"\\n\")\n\nNumber of spectra: 9602 \n\nCodecat(\"MS levels present:\", paste(unique(msLevel(ms_data)), collapse = \", \"), \"\\n\")\n\nMS levels present: 1, 2, 3 \n\nCodecat(\"Retention time range:\", range(rtime(ms_data)), \"seconds\\n\")\n\nRetention time range: 0.4584 4494.339 seconds\n\n\n\nCode# MS level distribution\ntable(msLevel(ms_data))\n\n\n   1    2    3 \n1544 7306  752 \n\n\n\n4.1.3 Core Spectra Variables\nThe Spectra class defines core variables that characterize each spectrum:\n\nCode# Core spectra variables\nspectraVariables(ms_data) %&gt;% head(15)\n\n [1] \"msLevel\"                \"rtime\"                  \"acquisitionNum\"        \n [4] \"scanIndex\"              \"dataStorage\"            \"dataOrigin\"            \n [7] \"centroided\"             \"smoothed\"               \"polarity\"              \n[10] \"precScanNum\"            \"precursorMz\"            \"precursorIntensity\"    \n[13] \"precursorCharge\"        \"collisionEnergy\"        \"isolationWindowLowerMz\"\n\n\n\nCode# Access specific variables\n# Retention times for first 10 spectra\nrtime(ms_data)[1:10]\n\n [1] 4422.620 4422.648 4422.670 4422.735 4422.800 4423.036 4423.053 4423.094\n [9] 4423.367 4423.384\n\n\n\nCode# Precursor information for MS2 spectra\nms2_data &lt;- filterMsLevel(ms_data, msLevel = 2L)\nif (length(ms2_data) &gt; 0) {\n  precursor_info &lt;- data.frame(\n    scan = acquisitionNum(ms2_data)[1:10],\n    precursor_mz = precursorMz(ms2_data)[1:10],\n    precursor_charge = precursorCharge(ms2_data)[1:10],\n    retention_time = rtime(ms2_data)[1:10]\n  )\n  print(precursor_info)\n}\n\n    scan precursor_mz precursor_charge retention_time\n1  32919     652.3426                3       4422.648\n2  32920     647.3441                3       4422.670\n3  32921     673.3353                3       4422.735\n4  32922     575.0026                3       4422.800\n5  32924     820.9681                2       4423.053\n6  32925     675.6938                3       4423.094\n7  32927     608.2888                2       4423.384\n8  32928     685.7155                3       4423.414\n9  32930     659.0151                3       4423.699\n10 32931     438.7607                2       4423.748\n\n\n\n4.1.4 mzXML Format\nOlder but still commonly used XML format.\n\nCode# Example with mzXML (if available)\n# Similar syntax to mzML",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "03-data-formats-import.html#working-with-spectra-objects",
    "href": "03-data-formats-import.html#working-with-spectra-objects",
    "title": "\n4  Mass Spectrometry Data Formats and Import\n",
    "section": "\n4.2 Working with Spectra Objects",
    "text": "4.2 Working with Spectra Objects\n\n4.2.1 Accessing Spectral Data\n\nCode# Get the first spectrum\nfirst_spectrum &lt;- ms_data[1]\nmz(first_spectrum)[[1]] %&gt;% head(10)\n\n [1] 376.2217 376.2227 376.2237 376.2247 380.7361 380.7371 380.7381 380.7391\n [9] 380.7401 380.7411\n\nCodeintensity(first_spectrum)[[1]] %&gt;% head(10)\n\n [1]     0.00     0.00     0.00     0.00     0.00     0.00     0.00     0.00\n [9] 21175.68 57675.16\n\n\n\n4.2.2 Filtering Spectra\n\nCode# Filter by MS level\nms1_spectra &lt;- filterMsLevel(ms_data, msLevel = 1)\nms2_spectra &lt;- filterMsLevel(ms_data, msLevel = 2)\n\ncat(\"MS1 spectra:\", length(ms1_spectra), \"\\n\")\n\nMS1 spectra: 1544 \n\nCodecat(\"MS2 spectra:\", length(ms2_spectra), \"\\n\")\n\nMS2 spectra: 7306 \n\n\n\nCode# Filter by retention time\nrt_filtered &lt;- filterRt(ms_data, rt = c(1000, 2000))\nlength(rt_filtered)\n\n[1] 3011\n\n\n\n4.2.3 Metadata Extraction\n\nCode# Extract comprehensive metadata\nspectra_df &lt;- spectraData(ms_data) %&gt;%\n  as.data.frame() %&gt;%\n  select(msLevel, rtime, precursorMz, precursorCharge, \n         precursorIntensity) %&gt;%\n  head(10)\n\nprint(spectra_df)\n\n   msLevel    rtime precursorMz precursorCharge precursorIntensity\n1        1 4422.620          NA              NA                 NA\n2        2 4422.648    652.3426               3            9841531\n3        2 4422.670    647.3441               3            3921567\n4        2 4422.735    673.3353               3            7623700\n5        2 4422.800    575.0026               3            2357085\n6        3 4423.036    490.2629               0                  0\n7        2 4423.053    820.9681               2            1847945\n8        2 4423.094    675.6938               3            4257284\n9        3 4423.367    756.4332               1                  0\n10       2 4423.384    608.2888               2            5362638",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "03-data-formats-import.html#other-important-formats",
    "href": "03-data-formats-import.html#other-important-formats",
    "title": "\n4  Mass Spectrometry Data Formats and Import\n",
    "section": "\n4.3 Other Important Formats",
    "text": "4.3 Other Important Formats\n\n4.3.1 MGF (Mascot Generic Format)\n\nCode# Reading MGF files\nmgf_file &lt;- \"path/to/file.mgf\"\nmgf_data &lt;- Spectra(mgf_file)\n\n\n\n4.3.2 Text-based Formats\n\nCode# Converting to data frames for analysis\nms1_df &lt;- ms1_spectra %&gt;%\n  spectraData() %&gt;%\n  as.data.frame() %&gt;%\n  select(rtime, precursorMz, precursorIntensity)\n\nhead(ms1_df)\n\n     rtime precursorMz precursorIntensity\n1 4422.620          NA                 NA\n2 4425.460          NA                 NA\n3 4428.595          NA                 NA\n4 4431.419          NA                 NA\n5 4434.330          NA                 NA\n6 4437.434          NA                 NA",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "03-data-formats-import.html#data-export",
    "href": "03-data-formats-import.html#data-export",
    "title": "\n4  Mass Spectrometry Data Formats and Import\n",
    "section": "\n4.4 Data Export",
    "text": "4.4 Data Export\n\n4.4.1 Exporting to Different Formats\n\nCode# Export to mzML\nexport(ms_data, MzMLFile(\"output.mzML\"))\n\n# Export to MGF\nexport(ms2_spectra, MgfFile(\"ms2_spectra.mgf\"))\n\n\n\n4.4.2 Exporting Processed Data\n\nCode# Export metadata to CSV\nwrite.csv(spectra_df, \"spectra_metadata.csv\", row.names = FALSE)",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "03-data-formats-import.html#quality-control-and-data-inspection",
    "href": "03-data-formats-import.html#quality-control-and-data-inspection",
    "title": "\n4  Mass Spectrometry Data Formats and Import\n",
    "section": "\n4.5 Quality Control and Data Inspection",
    "text": "4.5 Quality Control and Data Inspection\n\n4.5.1 Basic QC Checks\n\nCode# Check for missing data\nsummary(is.na(precursorMz(ms_data)))\n\n   Mode   FALSE    TRUE \nlogical    8058    1544 \n\nCodesummary(is.na(rtime(ms_data)))\n\n   Mode   FALSE \nlogical    9602 \n\n\n\n4.5.2 Visualizing Data Overview\n\nCodelibrary(ggplot2)\n\n# Retention time distribution\nrt_data &lt;- data.frame(rtime = rtime(ms_data), msLevel = msLevel(ms_data))\n\nggplot(rt_data, aes(x = rtime, fill = factor(msLevel))) +\n  geom_histogram(bins = 50, alpha = 0.7) +\n  facet_wrap(~msLevel) +\n  labs(title = \"Retention Time Distribution by MS Level\",\n       x = \"Retention Time (s)\", y = \"Count\") +\n  theme_minimal()",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "03-data-formats-import.html#practical-examples",
    "href": "03-data-formats-import.html#practical-examples",
    "title": "\n4  Mass Spectrometry Data Formats and Import\n",
    "section": "\n4.6 Practical Examples",
    "text": "4.6 Practical Examples\n\n4.6.1 Loading Multiple Files\n\nCode# Load multiple mzML files\nfile_list &lt;- list.files(\"data/\", pattern = \"*.mzML\", full.names = TRUE)\nall_data &lt;- Spectra(file_list)\n\n\n\n4.6.2 Combining Datasets\n\nCode# Combine multiple Spectra objects\ncombined_data &lt;- c(ms_data1, ms_data2, ms_data3)",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "03-data-formats-import.html#exercises",
    "href": "03-data-formats-import.html#exercises",
    "title": "\n4  Mass Spectrometry Data Formats and Import\n",
    "section": "\n4.7 Exercises",
    "text": "4.7 Exercises\n\nLoad an mzML file and examine its basic properties\nFilter spectra by different criteria (RT, MS level, m/z range)\nExport filtered data to different formats\nCreate quality control plots for your data",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "03-data-formats-import.html#summary",
    "href": "03-data-formats-import.html#summary",
    "title": "\n4  Mass Spectrometry Data Formats and Import\n",
    "section": "\n4.8 Summary",
    "text": "4.8 Summary\nThis chapter covered the major MS data formats and how to import, manipulate, and export them using R. Understanding these fundamentals is essential for all downstream analysis tasks.",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Mass Spectrometry Data Formats and Import</span>"
    ]
  },
  {
    "objectID": "04-spectral-preprocessing.html",
    "href": "04-spectral-preprocessing.html",
    "title": "\n5  Spectral Data Preprocessing\n",
    "section": "",
    "text": "5.1 Loading Required Libraries\nPreprocessing is a critical step in MS data analysis that improves data quality and enables accurate downstream analysis. This chapter covers baseline correction, smoothing, normalization, and peak picking using modern R for Mass Spectrometry tools.\nCodelibrary(Spectra)           # Core MS data handling\nlibrary(MsCoreUtils)       # MS processing utilities\nlibrary(msdata)            # Example datasets\nlibrary(ProtGenerics)      # Generic functions\nlibrary(ggplot2)           # Visualization\nlibrary(dplyr)             # Data manipulation\nlibrary(patchwork)         # Plot composition",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "04-spectral-preprocessing.html#understanding-raw-spectral-data",
    "href": "04-spectral-preprocessing.html#understanding-raw-spectral-data",
    "title": "\n5  Spectral Data Preprocessing\n",
    "section": "\n5.2 Understanding Raw Spectral Data",
    "text": "5.2 Understanding Raw Spectral Data\n\n5.2.1 Loading and Inspecting Spectra\n\nCode# Load example proteomics data\n# Note: Using setBackend to convert to in-memory storage to avoid mzR issues\nms_file &lt;- msdata::proteomics(full.names = TRUE)[1]\n\n# First load with MsBackendMzR, then convert to DataFrame backend\ntryCatch({\n  ms_data &lt;- Spectra(ms_file, backend = MsBackendMzR())\n  # Convert to in-memory backend for better compatibility\n  ms_data &lt;- setBackend(ms_data, backend = MsBackendDataFrame())\n  cat(\"Successfully loaded real MS data\\n\")\n}, error = function(e) {\n  # If mzR fails, create synthetic data for demonstration\n  cat(\"Note: Using synthetic data due to mzR compatibility issues\\n\")\n  cat(\"Error was:\", conditionMessage(e), \"\\n\\n\")\n  \n  # Create synthetic MS2 spectra using proper format\n  set.seed(123)\n  n_spectra &lt;- 100\n  \n  # Create peaks data - separate m/z and intensity lists\n  mz_list &lt;- lapply(1:n_spectra, function(i) {\n    n_peaks &lt;- sample(50:200, 1)\n    sort(runif(n_peaks, 100, 2000))  # Already sorted\n  })\n  \n  intensity_list &lt;- lapply(mz_list, function(mz_vals) {\n    rlnorm(length(mz_vals), meanlog = 8, sdlog = 2)\n  })\n  \n  # Create spectra data frame with metadata\n  library(S4Vectors)\n  library(IRanges)\n  spd &lt;- DataFrame(\n    msLevel = rep(2L, n_spectra),\n    rtime = seq(100, 6000, length.out = n_spectra),\n    precursorMz = runif(n_spectra, 400, 1500),\n    precursorCharge = sample(2:3, n_spectra, replace = TRUE),\n    polarity = rep(1L, n_spectra)\n  )\n  \n  # Add list columns using NumericList\n  spd$mz &lt;- NumericList(mz_list)\n  spd$intensity &lt;- NumericList(intensity_list)\n  \n  # Create Spectra object from DataFrame backend\n  backend &lt;- MsBackendDataFrame()\n  backend &lt;- backendInitialize(backend, spd)\n  ms_data &lt;&lt;- Spectra(backend)\n})\n\nNote: Using synthetic data due to mzR compatibility issues\nError was: BiocParallel errors\n  1 remote errors, element index: 1\n  0 unevaluated and other errors\n  first remote error:\nError in DataFrame(..., check.names = FALSE): different row counts implied by arguments\n \n\nCode# Focus on MS2 spectra for preprocessing examples\nms2_data &lt;- filterMsLevel(ms_data, 2)\ncat(\"Total MS2 spectra:\", length(ms2_data), \"\\n\")\n\nTotal MS2 spectra: 100 \n\nCode# Select a representative spectrum\nspectrum &lt;- ms2_data[10]\n\n# Extract peak data\npeaks &lt;- peaksData(spectrum)[[1]]\nmz_vals &lt;- peaks[, 1]\nint_vals &lt;- peaks[, 2]\n\ncat(\"Spectrum contains\", length(mz_vals), \"peaks\\n\")\n\nSpectrum contains 109 peaks\n\nCodecat(\"m/z range:\", round(range(mz_vals), 2), \"\\n\")\n\nm/z range: 124.91 1985.97 \n\nCodecat(\"Intensity range:\", sprintf(\"%.2e - %.2e\", min(int_vals), max(int_vals)), \"\\n\")\n\nIntensity range: 1.37e+01 - 9.02e+05 \n\n\n\n5.2.2 Visualizing Raw Data\n\nCode# Create a visualization of raw spectrum\nraw_df &lt;- data.frame(mz = mz_vals, intensity = int_vals)\n\np_raw &lt;- ggplot(raw_df, aes(x = mz, y = intensity)) +\n  geom_segment(aes(xend = mz, yend = 0), color = \"steelblue\", alpha = 0.6) +\n  labs(title = \"Raw MS2 Spectrum\",\n       subtitle = paste(\"Precursor:\", round(precursorMz(spectrum), 3), \"m/z\"),\n       x = \"m/z\", y = \"Intensity\") +\n  theme_minimal()\n\nprint(p_raw)",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "04-spectral-preprocessing.html#baseline-correction",
    "href": "04-spectral-preprocessing.html#baseline-correction",
    "title": "\n5  Spectral Data Preprocessing\n",
    "section": "\n5.3 Baseline Correction",
    "text": "5.3 Baseline Correction\n\n5.3.1 Understanding Baseline Issues\n\nCode# Use the already loaded ms_data from above to avoid reloading\n# Get a representative spectrum\nspectrum &lt;- ms_data[min(100, length(ms_data))]\npeaks &lt;- peaksData(spectrum)[[1]]\nmz_vals &lt;- peaks[, 1]\nint_vals &lt;- peaks[, 2]\n\n# Plot raw spectrum\nplot(mz_vals, int_vals, type = \"l\", \n     main = \"Raw Spectrum\", \n     xlab = \"m/z\", ylab = \"Intensity\")\n\n\n\n\n\n\n\n\n5.3.2 Baseline Removal Methods\n\nCode# Simple baseline correction using quantile-based approach\nbaseline_estimate &lt;- quantile(int_vals, 0.05)  # 5th percentile\ncorrected_intensity &lt;- pmax(int_vals - baseline_estimate, 0)\n\n# Plot corrected spectrum\nplot(mz_vals, corrected_intensity, type = \"l\",\n     main = \"Baseline Corrected Spectrum\",\n     xlab = \"m/z\", ylab = \"Intensity\")",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "04-spectral-preprocessing.html#smoothing-techniques",
    "href": "04-spectral-preprocessing.html#smoothing-techniques",
    "title": "\n5  Spectral Data Preprocessing\n",
    "section": "\n5.4 Smoothing Techniques",
    "text": "5.4 Smoothing Techniques\nSmoothing reduces noise while preserving spectral features. The Spectra package provides built-in smoothing methods.\n\n5.4.1 Savitzky-Golay Smoothing\n\nCode# Apply Savitzky-Golay smoothing using Spectra\n# halfWindowSize must be an integer\nsmoothed_spectrum &lt;- smooth(spectrum, method = \"SavitzkyGolay\", halfWindowSize = 2L)\n\n# Extract smoothed data\nsmoothed_peaks &lt;- peaksData(smoothed_spectrum)[[1]]\n\n# Compare original and smoothed\ncomparison_df &lt;- data.frame(\n  mz = c(mz_vals, smoothed_peaks[, 1]),\n  intensity = c(int_vals, smoothed_peaks[, 2]),\n  type = rep(c(\"Original\", \"Smoothed\"), c(length(mz_vals), nrow(smoothed_peaks)))\n)\n\np_smooth &lt;- ggplot(comparison_df, aes(x = mz, y = intensity, color = type)) +\n  geom_line(alpha = 0.7) +\n  scale_color_manual(values = c(\"Original\" = \"gray60\", \"Smoothed\" = \"red\")) +\n  labs(title = \"Savitzky-Golay Smoothing\",\n       x = \"m/z\", y = \"Intensity\", color = \"Type\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nprint(p_smooth)\n\n\n\n\n\n\n\n\n5.4.2 Moving Average Smoothing\n\nCode# Custom moving average implementation\nmoving_average &lt;- function(x, window = 5) {\n  n &lt;- length(x)\n  smoothed &lt;- numeric(n)\n  half_window &lt;- floor(window / 2)\n  \n  for (i in 1:n) {\n    start_idx &lt;- max(1, i - half_window)\n    end_idx &lt;- min(n, i + half_window)\n    smoothed[i] &lt;- mean(x[start_idx:end_idx])\n  }\n  return(smoothed)\n}\n\n# Apply moving average\nma_intensity &lt;- moving_average(int_vals, window = 5)\n\n# Visualize comparison\nma_df &lt;- data.frame(\n  mz = mz_vals,\n  original = int_vals,\n  moving_avg = ma_intensity\n)\n\nggplot(ma_df) +\n  geom_line(aes(x = mz, y = original), color = \"gray60\", alpha = 0.7) +\n  geom_line(aes(x = mz, y = moving_avg), color = \"blue\", size = 1) +\n  labs(title = \"Moving Average Smoothing (window = 5)\",\n       x = \"m/z\", y = \"Intensity\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n5.4.3 Gaussian Smoothing\n\nCode# Gaussian smoothing implementation\ngaussian_smooth &lt;- function(x, sigma = 1) {\n  n &lt;- length(x)\n  kernel_size &lt;- ceiling(3 * sigma)\n  kernel &lt;- exp(-((-kernel_size):kernel_size)^2 / (2 * sigma^2))\n  kernel &lt;- kernel / sum(kernel)\n  \n  # Apply convolution (simplified)\n  smoothed &lt;- stats::filter(x, kernel, sides = 2)\n  smoothed[is.na(smoothed)] &lt;- x[is.na(smoothed)]  # Handle edges\n  return(as.numeric(smoothed))\n}\n\n# Apply Gaussian smoothing to the original intensity values\ngaussian_smoothed &lt;- gaussian_smooth(int_vals, sigma = 2)\n\nplot(mz_vals, gaussian_smoothed, type = \"l\", col = \"blue\",\n     main = \"Gaussian Smoothed Spectrum\", xlab = \"m/z\", ylab = \"Intensity\")\nlines(mz_vals, int_vals, col = \"gray\", lty = 2)\nlegend(\"topright\", c(\"Gaussian Smoothed\", \"Original\"), \n       col = c(\"blue\", \"gray\"), lty = c(1, 2))",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "04-spectral-preprocessing.html#peak-detection",
    "href": "04-spectral-preprocessing.html#peak-detection",
    "title": "\n5  Spectral Data Preprocessing\n",
    "section": "\n5.5 Peak Detection",
    "text": "5.5 Peak Detection\n\n5.5.1 Simple Peak Picking Algorithm\n\nCode# Use the moving average smoothed data for peak detection\nsmoothed_intensity &lt;- ma_intensity\n\n# Simple peak detection function\ndetect_peaks &lt;- function(mz, intensity, min_intensity = 1000, min_distance = 0.1) {\n  n &lt;- length(intensity)\n  peaks &lt;- logical(n)\n  \n  for (i in 2:(n-1)) {\n    if (intensity[i] &gt; intensity[i-1] && \n        intensity[i] &gt; intensity[i+1] && \n        intensity[i] &gt; min_intensity) {\n      peaks[i] &lt;- TRUE\n    }\n  }\n  \n  # Filter by minimum distance\n  peak_indices &lt;- which(peaks)\n  if (length(peak_indices) &gt; 1) {\n    keep &lt;- logical(length(peak_indices))\n    keep[1] &lt;- TRUE\n    \n    for (i in 2:length(peak_indices)) {\n      if (mz[peak_indices[i]] - mz[peak_indices[keep][sum(keep)]] &gt; min_distance) {\n        keep[i] &lt;- TRUE\n      }\n    }\n    peak_indices &lt;- peak_indices[keep]\n  }\n  \n  return(list(\n    mz = mz[peak_indices],\n    intensity = intensity[peak_indices],\n    indices = peak_indices\n  ))\n}\n\n# Detect peaks\npeaks &lt;- detect_peaks(mz_vals, smoothed_intensity, min_intensity = 5000)\n\n# Plot spectrum with detected peaks\nplot(mz_vals, smoothed_intensity, type = \"l\",\n     main = \"Peak Detection Results\", xlab = \"m/z\", ylab = \"Intensity\")\npoints(peaks$mz, peaks$intensity, col = \"red\", pch = 19)\n\n\n\n\n\n\n\n\n5.5.2 Peak Statistics\n\nCode# Analyze detected peaks\ncat(\"Number of peaks detected:\", length(peaks$mz), \"\\n\")\n\nNumber of peaks detected: 40 \n\nCodecat(\"Peak m/z range:\", range(peaks$mz), \"\\n\")\n\nPeak m/z range: 170.7168 1915.761 \n\nCodecat(\"Peak intensity range:\", range(peaks$intensity), \"\\n\")\n\nPeak intensity range: 5006.15 212604.2 \n\nCode# Create peak list data frame\npeak_list &lt;- data.frame(\n  mz = peaks$mz,\n  intensity = peaks$intensity,\n  relative_intensity = peaks$intensity / max(peaks$intensity) * 100\n)\n\nhead(peak_list)\n\n        mz intensity relative_intensity\n1 170.7168  35103.09           16.51100\n2 208.6494  34931.65           16.43037\n3 266.4594 122379.71           57.56223\n4 309.7539  45276.72           21.29625\n5 374.9435  35739.82           16.81049\n6 400.7718  28922.93           13.60412",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "04-spectral-preprocessing.html#normalization",
    "href": "04-spectral-preprocessing.html#normalization",
    "title": "\n5  Spectral Data Preprocessing\n",
    "section": "\n5.6 Normalization",
    "text": "5.6 Normalization\n\n5.6.1 Total Ion Current (TIC) Normalization\n\nCode# TIC normalization\ntic_normalize &lt;- function(intensity) {\n  tic &lt;- sum(intensity)\n  return(intensity / tic * 1e6)  # Scale to parts per million\n}\n\nnormalized_intensity &lt;- tic_normalize(smoothed_intensity)\n\n# Compare before and after normalization\npar(mfrow = c(2, 1))\nplot(mz_vals, smoothed_intensity, type = \"l\",\n     main = \"Before TIC Normalization\", xlab = \"m/z\", ylab = \"Intensity\")\nplot(mz_vals, normalized_intensity, type = \"l\",\n     main = \"After TIC Normalization\", xlab = \"m/z\", ylab = \"Normalized Intensity\")\n\n\n\n\n\n\nCodepar(mfrow = c(1, 1))\n\n\n\n5.6.2 Base Peak Normalization\n\nCode# Base peak normalization\nbase_peak_normalize &lt;- function(intensity) {\n  base_peak &lt;- max(intensity)\n  return(intensity / base_peak * 100)\n}\n\nbp_normalized &lt;- base_peak_normalize(smoothed_intensity)\n\nplot(mz_vals, bp_normalized, type = \"l\",\n     main = \"Base Peak Normalized Spectrum\", \n     xlab = \"m/z\", ylab = \"Relative Intensity (%)\")",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "04-spectral-preprocessing.html#processing-multiple-spectra",
    "href": "04-spectral-preprocessing.html#processing-multiple-spectra",
    "title": "\n5  Spectral Data Preprocessing\n",
    "section": "\n5.7 Processing Multiple Spectra",
    "text": "5.7 Processing Multiple Spectra\n\n5.7.1 Batch Processing Function\n\nCodeprocess_spectrum &lt;- function(spec, baseline_quantile = 0.05, \n                            smooth_window = 5, min_peak_intensity = 1000) {\n  mz_vals &lt;- mz(spec)[[1]]\n  int_vals &lt;- intensity(spec)[[1]]\n  \n  # Baseline correction\n  baseline &lt;- quantile(int_vals, baseline_quantile)\n  int_vals &lt;- pmax(int_vals - baseline, 0)\n  \n  # Smoothing\n  int_vals &lt;- moving_average(int_vals, smooth_window)\n  \n  # Normalization\n  int_vals &lt;- base_peak_normalize(int_vals)\n  \n  # Peak detection\n  peaks &lt;- detect_peaks(mz_vals, int_vals, min_peak_intensity)\n  \n  return(list(\n    mz = mz_vals,\n    intensity = int_vals,\n    peaks = peaks\n  ))\n}\n\n# Process first 10 spectra\nprocessed_results &lt;- list()\nfor (i in 1:min(10, length(ms_data))) {\n  processed_results[[i]] &lt;- process_spectrum(ms_data[i])\n}\n\n\n\n5.7.2 Quality Assessment\n\nCode# Assess processing quality\npeak_counts &lt;- sapply(processed_results, function(x) length(x$peaks$mz))\ncat(\"Peak counts across processed spectra:\\n\")\n\nPeak counts across processed spectra:\n\nCodesummary(peak_counts)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0       0       0       0       0       0 \n\nCode# Plot peak count distribution\nhist(peak_counts, breaks = 10, \n     main = \"Distribution of Peak Counts\", \n     xlab = \"Number of Peaks\", ylab = \"Frequency\")",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "04-spectral-preprocessing.html#advanced-preprocessing-techniques",
    "href": "04-spectral-preprocessing.html#advanced-preprocessing-techniques",
    "title": "\n5  Spectral Data Preprocessing\n",
    "section": "\n5.8 Advanced Preprocessing Techniques",
    "text": "5.8 Advanced Preprocessing Techniques\n\n5.8.1 Mass Calibration Concepts\n\nCode# Simple mass calibration example (theoretical)\ncalibrate_mass &lt;- function(observed_mz, reference_mz, expected_mz) {\n  # Linear calibration\n  calibration_factor &lt;- expected_mz / reference_mz\n  calibrated_mz &lt;- observed_mz * calibration_factor\n  return(calibrated_mz)\n}\n\n# Example usage (with hypothetical values)\nobserved &lt;- c(100.0, 200.1, 300.2)\nreference &lt;- 200.1\nexpected &lt;- 200.0\n\ncalibrated &lt;- calibrate_mass(observed, reference, expected)\ncat(\"Original m/z:\", observed, \"\\n\")\n\nOriginal m/z: 100 200.1 300.2 \n\nCodecat(\"Calibrated m/z:\", calibrated, \"\\n\")\n\nCalibrated m/z: 99.95002 200 300.05",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "04-spectral-preprocessing.html#preprocessing-pipeline",
    "href": "04-spectral-preprocessing.html#preprocessing-pipeline",
    "title": "\n5  Spectral Data Preprocessing\n",
    "section": "\n5.9 Preprocessing Pipeline",
    "text": "5.9 Preprocessing Pipeline\n\n5.9.1 Complete Pipeline Function\n\nCodecomplete_preprocessing &lt;- function(spectra_obj, \n                                  baseline_quantile = 0.05,\n                                  smooth_sigma = 2,\n                                  normalization = \"base_peak\",\n                                  min_peak_intensity = 1000) {\n  \n  processed_spectra &lt;- list()\n  \n  for (i in seq_along(spectra_obj)) {\n    spec &lt;- spectra_obj[i]\n    mz_vals &lt;- mz(spec)[[1]]\n    int_vals &lt;- intensity(spec)[[1]]\n    \n    # Step 1: Baseline correction\n    baseline &lt;- quantile(int_vals, baseline_quantile)\n    int_vals &lt;- pmax(int_vals - baseline, 0)\n    \n    # Step 2: Smoothing\n    int_vals &lt;- gaussian_smooth(int_vals, sigma = smooth_sigma)\n    \n    # Step 3: Normalization\n    if (normalization == \"tic\") {\n      int_vals &lt;- tic_normalize(int_vals)\n    } else if (normalization == \"base_peak\") {\n      int_vals &lt;- base_peak_normalize(int_vals)\n    }\n    \n    # Step 4: Peak detection\n    peaks &lt;- detect_peaks(mz_vals, int_vals, min_peak_intensity)\n    \n    processed_spectra[[i]] &lt;- list(\n      original_index = i,\n      mz = mz_vals,\n      intensity = int_vals,\n      peaks = peaks,\n      metadata = list(\n        processing_date = Sys.time(),\n        parameters = list(\n          baseline_quantile = baseline_quantile,\n          smooth_sigma = smooth_sigma,\n          normalization = normalization,\n          min_peak_intensity = min_peak_intensity\n        )\n      )\n    )\n  }\n  \n  return(processed_spectra)\n}",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "04-spectral-preprocessing.html#exercises",
    "href": "04-spectral-preprocessing.html#exercises",
    "title": "\n5  Spectral Data Preprocessing\n",
    "section": "\n5.10 Exercises",
    "text": "5.10 Exercises\n\nApply different baseline correction methods and compare results\nExperiment with various smoothing parameters\nImplement and test different peak detection algorithms\nCompare TIC and base peak normalization approaches\nCreate a quality control function for preprocessing results",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "04-spectral-preprocessing.html#summary",
    "href": "04-spectral-preprocessing.html#summary",
    "title": "\n5  Spectral Data Preprocessing\n",
    "section": "\n5.11 Summary",
    "text": "5.11 Summary\nThis chapter covered essential preprocessing techniques for MS data, including baseline correction, smoothing, peak detection, and normalization. These steps are fundamental for preparing data for downstream analysis and ensuring reliable results.",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Spectral Data Preprocessing</span>"
    ]
  },
  {
    "objectID": "05-peak-detection-quantification.html",
    "href": "05-peak-detection-quantification.html",
    "title": "\n6  Peak Detection and Quantification\n",
    "section": "",
    "text": "6.1 Setting Up the Environment\nPeak detection and quantification form the foundation of MS data analysis workflows. This chapter covers modern algorithms implemented in the R for Mass Spectrometry ecosystem and practical implementations using Spectra and MsCoreUtils.\nCodeif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(c(\"xcms\", \"Spectra\", \"MetaboCoreUtils\", \"MsCoreUtils\"))\n\nlibrary(Spectra)           # Core MS data structures\nlibrary(MsCoreUtils)       # MS processing utilities\nlibrary(msdata)            # Example datasets\nlibrary(ggplot2)           # Visualization\nlibrary(dplyr)             # Data manipulation\nlibrary(patchwork)         # Plot composition\nCode# Load example data with error handling\nms_file &lt;- msdata::proteomics(full.names = TRUE)[1]\n\ntryCatch({\n  ms_data &lt;- Spectra(ms_file, backend = MsBackendMzR())\n  ms_data &lt;- setBackend(ms_data, backend = MsBackendDataFrame())\n  cat(\"Successfully loaded real MS data\\n\")\n}, error = function(e) {\n  cat(\"Note: Using synthetic data due to mzR compatibility issues\\n\\n\")\n  \n  # Create synthetic MS2 data\n  set.seed(456)\n  n_spectra &lt;- 100\n  \n  library(S4Vectors)\n  library(IRanges)\n  \n  mz_list &lt;- lapply(1:n_spectra, function(i) {\n    sort(runif(sample(80:200, 1), 100, 2000))\n  })\n  \n  intensity_list &lt;- lapply(mz_list, function(mz_vals) {\n    rlnorm(length(mz_vals), meanlog = 8, sdlog = 2)\n  })\n  \n  spd &lt;- DataFrame(\n    msLevel = rep(2L, n_spectra),\n    rtime = seq(100, 6000, length.out = n_spectra),\n    precursorMz = runif(n_spectra, 400, 1500),\n    precursorCharge = sample(2:3, n_spectra, replace = TRUE),\n    polarity = rep(1L, n_spectra)\n  )\n  \n  spd$mz &lt;- NumericList(mz_list)\n  spd$intensity &lt;- NumericList(intensity_list)\n  \n  backend &lt;- MsBackendDataFrame()\n  backend &lt;- backendInitialize(backend, spd)\n  ms_data &lt;&lt;- Spectra(backend)\n})\n\nNote: Using synthetic data due to mzR compatibility issues\n\nCode# Focus on MS2 spectra\nms2_data &lt;- filterMsLevel(ms_data, 2)\n\n# Select a representative spectrum for analysis\ntest_spectrum &lt;- ms2_data[min(50, length(ms2_data))]\npeaks &lt;- peaksData(test_spectrum)[[1]]\nmz_vals &lt;- peaks[, 1]\nint_vals &lt;- peaks[, 2]\n\ncat(\"Selected spectrum:\\n\")\n\nSelected spectrum:\n\nCodecat(\"  Precursor m/z:\", round(precursorMz(test_spectrum), 3), \"\\n\")\n\n  Precursor m/z: 754.184 \n\nCodecat(\"  Number of peaks:\", length(mz_vals), \"\\n\")\n\n  Number of peaks: 133 \n\nCodecat(\"  Intensity range:\", sprintf(\"%.2e - %.2e\", min(int_vals), max(int_vals)), \"\\n\")\n\n  Intensity range: 1.19e+01 - 2.37e+06",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "05-peak-detection-quantification.html#modern-peak-detection-with-spectra",
    "href": "05-peak-detection-quantification.html#modern-peak-detection-with-spectra",
    "title": "\n6  Peak Detection and Quantification\n",
    "section": "\n6.2 Modern Peak Detection with Spectra",
    "text": "6.2 Modern Peak Detection with Spectra\n\n6.2.1 Built-in Peak Picking\nThe Spectra package provides optimized peak picking methods:\n\nCode# Apply MAD (Median Absolute Deviation) based peak picking\npicked_spectrum &lt;- pickPeaks(test_spectrum, \n                             method = \"MAD\",\n                             snr = 2,  # Signal-to-noise ratio threshold\n                             k = 1L,   # Half window size (must be integer)\n                             descending = FALSE)\n\n# Compare before and after\noriginal_peaks &lt;- peaksData(test_spectrum)[[1]]\npicked_peaks &lt;- peaksData(picked_spectrum)[[1]]\n\ncat(\"Peak reduction:\\n\")\n\nPeak reduction:\n\nCodecat(\"  Original peaks:\", nrow(original_peaks), \"\\n\")\n\n  Original peaks: 133 \n\nCodecat(\"  After picking:\", nrow(picked_peaks), \"\\n\")\n\n  After picking: 24 \n\nCodecat(\"  Reduction:\", round((1 - nrow(picked_peaks)/nrow(original_peaks)) * 100, 1), \"%\\n\")\n\n  Reduction: 82 %\n\nCode# Visualize the difference\ncomparison_df &lt;- data.frame(\n  mz = c(original_peaks[, 1], picked_peaks[, 1]),\n  intensity = c(original_peaks[, 2], picked_peaks[, 2]),\n  type = rep(c(\"Original\", \"Picked\"), c(nrow(original_peaks), nrow(picked_peaks)))\n)\n\nggplot(comparison_df, aes(x = mz, y = intensity, color = type)) +\n  geom_segment(aes(xend = mz, yend = 0), alpha = 0.6) +\n  scale_color_manual(values = c(\"Original\" = \"gray70\", \"Picked\" = \"red\")) +\n  labs(title = \"Peak Picking with MAD Method\",\n       subtitle = paste(\"SNR threshold:\", 2),\n       x = \"m/z\", y = \"Intensity\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n6.2.2 Noise Estimation\n\nCode# Estimate noise level using MAD (Median Absolute Deviation)\n# Using a simple implementation since MsCoreUtils::noise has specific requirements\nestimate_noise_mad &lt;- function(x) {\n  # Use MAD (Median Absolute Deviation) for robust noise estimation\n  median_val &lt;- median(x, na.rm = TRUE)\n  mad_val &lt;- median(abs(x - median_val), na.rm = TRUE)\n  # MAD-based noise estimate\n  return(mad_val * 1.4826)  # Scale factor for normal distribution\n}\n\nnoise_level &lt;- estimate_noise_mad(int_vals)\ncat(\"Estimated noise level:\", round(noise_level, 2), \"\\n\")\n\nEstimated noise level: 3277.76 \n\nCode# Calculate signal-to-noise ratios\nsnr_values &lt;- int_vals / noise_level\ncat(\"SNR statistics:\\n\")\n\nSNR statistics:\n\nCodecat(\"  Median SNR:\", round(median(snr_values), 2), \"\\n\")\n\n  Median SNR: 0.75 \n\nCodecat(\"  Mean SNR:\", round(mean(snr_values), 2), \"\\n\")\n\n  Mean SNR: 9.31 \n\nCodecat(\"  Max SNR:\", round(max(snr_values), 2), \"\\n\")\n\n  Max SNR: 723.41 \n\nCode# Visualize SNR distribution\nsnr_df &lt;- data.frame(snr = snr_values)\nggplot(snr_df, aes(x = snr)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = 2, color = \"red\", linetype = \"dashed\") +\n  scale_x_log10() +\n  labs(title = \"Signal-to-Noise Ratio Distribution\",\n       subtitle = \"Red line indicates common SNR threshold (2)\",\n       x = \"SNR (log scale)\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCode# Matched filter approach for peak detection\nmatched_filter_detection &lt;- function(mz, intensity, \n                                   peak_width = 0.1,  # Expected peak width\n                                   threshold = 0.1) {\n  \n  # Create Gaussian template\n  sigma &lt;- peak_width / (2 * sqrt(2 * log(2)))  # Convert FWHM to sigma\n  template_size &lt;- ceiling(6 * sigma)  # Template size\n  x &lt;- seq(-template_size, template_size, length.out = 2 * template_size + 1)\n  template &lt;- exp(-x^2 / (2 * sigma^2))\n  template &lt;- template / sum(template)\n  \n  # Apply matched filter\n  filtered &lt;- stats::filter(intensity, template, sides = 2)\n  filtered[is.na(filtered)] &lt;- 0\n  \n  # Threshold and find local maxima\n  above_threshold &lt;- filtered &gt; threshold * max(filtered)\n  peaks &lt;- logical(length(intensity))\n  \n  for (i in 2:(length(intensity)-1)) {\n    if (above_threshold[i] && \n        filtered[i] &gt; filtered[i-1] && \n        filtered[i] &gt; filtered[i+1]) {\n      peaks[i] &lt;- TRUE\n    }\n  }\n  \n  return(list(\n    peaks = which(peaks),\n    filtered_signal = filtered\n  ))\n}\n\n# Apply matched filter\nmf_result &lt;- matched_filter_detection(mz_vals, int_vals)\n\n# Plot results\npar(mfrow = c(2, 1))\nplot(mz_vals, int_vals, type = \"l\", main = \"Original Signal\", \n     xlab = \"m/z\", ylab = \"Intensity\")\nplot(mz_vals, mf_result$filtered_signal, type = \"l\", main = \"Matched Filter Response\",\n     xlab = \"m/z\", ylab = \"Filtered Response\", col = \"blue\")\npoints(mz_vals[mf_result$peaks], mf_result$filtered_signal[mf_result$peaks], \n       col = \"red\", pch = 19)\n\n\n\n\n\n\nCodepar(mfrow = c(1, 1))",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "05-peak-detection-quantification.html#peak-quantification-methods",
    "href": "05-peak-detection-quantification.html#peak-quantification-methods",
    "title": "\n6  Peak Detection and Quantification\n",
    "section": "\n6.3 Peak Quantification Methods",
    "text": "6.3 Peak Quantification Methods\n\n6.3.1 Peak Integration\n\nCode# Peak integration function\nintegrate_peak &lt;- function(mz, intensity, peak_center, integration_method = \"trapezoid\") {\n  # Find peak boundaries\n  peak_idx &lt;- which.min(abs(mz - peak_center))\n  \n  # Simple peak boundary detection\n  left_bound &lt;- peak_idx\n  right_bound &lt;- peak_idx\n  \n  # Find left boundary (where intensity drops significantly)\n  while (left_bound &gt; 1 && \n         intensity[left_bound - 1] &gt; 0.1 * intensity[peak_idx]) {\n    left_bound &lt;- left_bound - 1\n  }\n  \n  # Find right boundary\n  while (right_bound &lt; length(intensity) && \n         intensity[right_bound + 1] &gt; 0.1 * intensity[peak_idx]) {\n    right_bound &lt;- right_bound + 1\n  }\n  \n  # Integration\n  if (integration_method == \"trapezoid\") {\n    # Trapezoidal rule\n    mz_region &lt;- mz[left_bound:right_bound]\n    int_region &lt;- intensity[left_bound:right_bound]\n    \n    area &lt;- sum(diff(mz_region) * (int_region[-1] + int_region[-length(int_region)]) / 2)\n  } else if (integration_method == \"sum\") {\n    # Simple sum\n    area &lt;- sum(intensity[left_bound:right_bound])\n  }\n  \n  return(list(\n    area = area,\n    peak_height = intensity[peak_idx],\n    peak_mz = mz[peak_idx],\n    left_bound = left_bound,\n    right_bound = right_bound,\n    boundaries_mz = c(mz[left_bound], mz[right_bound])\n  ))\n}\n\n# Demonstrate peak integration\nif (length(mf_result$peaks) &gt; 0) {\n  # Integrate first detected peak\n  peak_result &lt;- integrate_peak(mz_vals, int_vals, mz_vals[mf_result$peaks[1]])\n  \n  # Visualize integration region\n  plot(mz_vals, int_vals, type = \"l\", \n       main = \"Peak Integration Example\", xlab = \"m/z\", ylab = \"Intensity\")\n  \n  # Highlight integration region\n  int_region &lt;- peak_result$left_bound:peak_result$right_bound\n  polygon(c(mz_vals[int_region], rev(mz_vals[int_region])),\n           c(int_vals[int_region], rep(0, length(int_region))),\n           col = \"lightblue\", border = NA)\n  lines(mz_vals, int_vals)\n  points(peak_result$peak_mz, peak_result$peak_height, col = \"red\", pch = 19, cex = 1.5)\n  \n  cat(\"Peak Area:\", round(peak_result$area, 2), \"\\n\")\n  cat(\"Peak Height:\", round(peak_result$peak_height, 2), \"\\n\")\n  cat(\"Peak m/z:\", round(peak_result$peak_mz, 4), \"\\n\")\n}\n\n\n\n\n\n\n\nPeak Area: 0 \nPeak Height: 2371160 \nPeak m/z: 719.2207 \n\n\n\n6.3.2 Gaussian Fitting for Peak Quantification\n\nCode# Gaussian peak fitting\nfit_gaussian_peak &lt;- function(mz, intensity, peak_center) {\n  # Extract region around peak\n  peak_idx &lt;- which.min(abs(mz - peak_center))\n  \n  # Define fitting region (±20 points around peak)\n  start_idx &lt;- max(1, peak_idx - 20)\n  end_idx &lt;- min(length(mz), peak_idx + 20)\n  \n  mz_region &lt;- mz[start_idx:end_idx]\n  int_region &lt;- intensity[start_idx:end_idx]\n  \n  # Gaussian function\n  gaussian &lt;- function(x, params) {\n    A &lt;- params[1]  # Amplitude\n    mu &lt;- params[2] # Mean (center)\n    sigma &lt;- params[3] # Standard deviation\n    baseline &lt;- params[4] # Baseline\n    \n    return(baseline + A * exp(-(x - mu)^2 / (2 * sigma^2)))\n  }\n  \n  # Objective function for fitting\n  objective &lt;- function(params) {\n    predicted &lt;- gaussian(mz_region, params)\n    return(sum((int_region - predicted)^2))\n  }\n  \n  # Initial parameters\n  initial_params &lt;- c(\n    max(int_region),  # Amplitude\n    mz_region[which.max(int_region)],  # Center\n    0.05,  # Width\n    min(int_region)  # Baseline\n  )\n  \n  # Fit (simple optimization)\n  tryCatch({\n    fit_result &lt;- optim(initial_params, objective, method = \"Nelder-Mead\")\n    fitted_params &lt;- fit_result$par\n    \n    # Calculate R-squared\n    fitted_values &lt;- gaussian(mz_region, fitted_params)\n    ss_res &lt;- sum((int_region - fitted_values)^2)\n    ss_tot &lt;- sum((int_region - mean(int_region))^2)\n    r_squared &lt;- 1 - ss_res / ss_tot\n    \n    # Calculate area under Gaussian curve\n    gaussian_area &lt;- fitted_params[1] * fitted_params[3] * sqrt(2 * pi)\n    \n    return(list(\n      amplitude = fitted_params[1],\n      center = fitted_params[2],\n      sigma = fitted_params[3],\n      baseline = fitted_params[4],\n      area = gaussian_area,\n      r_squared = r_squared,\n      fitted_values = fitted_values,\n      mz_region = mz_region\n    ))\n  }, error = function(e) {\n    return(NULL)\n  })\n}\n\n# Demonstrate Gaussian fitting\nif (length(mf_result$peaks) &gt; 0) {\n  peak_mz &lt;- mz_vals[mf_result$peaks[1]]\n  gaussian_fit &lt;- fit_gaussian_peak(mz_vals, int_vals, peak_mz)\n  \n  if (!is.null(gaussian_fit)) {\n    # Get the indices for plotting\n    peak_idx &lt;- which.min(abs(mz_vals - peak_mz))\n    plot_start_idx &lt;- max(1, peak_idx - 20)\n    plot_end_idx &lt;- min(length(mz_vals), peak_idx + 20)\n    \n    # Plot fitting results\n    plot(gaussian_fit$mz_region, int_vals[plot_start_idx:plot_end_idx], \n         pch = 19, col = \"blue\",\n         main = \"Gaussian Peak Fitting\",\n         xlab = \"m/z\", ylab = \"Intensity\")\n    lines(gaussian_fit$mz_region, gaussian_fit$fitted_values, \n          col = \"red\", lwd = 2)\n    legend(\"topright\", c(\"Data\", \"Gaussian Fit\"), \n           col = c(\"blue\", \"red\"), lty = c(NA, 1), pch = c(19, NA))\n    \n    cat(\"Gaussian Fit Results:\\n\")\n    cat(\"  Center:\", round(gaussian_fit$center, 4), \"\\n\")\n    cat(\"  Area:\", round(gaussian_fit$area, 2), \"\\n\")\n    cat(\"  R-squared:\", round(gaussian_fit$r_squared, 3), \"\\n\")\n  } else {\n    cat(\"Gaussian fitting failed for the selected peak.\\n\")\n  }\n} else {\n  cat(\"No peaks detected for Gaussian fitting demonstration.\\n\")\n}\n\n\n\n\n\n\n\nGaussian Fit Results:\n  Center: 719.1114 \n  Area: -6674634 \n  R-squared: 0.994",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "05-peak-detection-quantification.html#peak-quality-assessment",
    "href": "05-peak-detection-quantification.html#peak-quality-assessment",
    "title": "\n6  Peak Detection and Quantification\n",
    "section": "\n6.4 Peak Quality Assessment",
    "text": "6.4 Peak Quality Assessment\n\n6.4.1 Signal-to-Noise Ratio Calculation\n\nCodecalculate_snr &lt;- function(mz, intensity, peak_center, noise_region_width = 2.0) {\n  peak_idx &lt;- which.min(abs(mz - peak_center))\n  peak_intensity &lt;- intensity[peak_idx]\n  \n  # Define noise region (away from peak)\n  noise_indices &lt;- which(abs(mz - peak_center) &gt; noise_region_width)\n  \n  if (length(noise_indices) &gt; 10) {\n    noise_level &lt;- sd(intensity[noise_indices])\n    snr &lt;- peak_intensity / noise_level\n  } else {\n    # Fallback: use entire spectrum for noise estimation\n    snr &lt;- peak_intensity / sd(intensity)\n  }\n  \n  return(snr)\n}\n\n# Calculate SNR for detected peaks\nsnr_values &lt;- numeric(length(mf_result$peaks))\nfor (i in seq_along(mf_result$peaks)) {\n  snr_values[i] &lt;- calculate_snr(mz_vals, int_vals, mz_vals[mf_result$peaks[i]])\n}\n\ncat(\"SNR values for detected peaks:\\n\")\n\nSNR values for detected peaks:\n\nCodeprint(round(snr_values, 2))\n\n[1] 69.69\n\n\n\n6.4.2 Peak Symmetry Assessment\n\nCodeassess_peak_symmetry &lt;- function(mz, intensity, peak_center) {\n  peak_idx &lt;- which.min(abs(mz - peak_center))\n  peak_height &lt;- intensity[peak_idx]\n  \n  # Find half-maximum points\n  half_max &lt;- peak_height / 2\n  \n  # Find left half-maximum point\n  left_idx &lt;- peak_idx\n  while (left_idx &gt; 1 && intensity[left_idx] &gt; half_max) {\n    left_idx &lt;- left_idx - 1\n  }\n  \n  # Find right half-maximum point\n  right_idx &lt;- peak_idx\n  while (right_idx &lt; length(intensity) && intensity[right_idx] &gt; half_max) {\n    right_idx &lt;- right_idx + 1\n  }\n  \n  # Calculate asymmetry factor\n  left_width &lt;- mz[peak_idx] - mz[left_idx]\n  right_width &lt;- mz[right_idx] - mz[peak_idx]\n  \n  asymmetry_factor &lt;- right_width / left_width\n  \n  return(list(\n    asymmetry_factor = asymmetry_factor,\n    fwhm = left_width + right_width,\n    left_width = left_width,\n    right_width = right_width\n  ))\n}\n\n# Assess symmetry for first peak\nif (length(mf_result$peaks) &gt; 0) {\n  symmetry_result &lt;- assess_peak_symmetry(mz_vals, int_vals, mz_vals[mf_result$peaks[1]])\n  \n  cat(\"Peak Symmetry Assessment:\\n\")\n  cat(\"  Asymmetry Factor:\", round(symmetry_result$asymmetry_factor, 3), \"\\n\")\n  cat(\"  FWHM:\", round(symmetry_result$fwhm, 4), \"\\n\")\n}\n\nPeak Symmetry Assessment:\n  Asymmetry Factor: 5.621 \n  FWHM: 22.7887",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "05-peak-detection-quantification.html#peak-list-management",
    "href": "05-peak-detection-quantification.html#peak-list-management",
    "title": "\n6  Peak Detection and Quantification\n",
    "section": "\n6.5 Peak List Management",
    "text": "6.5 Peak List Management\n\n6.5.1 Creating Comprehensive Peak Lists\n\nCodecreate_peak_list &lt;- function(mz, intensity, peak_indices) {\n  peak_data &lt;- data.frame(\n    peak_id = seq_along(peak_indices),\n    mz = mz[peak_indices],\n    intensity = intensity[peak_indices],\n    relative_intensity = intensity[peak_indices] / max(intensity) * 100\n  )\n  \n  # Add quality metrics\n  peak_data$snr &lt;- sapply(peak_indices, function(idx) {\n    calculate_snr(mz, intensity, mz[idx])\n  })\n  \n  peak_data$asymmetry &lt;- sapply(peak_indices, function(idx) {\n    sym_result &lt;- assess_peak_symmetry(mz, intensity, mz[idx])\n    return(sym_result$asymmetry_factor)\n  })\n  \n  # Add area calculations\n  peak_data$area &lt;- sapply(peak_indices, function(idx) {\n    integration_result &lt;- integrate_peak(mz, intensity, mz[idx])\n    return(integration_result$area)\n  })\n  \n  return(peak_data)\n}\n\n# Create comprehensive peak list\nif (length(mf_result$peaks) &gt; 0) {\n  comprehensive_peaks &lt;- create_peak_list(mz_vals, int_vals, mf_result$peaks)\n  print(comprehensive_peaks)\n}\n\n  peak_id       mz intensity relative_intensity      snr asymmetry area\n1       1 719.2207   2371160                100 69.68987  5.620902    0",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "05-peak-detection-quantification.html#batch-peak-processing",
    "href": "05-peak-detection-quantification.html#batch-peak-processing",
    "title": "\n6  Peak Detection and Quantification\n",
    "section": "\n6.6 Batch Peak Processing",
    "text": "6.6 Batch Peak Processing\n\n6.6.1 Processing Multiple Spectra\n\nCodeprocess_multiple_spectra &lt;- function(spectra_obj, max_spectra = 10) {\n  all_peak_lists &lt;- list()\n  \n  for (i in 1:min(length(spectra_obj), max_spectra)) {\n    # Extract spectrum data\n    mz_vals &lt;- mz(spectra_obj[i])[[1]]\n    int_vals &lt;- intensity(spectra_obj[i])[[1]]\n    \n    # Skip empty spectra\n    if (length(mz_vals) == 0 || max(int_vals) == 0) next\n    \n    # Detect peaks\n    mf_result &lt;- matched_filter_detection(mz_vals, int_vals)\n    \n    if (length(mf_result$peaks) &gt; 0) {\n      # Create peak list\n      peak_list &lt;- create_peak_list(mz_vals, int_vals, mf_result$peaks)\n      peak_list$spectrum_id &lt;- i\n      peak_list$retention_time &lt;- rtime(spectra_obj[i])\n      \n      all_peak_lists[[i]] &lt;- peak_list\n    }\n  }\n  \n  # Combine all peak lists\n  combined_peaks &lt;- do.call(rbind, all_peak_lists)\n  return(combined_peaks)\n}\n\n# Process multiple spectra\nbatch_results &lt;- process_multiple_spectra(ms_data, max_spectra = 5)\nif (!is.null(batch_results)) {\n  cat(\"Total peaks detected across spectra:\", nrow(batch_results), \"\\n\")\n  print(head(batch_results))\n}\n\nTotal peaks detected across spectra: 60 \n  peak_id       mz intensity relative_intensity       snr  asymmetry    area\n1       1 244.7057  42771.88           12.34308 0.9532201  2.5576214       0\n2       2 327.8421  92756.94           26.76772 2.0967084  0.5159425 1722408\n3       3 499.9734  48168.09           13.90031 1.0818725  0.4781007       0\n4       4 583.4235 226616.75           65.39688 5.3889942 73.3594800       0\n5       5 625.0181 148558.39           42.87086 3.4045120 15.9325747       0\n6       6 653.3221 136722.88           39.45538 3.1273898  0.3150271       0\n  spectrum_id retention_time\n1           1            100\n2           1            100\n3           1            100\n4           1            100\n5           1            100\n6           1            100",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "05-peak-detection-quantification.html#exercises",
    "href": "05-peak-detection-quantification.html#exercises",
    "title": "\n6  Peak Detection and Quantification\n",
    "section": "\n6.7 Exercises",
    "text": "6.7 Exercises\n\nImplement and compare different peak detection algorithms\nEvaluate the effect of different integration methods on quantification\nCreate quality filters based on SNR and peak symmetry\nDevelop a peak alignment algorithm for multiple spectra\nBuild a complete peak processing pipeline with quality control",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "05-peak-detection-quantification.html#summary",
    "href": "05-peak-detection-quantification.html#summary",
    "title": "\n6  Peak Detection and Quantification\n",
    "section": "\n6.8 Summary",
    "text": "6.8 Summary\nThis chapter covered advanced peak detection and quantification methods, including CWT-based detection, matched filtering, various integration approaches, and quality assessment metrics. These techniques form the foundation for reliable quantitative analysis in mass spectrometry.",
    "crumbs": [
      "Core Techniques",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Peak Detection and Quantification</span>"
    ]
  },
  {
    "objectID": "06-data-visualization.html",
    "href": "06-data-visualization.html",
    "title": "\n7  Data Visualization for Mass Spectrometry\n",
    "section": "",
    "text": "7.1 Setting Up Visualization Environment\nEffective visualization is crucial for understanding MS data, identifying patterns, and communicating results. This chapter covers comprehensive visualization techniques using the R for Mass Spectrometry infrastructure.\nCodelibrary(Spectra)            # Core MS data structures\nlibrary(QFeatures)         # Quantitative features\nlibrary(msdata)            # Example data\nlibrary(ggplot2)           # Grammar of graphics\nlibrary(plotly)            # Interactive plots  \nlibrary(dplyr)             # Data manipulation\nlibrary(viridis)           # Color scales\nlibrary(gridExtra)         # Multiple plots\nlibrary(RColorBrewer)      # Color palettes\nlibrary(patchwork)         # Plot composition\nlibrary(MsCoreUtils)       # MS utilities\nCode# Load example data with comprehensive error handling\nms_file &lt;- msdata::proteomics(full.names = TRUE)[1]\n\ntryCatch({\n  ms_data &lt;- Spectra(ms_file, backend = MsBackendMzR())\n  ms_data &lt;- setBackend(ms_data, backend = MsBackendDataFrame())\n  cat(\"Successfully loaded real MS data\\n\")\n}, error = function(e) {\n  cat(\"Note: Using synthetic data due to mzR compatibility issues\\n\")\n  cat(\"Error details:\", conditionMessage(e), \"\\n\\n\")\n  \n  # Create synthetic MS data for visualization examples\n  set.seed(789)\n  n_spectra &lt;- 100\n  \n  library(S4Vectors)\n  library(IRanges)\n  \n  # Generate peak data\n  mz_list &lt;- lapply(1:n_spectra, function(i) {\n    sort(runif(sample(60:180, 1), 100, 2000))\n  })\n  \n  intensity_list &lt;- lapply(mz_list, function(mz_vals) {\n    rlnorm(length(mz_vals), meanlog = 8, sdlog = 2)\n  })\n  \n  # Generate MS levels consistently\n  ms_levels &lt;- sample(1:2, n_spectra, replace = TRUE, prob = c(0.4, 0.6))\n  \n  # Create metadata DataFrame\n  spd &lt;- DataFrame(\n    msLevel = ms_levels,\n    rtime = seq(50, 3500, length.out = n_spectra),\n    precursorMz = ifelse(ms_levels == 1, NA_real_, runif(n_spectra, 400, 1500)),\n    precursorCharge = ifelse(ms_levels == 1, NA_integer_, sample(2:3, n_spectra, replace = TRUE)),\n    polarity = rep(1L, n_spectra),\n    collisionEnergy = ifelse(ms_levels == 1, NA_real_, runif(n_spectra, 20, 40))\n  )\n  \n  # Add peak data as list columns\n  spd$mz &lt;- NumericList(mz_list)\n  spd$intensity &lt;- NumericList(intensity_list)\n  \n  # Initialize backend and create Spectra object\n  backend &lt;- MsBackendDataFrame()\n  backend &lt;- backendInitialize(backend, spd)\n  ms_data &lt;&lt;- Spectra(backend)\n})\n\nNote: Using synthetic data due to mzR compatibility issues\nError details: BiocParallel errors\n  1 remote errors, element index: 1\n  0 unevaluated and other errors\n  first remote error:\nError in DataFrame(..., check.names = FALSE): different row counts implied by arguments\n \n\nCodecat(\"\\nDataset summary:\\n\")\n\n\nDataset summary:\n\nCodecat(\"Total spectra:\", length(ms_data), \"\\n\")\n\nTotal spectra: 100 \n\nCodecat(\"MS levels:\", paste(unique(msLevel(ms_data)), collapse = \", \"), \"\\n\")\n\nMS levels: 1, 2 \n\nCodecat(\"RT range:\", round(range(rtime(ms_data)), 2), \"seconds\\n\")\n\nRT range: 50 3500 seconds",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "06-data-visualization.html#spectral-data-exploration",
    "href": "06-data-visualization.html#spectral-data-exploration",
    "title": "\n7  Data Visualization for Mass Spectrometry\n",
    "section": "\n7.2 Spectral Data Exploration",
    "text": "7.2 Spectral Data Exploration\n\n7.2.1 Core Spectral Variables\nUnderstanding the key variables in Spectra objects is essential for effective visualization:\n\nCode# Examine core spectral variables\ncore_vars &lt;- spectraVariables(ms_data)\ncat(\"Core spectral variables:\\n\")\n\nCore spectral variables:\n\nCodeprint(core_vars)\n\n [1] \"msLevel\"                 \"rtime\"                  \n [3] \"acquisitionNum\"          \"scanIndex\"              \n [5] \"dataStorage\"             \"dataOrigin\"             \n [7] \"centroided\"              \"smoothed\"               \n [9] \"polarity\"                \"precScanNum\"            \n[11] \"precursorMz\"             \"precursorIntensity\"     \n[13] \"precursorCharge\"         \"collisionEnergy\"        \n[15] \"isolationWindowLowerMz\"  \"isolationWindowTargetMz\"\n[17] \"isolationWindowUpperMz\" \n\nCode# Display key metadata for first few spectra\nspectral_summary &lt;- data.frame(\n  msLevel = msLevel(ms_data)[1:10],\n  rtime = round(rtime(ms_data)[1:10], 2),\n  precursorMz = round(precursorMz(ms_data)[1:10], 3),\n  collisionEnergy = collisionEnergy(ms_data)[1:10],\n  polarity = polarity(ms_data)[1:10]\n)\n\nprint(head(spectral_summary))\n\n  msLevel  rtime precursorMz collisionEnergy polarity\n1       1  50.00          NA              NA        1\n2       1  84.85          NA              NA        1\n3       1 119.70          NA              NA        1\n4       2 154.55    1190.050        22.58851        1\n5       2 189.39    1338.415        37.03558        1\n6       1 224.24          NA              NA        1\n\n\n\n7.2.2 TIC and BPC Visualization\n\nCode# Calculate Total Ion Chromatogram (TIC) and Base Peak Chromatogram (BPC)\ntic_data &lt;- tic(ms_data)\nrt_data &lt;- rtime(ms_data) \n\n# Create chromatogram visualization\nchrom_data &lt;- data.frame(\n  rtime = rt_data,\n  tic = tic_data,\n  ms_level = factor(msLevel(ms_data))\n)\n\n# TIC plot colored by MS level\np_tic &lt;- ggplot(chrom_data, aes(x = rtime, y = tic, color = ms_level)) +\n  geom_line(alpha = 0.7) +\n  scale_color_viridis_d(name = \"MS Level\") +\n  labs(title = \"Total Ion Chromatogram\",\n       x = \"Retention Time (seconds)\", \n       y = \"Total Ion Current\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nprint(p_tic)",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "06-data-visualization.html#single-spectrum-visualization",
    "href": "06-data-visualization.html#single-spectrum-visualization",
    "title": "\n7  Data Visualization for Mass Spectrometry\n",
    "section": "\n7.3 Single Spectrum Visualization",
    "text": "7.3 Single Spectrum Visualization\n\n7.3.1 Enhanced Spectrum Plots\n\nCode# Select a representative MS2 spectrum\nms2_data &lt;- filterMsLevel(ms_data, 2)\nspec_idx &lt;- 10\nsingle_spec &lt;- ms2_data[spec_idx]\n\n# Extract peak data\npeak_data &lt;- peaksData(single_spec)[[1]]\nmz_vals &lt;- peak_data[, 1]\nint_vals &lt;- peak_data[, 2]\n\n# Enhanced spectrum plot with annotations\nspectrum_df &lt;- data.frame(mz = mz_vals, intensity = int_vals)\n\np_spectrum &lt;- ggplot(spectrum_df, aes(x = mz, y = intensity)) +\n  geom_segment(aes(xend = mz, yend = 0), color = \"steelblue\", alpha = 0.8) +\n  geom_point(color = \"darkblue\", size = 0.8) +\n  labs(\n    title = paste(\"MS2 Spectrum\"),\n    subtitle = paste(\"Precursor m/z:\", round(precursorMz(single_spec), 3),\n                    \"| RT:\", round(rtime(single_spec), 2), \"sec\",\n                    \"| CE:\", collisionEnergy(single_spec)),\n    x = \"m/z\", y = \"Intensity\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\nprint(p_spectrum)\n\n\n\n\n\n\n\n\n7.3.2 Interactive Spectrum Visualization\n\nCode# Create interactive spectrum plot\np_interactive &lt;- plot_ly(spectrum_df, \n                        x = ~mz, y = ~intensity, \n                        type = \"scatter\", mode = \"lines+markers\",\n                        hovertemplate = \"m/z: %{x:.4f}&lt;br&gt;Intensity: %{y:.0f}&lt;extra&gt;&lt;/extra&gt;\",\n                        line = list(color = \"steelblue\")) %&gt;%\n  layout(\n    title = \"Interactive MS2 Spectrum\",\n    xaxis = list(title = \"m/z\"),\n    yaxis = list(title = \"Intensity\"),\n    hovermode = \"closest\"\n  )\n\np_interactive",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "06-data-visualization.html#spectral-comparison-and-mirror-plots",
    "href": "06-data-visualization.html#spectral-comparison-and-mirror-plots",
    "title": "\n7  Data Visualization for Mass Spectrometry\n",
    "section": "\n7.4 Spectral Comparison and Mirror Plots",
    "text": "7.4 Spectral Comparison and Mirror Plots\n\n7.4.1 Mirror Plot Implementation\n\nCode# Create mirror plot for spectrum comparison\ncreate_mirror_plot &lt;- function(spec1, spec2, \n                               title1 = \"Spectrum 1\", \n                               title2 = \"Spectrum 2\") {\n  \n  # Extract peak data\n  peaks1 &lt;- peaksData(spec1)[[1]]\n  peaks2 &lt;- peaksData(spec2)[[1]]\n  \n  # Extract m/z and intensity\n  mz1 &lt;- peaks1[, 1]\n  int1 &lt;- peaks1[, 2]\n  mz2 &lt;- peaks2[, 1]\n  int2 &lt;- peaks2[, 2]\n  \n  # Normalize intensities to 100%\n  int1_norm &lt;- int1 / max(int1) * 100\n  int2_norm &lt;- int2 / max(int2) * -100  # Negative for mirror effect\n  \n  # Combine data\n  plot_data &lt;- rbind(\n    data.frame(mz = mz1, intensity = int1_norm, spectrum = title1),\n    data.frame(mz = mz2, intensity = int2_norm, spectrum = title2)\n  )\n  \n  ggplot(plot_data, aes(x = mz, y = intensity, color = spectrum)) +\n    geom_segment(aes(xend = mz, yend = 0), linewidth = 0.5) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    scale_color_manual(values = c(\"blue\", \"red\")) +\n    labs(title = \"Mirror Plot Comparison\",\n         x = \"m/z\", y = \"Relative Intensity (%)\",\n         color = \"Spectrum\") +\n    theme_minimal() +\n    theme(legend.position = \"top\")\n}\n\n# Create mirror plot\nif (length(ms_data) &gt; 50) {\n  mirror_plot &lt;- create_mirror_plot(ms_data[10], ms_data[50], \n                                   \"Spectrum 10\", \"Spectrum 50\")\n  print(mirror_plot)\n}",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "06-data-visualization.html#chromatographic-visualizations",
    "href": "06-data-visualization.html#chromatographic-visualizations",
    "title": "\n7  Data Visualization for Mass Spectrometry\n",
    "section": "\n7.5 Chromatographic Visualizations",
    "text": "7.5 Chromatographic Visualizations\n\n7.5.1 Total Ion Chromatogram (TIC)\n\nCode# Calculate TIC\ncalculate_tic &lt;- function(spectra_obj) {\n  tic_data &lt;- data.frame(\n    retention_time = rtime(spectra_obj),\n    tic = sapply(seq_along(spectra_obj), function(i) {\n      sum(intensity(spectra_obj[i])[[1]], na.rm = TRUE)\n    }),\n    ms_level = msLevel(spectra_obj)\n  )\n  return(tic_data)\n}\n\ntic_data &lt;- calculate_tic(ms_data)\n\n# Plot TIC\nggplot(tic_data, aes(x = retention_time, y = tic)) +\n  geom_line(color = \"darkblue\", size = 0.7) +\n  labs(title = \"Total Ion Chromatogram (TIC)\",\n       x = \"Retention Time (seconds)\", y = \"Total Ion Current\") +\n  scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n7.5.2 Base Peak Chromatogram (BPC)\n\nCode# Calculate BPC\ncalculate_bpc &lt;- function(spectra_obj) {\n  bpc_data &lt;- data.frame(\n    retention_time = rtime(spectra_obj),\n    bpc = sapply(seq_along(spectra_obj), function(i) {\n      max(intensity(spectra_obj[i])[[1]], na.rm = TRUE)\n    }),\n    ms_level = msLevel(spectra_obj)\n  )\n  return(bpc_data)\n}\n\nbpc_data &lt;- calculate_bpc(ms_data)\n\n# Combined TIC/BPC plot\ncombined_chromato &lt;- rbind(\n  data.frame(\n    retention_time = bpc_data$retention_time,\n    intensity = bpc_data$bpc,\n    ms_level = bpc_data$ms_level,\n    type = \"BPC\"\n  ),\n  data.frame(\n    retention_time = tic_data$retention_time,\n    intensity = tic_data$tic,\n    ms_level = tic_data$ms_level,\n    type = \"TIC\"\n  )\n)\n\nggplot(combined_chromato, aes(x = retention_time, y = intensity, color = type)) +\n  geom_line(linewidth = 0.7) +\n  scale_color_manual(values = c(\"BPC\" = \"red\", \"TIC\" = \"blue\")) +\n  labs(title = \"Chromatographic Profiles\",\n       x = \"Retention Time (seconds)\", y = \"Intensity\",\n       color = \"Type\") +\n  scale_y_continuous(labels = function(x) format(x, scientific = TRUE)) +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n7.5.3 Extracted Ion Chromatogram (EIC)\n\nCode# Extract ion chromatogram for specific m/z\nextract_ion_chromatogram &lt;- function(spectra_obj, target_mz, tolerance = 0.01) {\n  eic_data &lt;- data.frame(\n    retention_time = rtime(spectra_obj),\n    intensity = sapply(seq_along(spectra_obj), function(i) {\n      mz_vals &lt;- mz(spectra_obj[i])[[1]]\n      int_vals &lt;- intensity(spectra_obj[i])[[1]]\n      \n      # Find ions within tolerance\n      mz_mask &lt;- abs(mz_vals - target_mz) &lt;= tolerance\n      \n      if (any(mz_mask)) {\n        return(max(int_vals[mz_mask]))\n      } else {\n        return(0)\n      }\n    }),\n    ms_level = msLevel(spectra_obj)\n  )\n  \n  return(eic_data)\n}\n\n# Create EIC for multiple target masses\ntarget_masses &lt;- c(200, 300, 400, 500)\neic_plots &lt;- list()\n\nfor (i in seq_along(target_masses)) {\n  eic_data &lt;- extract_ion_chromatogram(ms_data, target_masses[i])\n  \n  eic_plots[[i]] &lt;- ggplot(eic_data, aes(x = retention_time, y = intensity)) +\n    geom_line(color = rainbow(length(target_masses))[i], size = 0.7) +\n    labs(title = paste(\"EIC m/z\", target_masses[i]),\n         x = \"Retention Time (s)\", y = \"Intensity\") +\n    theme_minimal() +\n    theme(plot.title = element_text(size = 10))\n}\n\n# Arrange EIC plots\ngrid.arrange(grobs = eic_plots, ncol = 2)",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "06-data-visualization.html#heat-maps-and-2d-visualizations",
    "href": "06-data-visualization.html#heat-maps-and-2d-visualizations",
    "title": "\n7  Data Visualization for Mass Spectrometry\n",
    "section": "\n7.6 Heat Maps and 2D Visualizations",
    "text": "7.6 Heat Maps and 2D Visualizations\n\n7.6.1 m/z vs Retention Time Heat Map\n\nCode# Create 2D representation of MS data\ncreate_ms_heatmap &lt;- function(spectra_obj, mz_range = c(200, 800), \n                              mz_bins = 100, rt_bins = 50) {\n  \n  # Create binning grids\n  mz_breaks &lt;- seq(mz_range[1], mz_range[2], length.out = mz_bins + 1)\n  rt_values &lt;- rtime(spectra_obj)\n  rt_breaks &lt;- seq(min(rt_values), max(rt_values), length.out = rt_bins + 1)\n  \n  # Initialize intensity matrix\n  intensity_matrix &lt;- matrix(0, nrow = rt_bins, ncol = mz_bins)\n  \n  # Fill matrix\n  for (i in seq_along(spectra_obj)) {\n    mz_vals &lt;- mz(spectra_obj[i])[[1]]\n    int_vals &lt;- intensity(spectra_obj[i])[[1]]\n    rt_val &lt;- rt_values[i]\n    \n    # Skip if outside RT range\n    rt_bin &lt;- findInterval(rt_val, rt_breaks)\n    if (rt_bin &lt; 1 || rt_bin &gt; rt_bins) next\n    \n    # Process each peak\n    for (j in seq_along(mz_vals)) {\n      if (mz_vals[j] &gt;= mz_range[1] && mz_vals[j] &lt;= mz_range[2]) {\n        mz_bin &lt;- findInterval(mz_vals[j], mz_breaks)\n        if (mz_bin &gt;= 1 && mz_bin &lt;= mz_bins) {\n          intensity_matrix[rt_bin, mz_bin] &lt;- \n            max(intensity_matrix[rt_bin, mz_bin], int_vals[j])\n        }\n      }\n    }\n  }\n  \n  # Convert to data frame for plotting\n  rt_centers &lt;- rt_breaks[-length(rt_breaks)] + diff(rt_breaks)/2\n  mz_centers &lt;- mz_breaks[-length(mz_breaks)] + diff(mz_breaks)/2\n  \n  heat_data &lt;- expand.grid(\n    retention_time = rt_centers,\n    mz = mz_centers\n  )\n  heat_data$intensity &lt;- as.vector(intensity_matrix)\n  \n  return(heat_data)\n}\n\n# Create and plot heatmap\nheat_data &lt;- create_ms_heatmap(ms_data[1:100])  # Use subset for faster processing\n\nggplot(heat_data, aes(x = retention_time, y = mz, fill = intensity)) +\n  geom_raster() +\n  scale_fill_viridis_c(trans = \"sqrt\", name = \"Intensity\") +\n  labs(title = \"MS Data Heat Map\",\n       x = \"Retention Time (seconds)\", y = \"m/z\") +\n  theme_minimal()",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "06-data-visualization.html#interactive-visualizations",
    "href": "06-data-visualization.html#interactive-visualizations",
    "title": "\n7  Data Visualization for Mass Spectrometry\n",
    "section": "\n7.7 Interactive Visualizations",
    "text": "7.7 Interactive Visualizations\n\n7.7.1 Interactive Spectrum Plot\n\nCode# Create interactive spectrum plot\ncreate_interactive_spectrum &lt;- function(spectrum_obj, title = \"Interactive Spectrum\") {\n  mz_vals &lt;- mz(spectrum_obj)[[1]]\n  int_vals &lt;- intensity(spectrum_obj)[[1]]\n  \n  plot_data &lt;- data.frame(mz = mz_vals, intensity = int_vals)\n  \n  p &lt;- ggplot(plot_data, aes(x = mz, y = intensity, \n                            text = paste(\"m/z:\", round(mz, 4), \n                                       \"&lt;br&gt;Intensity:\", round(intensity, 0)))) +\n    geom_line(color = \"blue\", linewidth = 0.5) +\n    labs(title = title, x = \"m/z\", y = \"Intensity\") +\n    theme_minimal()\n  \n  ggplotly(p, tooltip = \"text\")\n}\n\n# Create interactive plot\nif (length(ms_data) &gt; 0) {\n  interactive_plot &lt;- create_interactive_spectrum(ms_data[50])\n  interactive_plot\n}\n\n\n\n\n\n\n7.7.2 Interactive Chromatogram\n\nCode# Interactive TIC with zoom capability\ncreate_interactive_tic &lt;- function(tic_data) {\n  p &lt;- ggplot(tic_data, aes(x = retention_time, y = tic,\n                           text = paste(\"RT:\", round(retention_time, 2), \"s&lt;br&gt;\",\n                                       \"TIC:\", format(tic, scientific = TRUE)))) +\n    geom_line(color = \"darkblue\", size = 0.7) +\n    labs(title = \"Interactive Total Ion Chromatogram\",\n         x = \"Retention Time (seconds)\", y = \"Total Ion Current\") +\n    theme_minimal()\n  \n  ggplotly(p, tooltip = \"text\")\n}\n\n# Create interactive TIC\ninteractive_tic &lt;- create_interactive_tic(tic_data)\ninteractive_tic",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "06-data-visualization.html#specialized-ms-visualizations",
    "href": "06-data-visualization.html#specialized-ms-visualizations",
    "title": "\n7  Data Visualization for Mass Spectrometry\n",
    "section": "\n7.8 Specialized MS Visualizations",
    "text": "7.8 Specialized MS Visualizations\n\n7.8.1 Mass Defect Plot\n\nCode# Mass defect analysis\ncreate_mass_defect_plot &lt;- function(spectra_obj, intensity_threshold = 1000) {\n  # Extract all peaks above threshold\n  all_peaks &lt;- data.frame()\n  \n  for (i in seq_along(spectra_obj)[1:50]) {  # Use subset for demo\n    mz_vals &lt;- mz(spectra_obj[i])[[1]]\n    int_vals &lt;- intensity(spectra_obj[i])[[1]]\n    \n    # Filter by intensity threshold\n    mask &lt;- int_vals &gt; intensity_threshold\n    if (any(mask)) {\n      peaks &lt;- data.frame(\n        mz = mz_vals[mask],\n        intensity = int_vals[mask],\n        spectrum_id = i,\n        retention_time = rtime(spectra_obj[i])\n      )\n      all_peaks &lt;- rbind(all_peaks, peaks)\n    }\n  }\n  \n  if (nrow(all_peaks) &gt; 0) {\n    # Calculate mass defect\n    all_peaks$nominal_mass &lt;- floor(all_peaks$mz)\n    all_peaks$mass_defect &lt;- all_peaks$mz - all_peaks$nominal_mass\n    \n    # Plot mass defect\n    ggplot(all_peaks, aes(x = nominal_mass, y = mass_defect, \n                         color = log10(intensity))) +\n      geom_point(alpha = 0.6) +\n      scale_color_viridis_c(name = \"log10(Intensity)\") +\n      labs(title = \"Mass Defect Plot\",\n           x = \"Nominal Mass (Da)\", y = \"Mass Defect (Da)\") +\n      theme_minimal()\n  }\n}\n\n# Create mass defect plot\nmass_defect_plot &lt;- create_mass_defect_plot(ms_data)\nif (!is.null(mass_defect_plot)) {\n  print(mass_defect_plot)\n}\n\n\n\n\n\n\n\n\n7.8.2 3D Visualization\n\nCode# 3D surface plot for MS data\ncreate_3d_ms_plot &lt;- function(heat_data) {\n  # Reshape data for 3D plotting\n  rt_unique &lt;- sort(unique(heat_data$retention_time))\n  mz_unique &lt;- sort(unique(heat_data$mz))\n  \n  # Create intensity matrix\n  intensity_matrix &lt;- matrix(0, nrow = length(rt_unique), ncol = length(mz_unique))\n  \n  for (i in seq_along(rt_unique)) {\n    for (j in seq_along(mz_unique)) {\n      mask &lt;- heat_data$retention_time == rt_unique[i] & \n              heat_data$mz == mz_unique[j]\n      if (any(mask)) {\n        intensity_matrix[i, j] &lt;- heat_data$intensity[mask][1]\n      }\n    }\n  }\n  \n  # Create 3D plot\n  plot_ly(\n    x = ~mz_unique,\n    y = ~rt_unique,\n    z = ~intensity_matrix,\n    type = \"surface\",\n    colorscale = \"Viridis\"\n  ) %&gt;%\n    layout(\n      title = \"3D MS Data Visualization\",\n      scene = list(\n        xaxis = list(title = \"m/z\"),\n        yaxis = list(title = \"Retention Time (s)\"),\n        zaxis = list(title = \"Intensity\")\n      )\n    )\n}\n\n# Create 3D plot\nif (exists(\"heat_data\")) {\n  plot_3d &lt;- create_3d_ms_plot(heat_data[heat_data$intensity &gt; 0, ])\n  plot_3d\n}",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "06-data-visualization.html#quality-control-visualizations",
    "href": "06-data-visualization.html#quality-control-visualizations",
    "title": "\n7  Data Visualization for Mass Spectrometry\n",
    "section": "\n7.9 Quality Control Visualizations",
    "text": "7.9 Quality Control Visualizations\n\n7.9.1 Intensity Distribution\n\nCode# Visualize intensity distributions\nplot_intensity_distribution &lt;- function(spectra_obj) {\n  all_intensities &lt;- unlist(lapply(seq_along(spectra_obj)[1:100], function(i) {\n    intensity(spectra_obj[i])[[1]]\n  }))\n  \n  # Remove zeros for log scale\n  all_intensities &lt;- all_intensities[all_intensities &gt; 0]\n  \n  ggplot(data.frame(intensity = all_intensities), aes(x = intensity)) +\n    geom_histogram(bins = 50, fill = \"skyblue\", alpha = 0.7) +\n    scale_x_log10() +\n    labs(title = \"Intensity Distribution (Log Scale)\",\n         x = \"Intensity (log10)\", y = \"Frequency\") +\n    theme_minimal()\n}\n\nintensity_dist_plot &lt;- plot_intensity_distribution(ms_data)\nprint(intensity_dist_plot)\n\n\n\n\n\n\n\n\n7.9.2 MS Level Distribution\n\nCode# Visualize MS level distribution over time\nms_level_data &lt;- data.frame(\n  retention_time = rtime(ms_data),\n  ms_level = factor(msLevel(ms_data))\n)\n\nggplot(ms_level_data, aes(x = retention_time, y = ms_level, color = ms_level)) +\n  geom_point(alpha = 0.6, size = 0.5) +\n  labs(title = \"MS Level Distribution Over Time\",\n       x = \"Retention Time (seconds)\", y = \"MS Level\",\n       color = \"MS Level\") +\n  theme_minimal()",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "06-data-visualization.html#exercises",
    "href": "06-data-visualization.html#exercises",
    "title": "\n7  Data Visualization for Mass Spectrometry\n",
    "section": "\n7.10 Exercises",
    "text": "7.10 Exercises\n\nCreate a function to generate spectral annotations with peak labels\nDevelop a multi-panel visualization showing TIC, BPC, and selected EICs\nImplement a peak picking visualization with adjustable thresholds\nCreate an interactive dashboard for MS data exploration\nDesign visualization templates for different types of MS experiments",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "06-data-visualization.html#summary",
    "href": "06-data-visualization.html#summary",
    "title": "\n7  Data Visualization for Mass Spectrometry\n",
    "section": "\n7.11 Summary",
    "text": "7.11 Summary\nThis chapter covered comprehensive visualization techniques for MS data, from basic spectral plots to advanced interactive visualizations. Effective visualization is essential for data exploration, quality assessment, and result communication in mass spectrometry analysis.",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Visualization for Mass Spectrometry</span>"
    ]
  },
  {
    "objectID": "07-statistical-analysis.html",
    "href": "07-statistical-analysis.html",
    "title": "\n8  Statistical Analysis of MS Data\n",
    "section": "",
    "text": "8.1 Setting Up the Statistical Environment\nStatistical analysis is fundamental to extracting meaningful biological insights from mass spectrometry data. This chapter covers statistical methods integrated with the R for Mass Spectrometry ecosystem, including univariate and multivariate approaches.\nCodelibrary(Spectra)           # Core MS data structures\nlibrary(QFeatures)         # Quantitative features\nlibrary(msdata)            # Example datasets\nlibrary(tidyverse)         # Data manipulation and visualization\nlibrary(broom)             # Tidy model outputs\nlibrary(limma)             # Linear models for omics\nlibrary(corrplot)          # Correlation plots\nlibrary(cluster)           # Clustering methods\nlibrary(factoextra)        # Enhanced PCA visualization\nlibrary(pheatmap)          # Heatmaps\nlibrary(ggrepel)           # Label repulsion\nlibrary(patchwork)         # Plot composition\nCode# Create a realistic experimental dataset for statistical analysis\nset.seed(123)\n\n# Experiment design: 2 conditions, 3 time points, 5 replicates\nn_conditions &lt;- 2\nn_timepoints &lt;- 3\nn_replicates &lt;- 5\nn_samples &lt;- n_conditions * n_timepoints * n_replicates\nn_features &lt;- 100\n\n# Sample metadata\nexperimental_data &lt;- expand.grid(\n  condition = c(\"Control\", \"Treatment\"),\n  timepoint = c(\"T0\", \"T1\", \"T2\"),\n  replicate = 1:n_replicates\n) %&gt;%\n  mutate(\n    sample_id = paste0(\"S\", 1:n()),\n    batch = rep(1:3, length.out = n())\n  )\n\n# Simulate feature intensities with biological effects\nfeature_matrix &lt;- matrix(\n  rlnorm(n_samples * n_features, meanlog = 10, sdlog = 0.8),\n  nrow = n_samples,\n  ncol = n_features\n)\n\n# Add treatment effects to specific features\ntreatment_idx &lt;- experimental_data$condition == \"Treatment\"\ntime_effect &lt;- as.numeric(factor(experimental_data$timepoint)) - 1\n\n# Features 1-20: Treatment effect\nfor (i in 1:20) {\n  effect_size &lt;- runif(1, 0.3, 0.8)\n  feature_matrix[treatment_idx, i] &lt;- feature_matrix[treatment_idx, i] * \n    exp(effect_size)\n}\n\n# Features 21-40: Time effect\nfor (i in 21:40) {\n  time_coef &lt;- runif(1, 0.1, 0.3)\n  feature_matrix[, i] &lt;- feature_matrix[, i] * exp(time_coef * time_effect)\n}\n\n# Features 41-60: Interaction effect\nfor (i in 41:60) {\n  interaction_coef &lt;- runif(1, 0.2, 0.5)\n  feature_matrix[treatment_idx, i] &lt;- feature_matrix[treatment_idx, i] * \n    exp(interaction_coef * time_effect[treatment_idx])\n}\n\ncolnames(feature_matrix) &lt;- paste0(\"Feature_\", 1:n_features)\nrownames(feature_matrix) &lt;- experimental_data$sample_id\n\ncat(\"Dataset created:\\n\")\n\nDataset created:\n\nCodecat(\"  Samples:\", n_samples, \"\\n\")\n\n  Samples: 30 \n\nCodecat(\"  Features:\", n_features, \"\\n\")\n\n  Features: 100 \n\nCodecat(\"  Design: 2 conditions × 3 timepoints × 5 replicates\\n\")\n\n  Design: 2 conditions × 3 timepoints × 5 replicates",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "07-statistical-analysis.html#descriptive-statistics",
    "href": "07-statistical-analysis.html#descriptive-statistics",
    "title": "\n8  Statistical Analysis of MS Data\n",
    "section": "\n8.2 Descriptive Statistics",
    "text": "8.2 Descriptive Statistics\n\n8.2.1 Basic Summary Statistics\n\nCode# Calculate summary statistics for features\nsummary_stats &lt;- data.frame(\n  feature = colnames(feature_matrix),\n  mean = apply(feature_matrix, 2, mean),\n  median = apply(feature_matrix, 2, median),\n  sd = apply(feature_matrix, 2, sd),\n  cv = apply(feature_matrix, 2, function(x) sd(x) / mean(x) * 100),  # Coefficient of variation\n  min = apply(feature_matrix, 2, min),\n  max = apply(feature_matrix, 2, max)\n)\n\n# Display first few features\nhead(summary_stats, 10)\n\n              feature     mean   median        sd        cv       min      max\nFeature_1   Feature_1 38539.95 30587.06  36945.31  95.86238  7683.549 154764.7\nFeature_2   Feature_2 43532.54 32455.23  40455.91  92.93257  6380.475 205635.6\nFeature_3   Feature_3 37879.54 27653.87  34593.29  91.32447  5644.510 184580.6\nFeature_4   Feature_4 36481.56 23965.46  31656.02  86.77265  6037.189 131991.5\nFeature_5   Feature_5 33341.32 23482.69  30827.79  92.46123  4261.610 118194.5\nFeature_6   Feature_6 57532.78 31559.10 104430.56 181.51490  8037.520 568784.5\nFeature_7   Feature_7 38466.44 23346.28  37830.52  98.34684  7718.372 165906.0\nFeature_8   Feature_8 38978.13 25492.49  34694.03  89.00897  8192.558 173304.9\nFeature_9   Feature_9 50905.26 27036.39  52624.87 103.37805 11720.590 224033.0\nFeature_10 Feature_10 42747.82 33157.06  28850.95  67.49104  5802.359 116445.9\n\n\n\n8.2.2 Distribution Analysis\n\nCode# Analyze distribution of coefficient of variation\nggplot(summary_stats, aes(x = cv)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = median(summary_stats$cv), \n             color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(title = \"Distribution of Coefficient of Variation\",\n       subtitle = paste(\"Median CV =\", round(median(summary_stats$cv), 2), \"%\"),\n       x = \"Coefficient of Variation (%)\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n8.2.3 Missing Value Analysis\n\nCode# Simulate some missing values\nfeature_matrix_with_na &lt;- feature_matrix\nmissing_indices &lt;- sample(length(feature_matrix), size = length(feature_matrix) * 0.05)\nfeature_matrix_with_na[missing_indices] &lt;- NA\n\n# Calculate missing value statistics\nmissing_stats &lt;- data.frame(\n  feature = colnames(feature_matrix_with_na),\n  missing_count = apply(feature_matrix_with_na, 2, function(x) sum(is.na(x))),\n  missing_percent = apply(feature_matrix_with_na, 2, function(x) sum(is.na(x)) / length(x) * 100)\n)\n\n# Visualize missing value patterns\nmissing_pattern &lt;- missing_stats %&gt;%\n  filter(missing_count &gt; 0) %&gt;%\n  head(20)\n\nif (nrow(missing_pattern) &gt; 0) {\n  ggplot(missing_pattern, aes(x = reorder(feature, missing_percent), y = missing_percent)) +\n    geom_bar(stat = \"identity\", fill = \"coral\") +\n    coord_flip() +\n    labs(title = \"Missing Value Patterns\",\n         x = \"Feature\", y = \"Missing Percentage (%)\") +\n    theme_minimal()\n}",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "07-statistical-analysis.html#hypothesis-testing",
    "href": "07-statistical-analysis.html#hypothesis-testing",
    "title": "\n8  Statistical Analysis of MS Data\n",
    "section": "\n8.3 Hypothesis Testing",
    "text": "8.3 Hypothesis Testing\n\n8.3.1 Two-Sample t-tests\n\nCode# Perform t-tests for each feature comparing conditions\nperform_ttest &lt;- function(feature_data, groups) {\n  control_data &lt;- feature_data[groups == \"Control\"]\n  treatment_data &lt;- feature_data[groups == \"Treatment\"]\n  \n  # Check for sufficient data\n  if (length(control_data) &lt; 3 || length(treatment_data) &lt; 3) {\n    return(data.frame(p.value = NA, statistic = NA, estimate = NA))\n  }\n  \n  # Perform t-test\n  test_result &lt;- t.test(control_data, treatment_data)\n  \n  return(data.frame(\n    p.value = test_result$p.value,\n    statistic = test_result$statistic,\n    estimate_diff = test_result$estimate[2] - test_result$estimate[1]\n  ))\n}\n\n# Apply t-tests to all features\nttest_results &lt;- data.frame()\nfor (i in 1:ncol(feature_matrix)) {\n  result &lt;- perform_ttest(feature_matrix[, i], experimental_data$condition)\n  result$feature &lt;- colnames(feature_matrix)[i]\n  ttest_results &lt;- rbind(ttest_results, result)\n}\n\n# Add multiple testing correction\nttest_results$p.adjusted &lt;- p.adjust(ttest_results$p.value, method = \"fdr\")\n\n# Display significant results\nsignificant_features &lt;- ttest_results[ttest_results$p.adjusted &lt; 0.05 & !is.na(ttest_results$p.adjusted), ]\ncat(\"Number of significant features (FDR &lt; 0.05):\", nrow(significant_features), \"\\n\")\n\nNumber of significant features (FDR &lt; 0.05): 0 \n\nCodehead(significant_features)\n\n[1] p.value       statistic     estimate_diff feature       p.adjusted   \n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n8.3.2 Volcano Plot\n\nCode# Create volcano plot\nvolcano_data &lt;- ttest_results %&gt;%\n  mutate(\n    log2_fold_change = log2(abs(estimate_diff) + 1),  # Add 1 to avoid log(0)\n    neg_log10_p = -log10(p.value),\n    significant = p.adjusted &lt; 0.05 & !is.na(p.adjusted)\n  )\n\nggplot(volcano_data, aes(x = log2_fold_change, y = neg_log10_p)) +\n  geom_point(aes(color = significant), alpha = 0.7) +\n  scale_color_manual(values = c(\"FALSE\" = \"gray\", \"TRUE\" = \"red\")) +\n  geom_hline(yintercept = -log10(0.05), linetype = \"dashed\", color = \"blue\") +\n  labs(title = \"Volcano Plot\",\n       x = \"Log2 Fold Change\", y = \"-Log10 P-value\",\n       color = \"Significant\") +\n  theme_minimal()",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "07-statistical-analysis.html#anova-for-multiple-groups",
    "href": "07-statistical-analysis.html#anova-for-multiple-groups",
    "title": "\n8  Statistical Analysis of MS Data\n",
    "section": "\n8.4 ANOVA for Multiple Groups",
    "text": "8.4 ANOVA for Multiple Groups\n\nCode# Add a third condition for ANOVA demonstration\nexperimental_data_extended &lt;- rbind(\n  experimental_data,\n  data.frame(\n    condition = rep(\"Treatment2\", 10),\n    timepoint = rep(\"T0\", 10),\n    replicate = rep(1:5, 2),\n    sample_id = paste0(\"S\", (nrow(experimental_data) + 1):(nrow(experimental_data) + 10)),\n    batch = rep(1:2, each = 5)\n  )\n)\n\n# Extend feature matrix\nadditional_samples &lt;- matrix(\n  rlnorm(10 * n_features, meanlog = 10.2, sdlog = 1),\n  nrow = 10,\n  ncol = n_features\n)\ncolnames(additional_samples) &lt;- colnames(feature_matrix)\nrownames(additional_samples) &lt;- experimental_data_extended$sample_id[31:40]\n\nfeature_matrix_extended &lt;- rbind(feature_matrix, additional_samples)\n\n# Perform one-way ANOVA for each feature\nperform_anova &lt;- function(feature_data, groups) {\n  if (length(unique(groups)) &lt; 2) return(data.frame(p.value = NA, f.statistic = NA))\n  \n  anova_result &lt;- aov(feature_data ~ groups)\n  summary_result &lt;- summary(anova_result)\n  \n  return(data.frame(\n    p.value = summary_result[[1]][1, \"Pr(&gt;F)\"],\n    f.statistic = summary_result[[1]][1, \"F value\"]\n  ))\n}\n\n# Apply ANOVA to all features\nanova_results &lt;- lapply(1:ncol(feature_matrix_extended), function(i) {\n  result &lt;- perform_anova(feature_matrix_extended[, i], experimental_data_extended$condition)\n  result$feature &lt;- colnames(feature_matrix_extended)[i]\n  return(result)\n})\nanova_results &lt;- do.call(rbind, anova_results)\n\n# Add multiple testing correction\nanova_results$p.adjusted &lt;- p.adjust(anova_results$p.value, method = \"fdr\")\n\n# Display significant ANOVA results\nsignificant_anova &lt;- anova_results[anova_results$p.adjusted &lt; 0.05 & !is.na(anova_results$p.adjusted), ]\ncat(\"Number of significant features (ANOVA FDR &lt; 0.05):\", nrow(significant_anova), \"\\n\")\n\nNumber of significant features (ANOVA FDR &lt; 0.05): 0",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "07-statistical-analysis.html#correlation-analysis",
    "href": "07-statistical-analysis.html#correlation-analysis",
    "title": "\n8  Statistical Analysis of MS Data\n",
    "section": "\n8.5 Correlation Analysis",
    "text": "8.5 Correlation Analysis\n\n8.5.1 Feature-Feature Correlations\n\nCode# Calculate correlation matrix for a subset of features\nfeature_subset &lt;- feature_matrix[, 1:20]  # Use subset for visualization\ncor_matrix &lt;- cor(feature_subset, use = \"complete.obs\")\n\n# Visualize correlation matrix\ncorrplot(cor_matrix, method = \"circle\", type = \"upper\", \n         order = \"hclust\", tl.cex = 0.8, tl.col = \"black\")\ntitle(\"Feature-Feature Correlation Matrix\")\n\n\n\n\n\n\n\n\n8.5.2 Correlation with Experimental Factors\n\nCode# Encode experimental factors as numeric for correlation\nexperimental_numeric &lt;- experimental_data %&gt;%\n  mutate(\n    condition_numeric = ifelse(condition == \"Control\", 0, 1),\n    batch_numeric = as.numeric(batch)\n  )\n\n# Calculate correlations between features and experimental factors\ncor_with_condition &lt;- apply(feature_matrix, 2, function(x) {\n  cor(x, experimental_numeric$condition_numeric, use = \"complete.obs\")\n})\n\ncor_with_batch &lt;- apply(feature_matrix, 2, function(x) {\n  cor(x, experimental_numeric$batch_numeric, use = \"complete.obs\")\n})\n\n# Visualize correlations\ncorrelation_df &lt;- data.frame(\n  feature = names(cor_with_condition),\n  condition_cor = cor_with_condition,\n  batch_cor = cor_with_batch\n)\n\nggplot(correlation_df, aes(x = condition_cor, y = batch_cor)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  labs(title = \"Feature Correlations with Experimental Factors\",\n       x = \"Correlation with Treatment\", y = \"Correlation with Batch\") +\n  theme_minimal()",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "07-statistical-analysis.html#principal-component-analysis-pca",
    "href": "07-statistical-analysis.html#principal-component-analysis-pca",
    "title": "\n8  Statistical Analysis of MS Data\n",
    "section": "\n8.6 Principal Component Analysis (PCA)",
    "text": "8.6 Principal Component Analysis (PCA)\n\n8.6.1 Performing PCA\n\nCode# Standardize data for PCA\nfeature_matrix_scaled &lt;- scale(feature_matrix)\n\n# Perform PCA\npca_result &lt;- prcomp(feature_matrix_scaled, center = FALSE, scale. = FALSE)\n\n# Extract PC scores\npca_scores &lt;- data.frame(pca_result$x) %&gt;%\n  mutate(\n    sample_id = experimental_data$sample_id,\n    condition = experimental_data$condition,\n    batch = factor(experimental_data$batch)\n  )\n\n# Variance explained\nvariance_explained &lt;- (pca_result$sdev^2) / sum(pca_result$sdev^2) * 100\n\n\n\n8.6.2 PCA Visualization\n\nCode# PCA scores plot\nggplot(pca_scores, aes(x = PC1, y = PC2, color = condition, shape = batch)) +\n  geom_point(size = 3, alpha = 0.8) +\n  stat_ellipse(aes(group = condition), alpha = 0.3) +\n  labs(title = \"PCA Scores Plot\",\n       x = paste0(\"PC1 (\", round(variance_explained[1], 1), \"%)\"),\n       y = paste0(\"PC2 (\", round(variance_explained[2], 1), \"%)\"),\n       color = \"Condition\", shape = \"Batch\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n8.6.3 Scree Plot\n\nCode# Scree plot\nscree_data &lt;- data.frame(\n  PC = 1:min(10, length(variance_explained)),\n  Variance = variance_explained[1:min(10, length(variance_explained))]\n)\n\nggplot(scree_data, aes(x = PC, y = Variance)) +\n  geom_line(color = \"blue\", size = 1) +\n  geom_point(color = \"red\", size = 3) +\n  labs(title = \"Scree Plot\",\n       x = \"Principal Component\", y = \"Variance Explained (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n8.6.4 PCA Loadings\n\nCode# Extract and visualize loadings\nloadings_data &lt;- data.frame(\n  feature = colnames(feature_matrix),\n  PC1 = pca_result$rotation[, 1],\n  PC2 = pca_result$rotation[, 2]\n)\n\n# Plot loadings\nggplot(loadings_data, aes(x = PC1, y = PC2)) +\n  geom_point(alpha = 0.6) +\n  geom_text(aes(label = feature), size = 2, check_overlap = TRUE) +\n  labs(title = \"PCA Loadings Plot\",\n       x = \"PC1\", y = \"PC2\") +\n  theme_minimal()",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "07-statistical-analysis.html#clustering-analysis",
    "href": "07-statistical-analysis.html#clustering-analysis",
    "title": "\n8  Statistical Analysis of MS Data\n",
    "section": "\n8.7 Clustering Analysis",
    "text": "8.7 Clustering Analysis\n\n8.7.1 Hierarchical Clustering\n\nCode# Perform hierarchical clustering\ndist_matrix &lt;- dist(feature_matrix_scaled)\nhclust_result &lt;- hclust(dist_matrix, method = \"ward.D2\")\n\n# Cut tree to get clusters\nn_clusters &lt;- 3\ncluster_assignments &lt;- cutree(hclust_result, k = n_clusters)\n\n# Add cluster assignments to experimental data\nexperimental_data$cluster &lt;- factor(cluster_assignments)\n\n# Visualize dendrogram\nfviz_dend(hclust_result, k = n_clusters, \n          cex = 0.8,\n          color_labels_by_k = TRUE,\n          main = \"Hierarchical Clustering Dendrogram\")\n\n\n\n\n\n\n\n\n8.7.2 K-means Clustering\n\nCode# Perform k-means clustering\nset.seed(123)\nkmeans_result &lt;- kmeans(feature_matrix_scaled, centers = 3, nstart = 25)\n\n# Add k-means clusters to data\nexperimental_data$kmeans_cluster &lt;- factor(kmeans_result$cluster)\n\n# Visualize clusters in PCA space\nggplot(pca_scores, aes(x = PC1, y = PC2)) +\n  geom_point(aes(color = experimental_data$kmeans_cluster), size = 3, alpha = 0.8) +\n  labs(title = \"K-means Clustering in PCA Space\",\n       x = paste0(\"PC1 (\", round(variance_explained[1], 1), \"%)\"),\n       y = paste0(\"PC2 (\", round(variance_explained[2], 1), \"%)\"),\n       color = \"K-means Cluster\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n8.7.3 Cluster Validation\n\nCode# Silhouette analysis\nsil_scores &lt;- silhouette(kmeans_result$cluster, dist_matrix)\n\n# Visualize silhouette plot\nfviz_silhouette(sil_scores, main = \"Silhouette Plot for K-means Clustering\")\n\n  cluster size ave.sil.width\n1       1   22          0.10\n2       2    7         -0.03\n3       3    1          0.00\n\n\n\n\n\n\n\nCode# Average silhouette width\navg_sil_width &lt;- mean(sil_scores[, 3])\ncat(\"Average silhouette width:\", round(avg_sil_width, 3), \"\\n\")\n\nAverage silhouette width: 0.071",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "07-statistical-analysis.html#heat-map-analysis",
    "href": "07-statistical-analysis.html#heat-map-analysis",
    "title": "\n8  Statistical Analysis of MS Data\n",
    "section": "\n8.8 Heat Map Analysis",
    "text": "8.8 Heat Map Analysis\n\n8.8.1 Feature Heat Map\n\nCode# Create heat map of top variable features\ntop_variable_features &lt;- summary_stats %&gt;%\n  top_n(30, cv) %&gt;%\n  pull(feature)\n\nheatmap_data &lt;- feature_matrix[, top_variable_features]\n\n# Create annotation for samples\nannotation_df &lt;- experimental_data %&gt;%\n  select(condition, batch) %&gt;%\n  data.frame(row.names = experimental_data$sample_id)\n\n# Generate heat map\npheatmap(t(scale(heatmap_data)), \n         annotation_col = annotation_df,\n         show_rownames = FALSE,\n         show_colnames = TRUE,\n         clustering_distance_rows = \"euclidean\",\n         clustering_distance_cols = \"euclidean\",\n         main = \"Heat Map of Top Variable Features\")",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "07-statistical-analysis.html#power-analysis",
    "href": "07-statistical-analysis.html#power-analysis",
    "title": "\n8  Statistical Analysis of MS Data\n",
    "section": "\n8.9 Power Analysis",
    "text": "8.9 Power Analysis\n\n8.9.1 Sample Size Calculation\n\nCode# Function for power analysis\ncalculate_power &lt;- function(effect_size, sample_size_per_group, alpha = 0.05) {\n  # Calculate power for two-sample t-test\n  delta &lt;- effect_size\n  n &lt;- sample_size_per_group\n  \n  # Non-centrality parameter\n  ncp &lt;- delta * sqrt(n/2)\n  \n  # Critical value\n  t_crit &lt;- qt(1 - alpha/2, df = 2*n - 2)\n  \n  # Power calculation\n  power &lt;- 1 - pt(t_crit, df = 2*n - 2, ncp = ncp) + \n           pt(-t_crit, df = 2*n - 2, ncp = ncp)\n  \n  return(power)\n}\n\n# Power analysis for different effect sizes and sample sizes\neffect_sizes &lt;- seq(0.2, 2.0, by = 0.2)\nsample_sizes &lt;- seq(5, 30, by = 5)\n\npower_results &lt;- expand.grid(\n  effect_size = effect_sizes,\n  sample_size = sample_sizes\n) %&gt;%\n  rowwise() %&gt;%\n  mutate(power = calculate_power(effect_size, sample_size))\n\n# Visualize power analysis\nggplot(power_results, aes(x = sample_size, y = power, color = factor(effect_size))) +\n  geom_line(size = 1) +\n  geom_hline(yintercept = 0.8, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Power Analysis for Two-Sample t-test\",\n       x = \"Sample Size per Group\", y = \"Statistical Power\",\n       color = \"Effect Size\") +\n  theme_minimal()",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "07-statistical-analysis.html#exercises",
    "href": "07-statistical-analysis.html#exercises",
    "title": "\n8  Statistical Analysis of MS Data\n",
    "section": "\n8.10 Exercises",
    "text": "8.10 Exercises\n\nPerform statistical analysis on your own MS dataset\nImplement different multiple testing correction methods and compare results\nConduct time-series analysis for longitudinal MS data\nApply machine learning classification to distinguish sample groups\nDevelop quality control metrics based on statistical properties",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "07-statistical-analysis.html#summary",
    "href": "07-statistical-analysis.html#summary",
    "title": "\n8  Statistical Analysis of MS Data\n",
    "section": "\n8.11 Summary",
    "text": "8.11 Summary\nThis chapter covered essential statistical methods for MS data analysis, including descriptive statistics, hypothesis testing, multivariate analysis, and clustering. These statistical tools are fundamental for extracting meaningful biological insights from mass spectrometry experiments.",
    "crumbs": [
      "Analysis & Visualization",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistical Analysis of MS Data</span>"
    ]
  },
  {
    "objectID": "08-metabolomics-analysis.html",
    "href": "08-metabolomics-analysis.html",
    "title": "\n9  Metabolomics Data Analysis\n",
    "section": "",
    "text": "9.1 Setting Up Metabolomics Environment\nMetabolomics involves the comprehensive analysis of small molecules (metabolites) in biological systems. This chapter covers LC-MS metabolomics data processing using the R for Mass Spectrometry ecosystem, with emphasis on the xcms package and Spectra integration.\nCode# Load required packages (with conditional loading for Bioconductor packages)\nif (requireNamespace(\"xcms\", quietly = TRUE)) {\n  library(xcms)              # LC-MS data processing\n} else {\n  message(\"Note: xcms package not installed. Install with: BiocManager::install('xcms')\")\n}\n\nif (requireNamespace(\"Spectra\", quietly = TRUE)) {\n  library(Spectra)           # Core MS data structures\n} else {\n  message(\"Note: Spectra package not installed. Install with: BiocManager::install('Spectra')\")\n}\n\nif (requireNamespace(\"MetaboCoreUtils\", quietly = TRUE)) {\n  library(MetaboCoreUtils)   # Metabolomics utilities\n}\n\nif (requireNamespace(\"MsCoreUtils\", quietly = TRUE)) {\n  library(MsCoreUtils)       # MS data utilities\n}\n\n# Standard CRAN packages\nlibrary(ggplot2)           # Visualization\nlibrary(dplyr)             # Data manipulation\nlibrary(tidyr)             # Data tidying\n\n# Optional packages\nif (requireNamespace(\"pheatmap\", quietly = TRUE)) library(pheatmap)\nif (requireNamespace(\"patchwork\", quietly = TRUE)) library(patchwork)\nif (requireNamespace(\"CAMERA\", quietly = TRUE)) library(CAMERA)\nif (requireNamespace(\"CompoundDb\", quietly = TRUE)) library(CompoundDb)\nif (requireNamespace(\"mixOmics\", quietly = TRUE)) library(mixOmics)",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-metabolomics-analysis.html#understanding-metabolomics-workflows",
    "href": "08-metabolomics-analysis.html#understanding-metabolomics-workflows",
    "title": "\n9  Metabolomics Data Analysis\n",
    "section": "\n9.2 Understanding Metabolomics Workflows",
    "text": "9.2 Understanding Metabolomics Workflows\n\n9.2.1 LC-MS Metabolomics Pipeline\nA typical untargeted metabolomics workflow consists of:\n\n\nPeak detection: Identifying features (m/z-RT pairs) across samples\n\nAlignment: Correcting RT variations between samples\n\nCorrespondence: Matching features across samples\n\nGap filling: Integrating missing peaks\n\nAnnotation: Identifying metabolites\n\nStatistical analysis: Finding differential metabolites\n\n\nCodeflowchart TD\n    subgraph Input[\"Raw LC-MS Data\"]\n        A[Multiple mzML Files&lt;br/&gt;Control & Treatment Samples]\n    end\n    \n    subgraph XCMS[\"xcms Processing Pipeline\"]\n        B[1. Read Data&lt;br/&gt;readMSData] --&gt; C[2. Peak Detection&lt;br/&gt;findChromPeaks&lt;br/&gt;CentWave Algorithm]\n        C --&gt; D[3. RT Alignment&lt;br/&gt;adjustRtime&lt;br/&gt;obiwarp/peakGroups]\n        D --&gt; E[4. Correspondence&lt;br/&gt;groupChromPeaks&lt;br/&gt;PeakDensity]\n        E --&gt; F[5. Gap Filling&lt;br/&gt;fillChromPeaks&lt;br/&gt;ChromPeakArea]\n    end\n    \n    subgraph Annotation[\"Feature Annotation\"]\n        F --&gt; G[CAMERA&lt;br/&gt;Adducts & Isotopes]\n        G --&gt; H[CompoundDb&lt;br/&gt;Database Matching]\n        H --&gt; I[MetaboCoreUtils&lt;br/&gt;Mass Calculations]\n    end\n    \n    subgraph Output[\"Feature Matrix\"]\n        I --&gt; J[Features × Samples&lt;br/&gt;Intensity Matrix]\n    end\n    \n    subgraph Stats[\"Statistical Analysis\"]\n        J --&gt; K[Quality Control&lt;br/&gt;CV, Missing Values]\n        K --&gt; L[Normalization&lt;br/&gt;TIC/IS/QC-based]\n        L --&gt; M[Multivariate&lt;br/&gt;PCA, PLS-DA]\n        M --&gt; N[Univariate&lt;br/&gt;t-test, ANOVA]\n        N --&gt; O[Pathway Analysis&lt;br/&gt;Enrichment]\n    end\n    \n    A --&gt; B\n    \n  style Input fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style XCMS fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n  style Annotation fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style Output fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n  style Stats fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n\n\n\n\nflowchart TD\n    subgraph Input[\"Raw LC-MS Data\"]\n        A[Multiple mzML Files&lt;br/&gt;Control & Treatment Samples]\n    end\n    \n    subgraph XCMS[\"xcms Processing Pipeline\"]\n        B[1. Read Data&lt;br/&gt;readMSData] --&gt; C[2. Peak Detection&lt;br/&gt;findChromPeaks&lt;br/&gt;CentWave Algorithm]\n        C --&gt; D[3. RT Alignment&lt;br/&gt;adjustRtime&lt;br/&gt;obiwarp/peakGroups]\n        D --&gt; E[4. Correspondence&lt;br/&gt;groupChromPeaks&lt;br/&gt;PeakDensity]\n        E --&gt; F[5. Gap Filling&lt;br/&gt;fillChromPeaks&lt;br/&gt;ChromPeakArea]\n    end\n    \n    subgraph Annotation[\"Feature Annotation\"]\n        F --&gt; G[CAMERA&lt;br/&gt;Adducts & Isotopes]\n        G --&gt; H[CompoundDb&lt;br/&gt;Database Matching]\n        H --&gt; I[MetaboCoreUtils&lt;br/&gt;Mass Calculations]\n    end\n    \n    subgraph Output[\"Feature Matrix\"]\n        I --&gt; J[Features × Samples&lt;br/&gt;Intensity Matrix]\n    end\n    \n    subgraph Stats[\"Statistical Analysis\"]\n        J --&gt; K[Quality Control&lt;br/&gt;CV, Missing Values]\n        K --&gt; L[Normalization&lt;br/&gt;TIC/IS/QC-based]\n        L --&gt; M[Multivariate&lt;br/&gt;PCA, PLS-DA]\n        M --&gt; N[Univariate&lt;br/&gt;t-test, ANOVA]\n        N --&gt; O[Pathway Analysis&lt;br/&gt;Enrichment]\n    end\n    \n    A --&gt; B\n    \n  style Input fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style XCMS fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n  style Annotation fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style Output fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n  style Stats fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n\n\n\n\n\n\n\n\n\n\n\n\nXCMS Workflow Key Parameters\n\n\n\n\n\nCentWave: ppm tolerance (5-25), peakwidth (5-30 sec), snthresh (3-10)\n\nAlignment: bw (bandwidth 2-10), minfrac (0.5-0.9)\n\nCorrespondence: bw (1-5), minfrac (0.3-0.7)\n\nGap Filling: expandMz/Rt for integration windows\n\n\n\n\nCode# Display workflow overview\nworkflow_steps &lt;- data.frame(\n  Step = 1:6,\n  Process = c(\"Peak Detection\", \"Retention Time Correction\", \n              \"Correspondence\", \"Gap Filling\", \n              \"Annotation\", \"Statistical Analysis\"),\n  Package = c(\"xcms\", \"xcms\", \"xcms\", \"xcms\", \n              \"CAMERA/CompoundDb\", \"limma/mixOmics\"),\n  Output = c(\"Chromatographic peaks\", \"Aligned peaks\",\n             \"Feature matrix\", \"Complete matrix\",\n             \"Putative IDs\", \"Differential metabolites\")\n)\n\nprint(workflow_steps)\n\n  Step                   Process           Package                   Output\n1    1            Peak Detection              xcms    Chromatographic peaks\n2    2 Retention Time Correction              xcms            Aligned peaks\n3    3            Correspondence              xcms           Feature matrix\n4    4               Gap Filling              xcms          Complete matrix\n5    5                Annotation CAMERA/CompoundDb             Putative IDs\n6    6      Statistical Analysis    limma/mixOmics Differential metabolites",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-metabolomics-analysis.html#xcms-based-peak-detection",
    "href": "08-metabolomics-analysis.html#xcms-based-peak-detection",
    "title": "\n9  Metabolomics Data Analysis\n",
    "section": "\n9.3 XCMS-Based Peak Detection",
    "text": "9.3 XCMS-Based Peak Detection\n\n9.3.1 Loading Raw Data\n\nCode# Example workflow with real LC-MS data\n# Load raw files (typically multiple mzML files)\nraw_files &lt;- c(\"sample1.mzML\", \"sample2.mzML\", \"sample3.mzML\")\n\n# Create phenotype data\npd &lt;- data.frame(\n  sample_name = basename(raw_files),\n  sample_group = c(\"Control\", \"Control\", \"Treatment\"),\n  sample_type = \"Sample\",\n  stringsAsFactors = FALSE\n)\n\n# Read data into an XCMSnExp object\nraw_data &lt;- readMSData(\n  files = raw_files,\n  pdata = new(\"NAnnotatedDataFrame\", pd),\n  mode = \"onDisk\"\n)\n\n# Display data summary\nraw_data\n\n\n\n9.3.2 Chromatographic Peak Detection\n\nCode# Define peak detection parameters (CentWave for high-resolution data)\ncwp &lt;- CentWaveParam(\n  ppm = 20,                  # m/z tolerance in ppm\n  peakwidth = c(5, 20),      # Expected peak width range (seconds)\n  snthresh = 10,             # Signal-to-noise threshold\n  prefilter = c(3, 5000),   # Prefilter: min peaks, min intensity\n  mzCenterFun = \"wMean\",     # Weighted mean for m/z\n  integrate = 1,             # Integration method\n  mzdiff = 0.01,            # Minimum m/z difference\n  fitgauss = FALSE,          # Gaussian peak fitting\n  noise = 1000              # Noise threshold\n)\n\n# Perform peak detection\ndata_peaks &lt;- findChromPeaks(raw_data, param = cwp)\n\n# Summary of detected peaks\ncat(\"Total chromatographic peaks:\", sum(chromPeaks(data_peaks)[, \"into\"] &gt; 0), \"\\n\")\n\n\n\n9.3.3 Simulated XCMS Workflow\nFor demonstration, let’s create a simulated dataset:\n\nCode# Create synthetic metabolomics data for demonstration\nset.seed(42)\n\n# Simulate retention times and m/z values for metabolites\nn_metabolites &lt;- 50\nmetabolite_data &lt;- data.frame(\n  metabolite_id = paste0(\"Met_\", 1:n_metabolites),\n  mz = runif(n_metabolites, 100, 800),\n  rt = runif(n_metabolites, 60, 1200),  # RT in seconds\n  intensity_mean = rlnorm(n_metabolites, meanlog = 6, sdlog = 1)\n)\n\n# Create sample information\nsample_info &lt;- data.frame(\n  sample_name = paste0(\"Sample_\", 1:20),\n  group = rep(c(\"Control\", \"Treatment\"), each = 10),\n  batch = rep(1:2, each = 10),\n  injection_order = 1:20\n)\n\n# Simulate peak intensity matrix\nintensity_matrix &lt;- matrix(nrow = nrow(sample_info), ncol = nrow(metabolite_data))\nrownames(intensity_matrix) &lt;- sample_info$sample_name\ncolnames(intensity_matrix) &lt;- metabolite_data$metabolite_id\n\nfor (i in 1:nrow(sample_info)) {\n  for (j in 1:nrow(metabolite_data)) {\n    # Add group effect for some metabolites\n    group_effect &lt;- ifelse(sample_info$group[i] == \"Treatment\" & j &lt;= 15, 1.5, 1.0)\n    # Add some biological and technical variation\n    intensity_matrix[i, j] &lt;- metabolite_data$intensity_mean[j] * group_effect * \n                              rlnorm(1, 0, 0.3)  # 30% CV\n  }\n}\n\ncat(\"Created synthetic metabolomics dataset with\", nrow(sample_info), \"samples and\", \n    nrow(metabolite_data), \"metabolites\\n\")\n\nCreated synthetic metabolomics dataset with 20 samples and 50 metabolites\n\n\n\n9.3.4 Peak Quality Assessment\n\nCode# Function to assess peak quality\nassess_peak_quality &lt;- function(intensity_matrix, sample_info) {\n  # Calculate QC metrics\n  quality_metrics &lt;- data.frame(\n    metabolite_id = colnames(intensity_matrix),\n    mean_intensity = apply(intensity_matrix, 2, mean, na.rm = TRUE),\n    median_intensity = apply(intensity_matrix, 2, median, na.rm = TRUE),\n    cv_percent = apply(intensity_matrix, 2, function(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) * 100),\n    missing_percent = apply(intensity_matrix, 2, function(x) sum(is.na(x)) / length(x) * 100),\n    detection_rate = apply(intensity_matrix, 2, function(x) sum(x &gt; 0, na.rm = TRUE) / length(x) * 100)\n  )\n  \n  return(quality_metrics)\n}\n\npeak_quality &lt;- assess_peak_quality(intensity_matrix, sample_info)\n\n# Visualize peak quality metrics\nggplot(peak_quality, aes(x = cv_percent)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", alpha = 0.7) +\n  geom_vline(xintercept = 30, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Distribution of Coefficient of Variation\",\n       subtitle = \"Red line indicates 30% CV threshold\",\n       x = \"CV (%)\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n9.3.5 Peak Filtering\n\nCode# Apply quality filters\nfilter_peaks &lt;- function(intensity_matrix, quality_metrics, \n                        cv_threshold = 30, \n                        detection_threshold = 80,\n                        min_intensity = 1000) {\n  \n  # Define filters\n  cv_filter &lt;- quality_metrics$cv_percent &lt; cv_threshold\n  detection_filter &lt;- quality_metrics$detection_rate &gt;= detection_threshold\n  intensity_filter &lt;- quality_metrics$mean_intensity &gt;= min_intensity\n  \n  # Combine filters\n  pass_filter &lt;- cv_filter & detection_filter & intensity_filter\n  \n  cat(\"Filtering results:\\n\")\n  cat(\"  CV filter:\", sum(cv_filter, na.rm = TRUE), \"passed\\n\")\n  cat(\"  Detection rate filter:\", sum(detection_filter, na.rm = TRUE), \"passed\\n\")\n  cat(\"  Intensity filter:\", sum(intensity_filter, na.rm = TRUE), \"passed\\n\")\n  cat(\"  Combined filter:\", sum(pass_filter, na.rm = TRUE), \"passed\\n\")\n  \n  # Apply filter\n  filtered_matrix &lt;- intensity_matrix[, pass_filter]\n  filtered_metabolites &lt;- metabolite_data[pass_filter, ]\n  \n  return(list(\n    intensity_matrix = filtered_matrix,\n    metabolite_data = filtered_metabolites,\n    filter_summary = data.frame(\n      total_features = ncol(intensity_matrix),\n      passed_filter = sum(pass_filter, na.rm = TRUE),\n      filter_rate = sum(pass_filter, na.rm = TRUE) / ncol(intensity_matrix) * 100\n    )\n  ))\n}\n\nfiltered_data &lt;- filter_peaks(intensity_matrix, peak_quality)\n\nFiltering results:\n  CV filter: 21 passed\n  Detection rate filter: 50 passed\n  Intensity filter: 9 passed\n  Combined filter: 4 passed\n\nCodeprint(filtered_data$filter_summary)\n\n  total_features passed_filter filter_rate\n1             50             4           8",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-metabolomics-analysis.html#data-normalization-and-scaling",
    "href": "08-metabolomics-analysis.html#data-normalization-and-scaling",
    "title": "\n9  Metabolomics Data Analysis\n",
    "section": "\n9.4 Data Normalization and Scaling",
    "text": "9.4 Data Normalization and Scaling\n\n9.4.1 Different Normalization Methods\n\nCode# Implement various normalization methods\nnormalize_data &lt;- function(intensity_matrix, method = \"median\") {\n  normalized_matrix &lt;- intensity_matrix\n  \n  if (method == \"median\") {\n    # Median normalization\n    sample_medians &lt;- apply(intensity_matrix, 1, median, na.rm = TRUE)\n    global_median &lt;- median(sample_medians, na.rm = TRUE)\n    normalization_factors &lt;- global_median / sample_medians\n    \n    for (i in 1:nrow(intensity_matrix)) {\n      normalized_matrix[i, ] &lt;- intensity_matrix[i, ] * normalization_factors[i]\n    }\n    \n  } else if (method == \"tic\") {\n    # Total ion current normalization\n    sample_sums &lt;- apply(intensity_matrix, 1, sum, na.rm = TRUE)\n    global_sum &lt;- median(sample_sums, na.rm = TRUE)\n    normalization_factors &lt;- global_sum / sample_sums\n    \n    for (i in 1:nrow(intensity_matrix)) {\n      normalized_matrix[i, ] &lt;- intensity_matrix[i, ] * normalization_factors[i]\n    }\n    \n  } else if (method == \"quantile\") {\n    # Quantile normalization (simplified)\n    ranked_data &lt;- apply(intensity_matrix, 2, rank, na.rm = TRUE, ties.method = \"average\")\n    sorted_means &lt;- apply(apply(intensity_matrix, 2, sort, na.rm = TRUE), 1, mean, na.rm = TRUE)\n    \n    for (i in 1:ncol(intensity_matrix)) {\n      normalized_matrix[, i] &lt;- sorted_means[ranked_data[, i]]\n    }\n  }\n  \n  return(normalized_matrix)\n}\n\n# Apply different normalization methods\nmedian_normalized &lt;- normalize_data(filtered_data$intensity_matrix, \"median\")\ntic_normalized &lt;- normalize_data(filtered_data$intensity_matrix, \"tic\")\n\n# Compare normalization effects\ncompare_normalization &lt;- function(original, normalized, method_name) {\n  original_cv &lt;- apply(original, 1, function(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) * 100)\n  normalized_cv &lt;- apply(normalized, 1, function(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) * 100)\n  \n  comparison_df &lt;- data.frame(\n    sample = rownames(original),\n    original_cv = original_cv,\n    normalized_cv = normalized_cv,\n    method = method_name\n  )\n  \n  return(comparison_df)\n}\n\nnormalization_comparison &lt;- rbind(\n  compare_normalization(filtered_data$intensity_matrix, median_normalized, \"Median\"),\n  compare_normalization(filtered_data$intensity_matrix, tic_normalized, \"TIC\")\n)\n\n# Visualize normalization effects\nggplot(normalization_comparison, aes(x = original_cv, y = normalized_cv, color = method)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\") +\n  facet_wrap(~method) +\n  labs(title = \"Effect of Normalization on Sample CV\",\n       x = \"Original CV (%)\", y = \"Normalized CV (%)\",\n       color = \"Method\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n9.4.2 Data Scaling\n\nCode# Different scaling methods\nscale_data &lt;- function(normalized_matrix, method = \"auto\") {\n  scaled_matrix &lt;- normalized_matrix\n  \n  if (method == \"auto\") {\n    # Auto-scaling (mean-centering + unit variance)\n    scaled_matrix &lt;- scale(normalized_matrix)\n    \n  } else if (method == \"pareto\") {\n    # Pareto scaling (mean-centering + square root of standard deviation)\n    means &lt;- apply(normalized_matrix, 2, mean, na.rm = TRUE)\n    sds &lt;- apply(normalized_matrix, 2, sd, na.rm = TRUE)\n    \n    for (i in 1:ncol(normalized_matrix)) {\n      scaled_matrix[, i] &lt;- (normalized_matrix[, i] - means[i]) / sqrt(sds[i])\n    }\n    \n  } else if (method == \"range\") {\n    # Range scaling (0-1 normalization)\n    for (i in 1:ncol(normalized_matrix)) {\n      min_val &lt;- min(normalized_matrix[, i], na.rm = TRUE)\n      max_val &lt;- max(normalized_matrix[, i], na.rm = TRUE)\n      scaled_matrix[, i] &lt;- (normalized_matrix[, i] - min_val) / (max_val - min_val)\n    }\n  }\n  \n  return(scaled_matrix)\n}\n\n# Apply different scaling methods\nauto_scaled &lt;- scale_data(median_normalized, \"auto\")\npareto_scaled &lt;- scale_data(median_normalized, \"pareto\")\nrange_scaled &lt;- scale_data(median_normalized, \"range\")\n\n# Visualize scaling effects\nscaling_comparison &lt;- data.frame(\n  feature = rep(colnames(median_normalized), 3),\n  variance = c(\n    apply(auto_scaled, 2, var, na.rm = TRUE),\n    apply(pareto_scaled, 2, var, na.rm = TRUE),\n    apply(range_scaled, 2, var, na.rm = TRUE)\n  ),\n  method = rep(c(\"Auto\", \"Pareto\", \"Range\"), each = ncol(median_normalized))\n)\n\nggplot(scaling_comparison, aes(x = method, y = variance, fill = method)) +\n  geom_boxplot(alpha = 0.7) +\n  scale_y_log10() +\n  labs(title = \"Variance Distribution by Scaling Method\",\n       x = \"Scaling Method\", y = \"Variance (log10 scale)\",\n       fill = \"Method\") +\n  theme_minimal()",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-metabolomics-analysis.html#multivariate-analysis-for-metabolomics",
    "href": "08-metabolomics-analysis.html#multivariate-analysis-for-metabolomics",
    "title": "\n9  Metabolomics Data Analysis\n",
    "section": "\n9.5 Multivariate Analysis for Metabolomics",
    "text": "9.5 Multivariate Analysis for Metabolomics\n\n9.5.1 Principal Component Analysis (PCA)\n\nCode# Perform PCA on scaled data\nperform_metabolomics_pca &lt;- function(scaled_matrix, sample_info) {\n  # Remove any NA values\n  clean_matrix &lt;- scaled_matrix[complete.cases(scaled_matrix), ]\n  \n  # Perform PCA\n  pca_result &lt;- prcomp(clean_matrix, center = FALSE, scale. = FALSE)\n  \n  # Extract scores\n  pca_scores &lt;- data.frame(pca_result$x) %&gt;%\n    mutate(\n      sample_name = rownames(clean_matrix),\n      group = sample_info$group[match(rownames(clean_matrix), sample_info$sample_name)],\n      batch = factor(sample_info$batch[match(rownames(clean_matrix), sample_info$sample_name)])\n    )\n  \n  # Calculate variance explained\n  var_explained &lt;- (pca_result$sdev^2 / sum(pca_result$sdev^2)) * 100\n  \n  return(list(\n    pca_result = pca_result,\n    scores = pca_scores,\n    variance_explained = var_explained\n  ))\n}\n\npca_analysis &lt;- perform_metabolomics_pca(auto_scaled, sample_info)\n\n# PCA scores plot\nggplot(pca_analysis$scores, aes(x = PC1, y = PC2, color = group, shape = batch)) +\n  geom_point(size = 3, alpha = 0.8) +\n  stat_ellipse(aes(group = group), alpha = 0.3) +\n  labs(title = \"PCA Scores Plot - Metabolomics Data\",\n       x = paste0(\"PC1 (\", round(pca_analysis$variance_explained[1], 1), \"%)\"),\n       y = paste0(\"PC2 (\", round(pca_analysis$variance_explained[2], 1), \"%)\"),\n       color = \"Group\", shape = \"Batch\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n9.5.2 Partial Least Squares Discriminant Analysis (PLS-DA)\n\nCode# Perform PLS-DA using mixOmics (if available)\nif (requireNamespace(\"mixOmics\", quietly = TRUE)) {\n  perform_plsda &lt;- function(scaled_matrix, groups) {\n    # Clean data\n    clean_matrix &lt;- scaled_matrix[complete.cases(scaled_matrix), ]\n    clean_groups &lt;- groups[complete.cases(scaled_matrix)]\n    \n    # Perform PLS-DA\n    plsda_result &lt;- mixOmics::plsda(clean_matrix, clean_groups, ncomp = 3)\n    \n    return(plsda_result)\n  }\n  \n  plsda_analysis &lt;- perform_plsda(auto_scaled, sample_info$group)\n  \n  # Extract PLS-DA scores\n  plsda_scores &lt;- data.frame(plsda_analysis$variates$X) %&gt;%\n    mutate(\n      sample_name = rownames(auto_scaled)[complete.cases(auto_scaled)],\n      group = sample_info$group[complete.cases(auto_scaled)]\n    )\n  \n  # PLS-DA scores plot\n  p &lt;- ggplot(plsda_scores, aes(x = comp1, y = comp2, color = group)) +\n    geom_point(size = 3, alpha = 0.8) +\n    stat_ellipse(alpha = 0.3) +\n    labs(title = \"PLS-DA Scores Plot\",\n         x = \"Component 1\", y = \"Component 2\",\n         color = \"Group\") +\n    theme_minimal()\n  print(p)\n} else {\n  cat(\"Note: mixOmics package not available. Skipping PLS-DA analysis.\\n\")\n  cat(\"Install with: BiocManager::install('mixOmics')\\n\")\n}\n\nNote: mixOmics package not available. Skipping PLS-DA analysis.\nInstall with: BiocManager::install('mixOmics')\n\n\n\n9.5.3 Variable Importance Analysis\n\nCode# Calculate VIP scores (if mixOmics is available and PLS-DA was run)\nif (requireNamespace(\"mixOmics\", quietly = TRUE) && exists(\"plsda_analysis\")) {\n  calculate_vip &lt;- function(plsda_result) {\n    vip_scores &lt;- mixOmics::vip(plsda_result)\n    \n    vip_data &lt;- data.frame(\n      feature = rownames(vip_scores),\n      vip_comp1 = vip_scores[, 1],\n      vip_comp2 = if(ncol(vip_scores) &gt; 1) vip_scores[, 2] else NA\n    )\n    \n    return(vip_data)\n  }\n  \n  vip_scores &lt;- calculate_vip(plsda_analysis)\n  \n  # Plot VIP scores\n  p &lt;- ggplot(vip_scores, aes(x = reorder(feature, vip_comp1), y = vip_comp1)) +\n    geom_bar(stat = \"identity\", fill = \"steelblue\", alpha = 0.7) +\n    geom_hline(yintercept = 1, color = \"red\", linetype = \"dashed\") +\n    coord_flip() +\n    labs(title = \"Variable Importance in Projection (VIP) Scores\",\n         subtitle = \"Red line indicates VIP &gt; 1 threshold\",\n         x = \"Features\", y = \"VIP Score (Component 1)\") +\n    theme_minimal() +\n    theme(axis.text.y = element_text(size = 6))\n  print(p)\n} else {\n  cat(\"Note: VIP scores require mixOmics package and successful PLS-DA analysis.\\n\")\n  # Create empty vip_scores object for downstream code\n  vip_scores &lt;- NULL\n}\n\nNote: VIP scores require mixOmics package and successful PLS-DA analysis.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-metabolomics-analysis.html#metabolite-identification",
    "href": "08-metabolomics-analysis.html#metabolite-identification",
    "title": "\n9  Metabolomics Data Analysis\n",
    "section": "\n9.6 Metabolite Identification",
    "text": "9.6 Metabolite Identification\n\n9.6.1 Accurate Mass Matching\n\nCode# Function for accurate mass matching\naccurate_mass_search &lt;- function(query_mz, mass_database, tolerance_ppm = 5) {\n  # Create a simple mass database (normally you'd use real databases)\n  if (missing(mass_database)) {\n    mass_database &lt;- data.frame(\n      compound_name = c(\"Glucose\", \"Fructose\", \"Sucrose\", \"Lactose\", \"Maltose\",\n                       \"Alanine\", \"Glycine\", \"Serine\", \"Threonine\", \"Valine\"),\n      exact_mass = c(180.0634, 180.0634, 342.1162, 342.1162, 342.1162,\n                    89.0477, 75.0320, 105.0426, 119.0582, 117.0790),\n      formula = c(\"C6H12O6\", \"C6H12O6\", \"C12H22O11\", \"C12H22O11\", \"C12H22O11\",\n                 \"C3H7NO2\", \"C2H5NO2\", \"C3H7NO3\", \"C4H9NO3\", \"C5H11NO2\")\n    )\n  }\n  \n  matches &lt;- data.frame()\n  \n  for (i in seq_along(query_mz)) {\n    # Calculate mass differences\n    mass_diff_ppm &lt;- abs(mass_database$exact_mass - query_mz[i]) / query_mz[i] * 1e6\n    \n    # Find matches within tolerance\n    match_idx &lt;- which(mass_diff_ppm &lt;= tolerance_ppm)\n    \n    if (length(match_idx) &gt; 0) {\n      for (j in match_idx) {\n        matches &lt;- rbind(matches, data.frame(\n          query_mz = query_mz[i],\n          matched_compound = mass_database$compound_name[j],\n          exact_mass = mass_database$exact_mass[j],\n          formula = mass_database$formula[j],\n          mass_error_ppm = mass_diff_ppm[j]\n        ))\n      }\n    }\n  }\n  \n  return(matches)\n}\n\n# Perform mass matching for significant features\nif (!is.null(vip_scores)) {\n  significant_features &lt;- vip_scores$feature[vip_scores$vip_comp1 &gt; 1]\n} else {\n  # Use alternative method to identify significant features\n  # Based on highest variance across samples\n  feature_variance &lt;- apply(filtered_data$metabolite_data[, -1], 1, var, na.rm = TRUE)\n  top_features_idx &lt;- order(feature_variance, decreasing = TRUE)[1:min(10, length(feature_variance))]\n  significant_features &lt;- filtered_data$metabolite_data$metabolite_id[top_features_idx]\n}\n\nif (length(significant_features) &gt; 0) {\n  # Extract m/z values for significant features\n  significant_mz &lt;- filtered_data$metabolite_data$mz[\n    filtered_data$metabolite_data$metabolite_id %in% significant_features\n  ]\n  \n  mass_matches &lt;- accurate_mass_search(significant_mz)\n  \n  if (nrow(mass_matches) &gt; 0) {\n    cat(\"Potential metabolite identifications:\\n\")\n    print(mass_matches)\n  } else {\n    cat(\"No matches found in database\\n\")\n  }\n}\n\nNo matches found in database\n\n\n\n9.6.2 MS/MS Spectrum Matching\n\nCode# Simulate MS/MS data for demonstration\nsimulate_msms_spectrum &lt;- function(precursor_mz, intensity = 1000) {\n  # Generate realistic fragment ions\n  n_fragments &lt;- sample(5:15, 1)\n  \n  # Common neutral losses for metabolites\n  neutral_losses &lt;- c(18.01, 28.01, 44.00, 46.00, 60.02, 62.00)\n  \n  fragment_mz &lt;- numeric(n_fragments)\n  fragment_intensity &lt;- numeric(n_fragments)\n  \n  for (i in 1:n_fragments) {\n    if (i &lt;= length(neutral_losses) && runif(1) &gt; 0.3) {\n      # Use neutral loss\n      fragment_mz[i] &lt;- precursor_mz - neutral_losses[i]\n    } else {\n      # Random fragment\n      fragment_mz[i] &lt;- runif(1, 50, precursor_mz - 1)\n    }\n    \n    # Random intensity\n    fragment_intensity[i] &lt;- runif(1, 0.1, 1) * intensity\n  }\n  \n  # Sort by m/z\n  order_idx &lt;- order(fragment_mz)\n  \n  return(data.frame(\n    mz = fragment_mz[order_idx],\n    intensity = fragment_intensity[order_idx]\n  ))\n}\n\n# Create example MS/MS spectra\nexample_msms &lt;- simulate_msms_spectrum(180.0634)  # Glucose-like spectrum\n\n# Plot MS/MS spectrum\nggplot(example_msms, aes(x = mz, y = intensity)) +\n  geom_segment(aes(xend = mz, yend = 0), color = \"blue\", size = 1) +\n  geom_point(color = \"red\", size = 2) +\n  labs(title = \"Simulated MS/MS Spectrum\",\n       x = \"m/z\", y = \"Relative Intensity\") +\n  theme_minimal()",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-metabolomics-analysis.html#pathway-analysis",
    "href": "08-metabolomics-analysis.html#pathway-analysis",
    "title": "\n9  Metabolomics Data Analysis\n",
    "section": "\n9.7 Pathway Analysis",
    "text": "9.7 Pathway Analysis\n\n9.7.1 Metabolite Set Enrichment\n\nCode# Simulate pathway database\ncreate_pathway_database &lt;- function() {\n  pathways &lt;- list(\n    \"Glycolysis\" = c(\"Met_1\", \"Met_3\", \"Met_5\", \"Met_7\", \"Met_9\"),\n    \"TCA_Cycle\" = c(\"Met_2\", \"Met_4\", \"Met_6\", \"Met_8\", \"Met_10\"),\n    \"Amino_Acid_Metabolism\" = c(\"Met_11\", \"Met_12\", \"Met_13\", \"Met_14\", \"Met_15\"),\n    \"Lipid_Metabolism\" = c(\"Met_16\", \"Met_17\", \"Met_18\", \"Met_19\", \"Met_20\"),\n    \"Nucleotide_Metabolism\" = c(\"Met_21\", \"Met_22\", \"Met_23\", \"Met_24\", \"Met_25\")\n  )\n  \n  return(pathways)\n}\n\n# Perform pathway enrichment\npathway_enrichment &lt;- function(significant_metabolites, pathway_db, total_metabolites) {\n  enrichment_results &lt;- data.frame()\n  \n  for (pathway_name in names(pathway_db)) {\n    pathway_metabolites &lt;- pathway_db[[pathway_name]]\n    \n    # Calculate overlap\n    overlap &lt;- intersect(significant_metabolites, pathway_metabolites)\n    \n    # Calculate contingency table values\n    a &lt;- length(overlap)  # significant & in pathway\n    b &lt;- length(significant_metabolites) - length(overlap)  # significant & not in pathway\n    c &lt;- length(pathway_metabolites) - length(overlap)  # not significant & in pathway\n    d &lt;- total_metabolites - length(significant_metabolites) - \n         length(pathway_metabolites) + length(overlap)  # not significant & not in pathway\n    \n    # Ensure all values are non-negative\n    if (d &lt; 0 || any(c(a, b, c, d) &lt; 0)) {\n      # Skip this pathway if contingency table is invalid\n      next\n    }\n    \n    # Fisher's exact test\n    contingency_table &lt;- matrix(c(a, b, c, d), nrow = 2)\n    \n    fisher_test &lt;- tryCatch({\n      fisher.test(contingency_table, alternative = \"greater\")\n    }, error = function(e) {\n      list(p.value = 1, estimate = 1)\n    })\n    \n    enrichment_results &lt;- rbind(enrichment_results, data.frame(\n      pathway = pathway_name,\n      overlap_size = length(overlap),\n      pathway_size = length(pathway_metabolites),\n      significant_size = length(significant_metabolites),\n      p_value = fisher_test$p.value,\n      odds_ratio = as.numeric(fisher_test$estimate)\n    ))\n  }\n  \n  # Multiple testing correction\n  if (nrow(enrichment_results) &gt; 0) {\n    enrichment_results$p_adjusted &lt;- p.adjust(enrichment_results$p_value, method = \"fdr\")\n    return(enrichment_results[order(enrichment_results$p_value), ])\n  } else {\n    return(data.frame(\n      pathway = character(0),\n      overlap_size = integer(0),\n      pathway_size = integer(0),\n      significant_size = integer(0),\n      p_value = numeric(0),\n      odds_ratio = numeric(0),\n      p_adjusted = numeric(0)\n    ))\n  }\n}\n\n# Perform enrichment analysis\npathway_db &lt;- create_pathway_database()\nsignificant_metabolites &lt;- significant_features\n\nenrichment_results &lt;- pathway_enrichment(\n  significant_metabolites, \n  pathway_db, \n  ncol(filtered_data$intensity_matrix)\n)\n\nif (nrow(enrichment_results) &gt; 0) {\n  print(enrichment_results)\n  \n  # Visualize enrichment results\n  p &lt;- ggplot(enrichment_results, aes(x = reorder(pathway, -log10(p_value)), \n                                y = -log10(p_value))) +\n    geom_bar(stat = \"identity\", fill = \"coral\", alpha = 0.7) +\n    geom_hline(yintercept = -log10(0.05), linetype = \"dashed\", color = \"red\") +\n    coord_flip() +\n    labs(title = \"Pathway Enrichment Analysis\",\n         x = \"Pathway\", y = \"-log10(P-value)\") +\n    theme_minimal()\n  print(p)\n} else {\n  cat(\"No significant pathway enrichment found.\\n\")\n}\n\nNo significant pathway enrichment found.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-metabolomics-analysis.html#quality-control-and-batch-correction",
    "href": "08-metabolomics-analysis.html#quality-control-and-batch-correction",
    "title": "\n9  Metabolomics Data Analysis\n",
    "section": "\n9.8 Quality Control and Batch Correction",
    "text": "9.8 Quality Control and Batch Correction\n\n9.8.1 QC Sample Analysis\n\nCode# Simulate QC samples\nsimulate_qc_samples &lt;- function(original_matrix, n_qc = 5) {\n  # QC samples are typically pooled samples with intermediate values\n  qc_matrix &lt;- matrix(nrow = n_qc, ncol = ncol(original_matrix))\n  rownames(qc_matrix) &lt;- paste0(\"QC_\", 1:n_qc)\n  colnames(qc_matrix) &lt;- colnames(original_matrix)\n  \n  for (i in 1:n_qc) {\n    for (j in 1:ncol(original_matrix)) {\n      # QC intensity as mean of original samples with some variation\n      qc_matrix[i, j] &lt;- mean(original_matrix[, j], na.rm = TRUE) * \n                         rlnorm(1, 0, 0.1)  # 10% variation\n    }\n  }\n  \n  return(qc_matrix)\n}\n\nqc_matrix &lt;- simulate_qc_samples(filtered_data$intensity_matrix)\n\n# Calculate QC stability\ncalculate_qc_stability &lt;- function(qc_matrix) {\n  qc_cv &lt;- apply(qc_matrix, 2, function(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE) * 100)\n  \n  stability_summary &lt;- data.frame(\n    feature = names(qc_cv),\n    qc_cv = qc_cv,\n    stable = qc_cv &lt; 20  # 20% CV threshold for stability\n  )\n  \n  return(stability_summary)\n}\n\nqc_stability &lt;- calculate_qc_stability(qc_matrix)\n\n# Visualize QC stability\nggplot(qc_stability, aes(x = qc_cv, fill = stable)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  scale_fill_manual(values = c(\"TRUE\" = \"green\", \"FALSE\" = \"red\")) +\n  geom_vline(xintercept = 20, linetype = \"dashed\", color = \"black\") +\n  labs(title = \"QC Sample Stability Assessment\",\n       x = \"QC CV (%)\", y = \"Count\",\n       fill = \"Stable (CV &lt; 20%)\") +\n  theme_minimal()\n\n\n\n\n\n\nCodecat(\"Percentage of stable features:\", \n    round(sum(qc_stability$stable) / nrow(qc_stability) * 100, 1), \"%\\n\")\n\nPercentage of stable features: 100 %",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-metabolomics-analysis.html#exercises",
    "href": "08-metabolomics-analysis.html#exercises",
    "title": "\n9  Metabolomics Data Analysis\n",
    "section": "\n9.9 Exercises",
    "text": "9.9 Exercises\n\nProcess real metabolomics data using XCMS\nCompare different normalization methods on your dataset\nImplement a complete identification workflow using spectral databases\nPerform time-course metabolomics analysis\nBuild a metabolomics data processing pipeline with quality control",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-metabolomics-analysis.html#summary",
    "href": "08-metabolomics-analysis.html#summary",
    "title": "\n9  Metabolomics Data Analysis\n",
    "section": "\n9.10 Summary",
    "text": "9.10 Summary\nThis chapter covered comprehensive metabolomics data analysis workflows, including peak detection, data normalization, multivariate analysis, metabolite identification, and pathway analysis. These methods form the foundation for extracting biological insights from metabolomics experiments.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Metabolomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-proteomics-analysis.html",
    "href": "09-proteomics-analysis.html",
    "title": "\n10  Proteomics Data Analysis\n",
    "section": "",
    "text": "10.1 Setting Up Proteomics Environment\nProteomics focuses on the large-scale study of proteins, including their identification, quantification, and functional analysis. This chapter covers computational methods for bottom-up proteomics data analysis using the R for Mass Spectrometry ecosystem.\nThe R for Mass Spectrometry ecosystem provides specialized packages for proteomics analysis:\nCodelibrary(Spectra)          # Core MS data structures\nlibrary(PSMatch)          # Peptide-spectrum matching\nlibrary(ProtGenerics)     # Generic functions for proteomics\nlibrary(QFeatures)        # Quantitative features handling\nlibrary(msdata)           # Example MS data\nlibrary(mzR)             # Reading raw MS data\nlibrary(dplyr)           # Data manipulation\nlibrary(ggplot2)         # Visualization\nlibrary(pheatmap)        # Heatmaps\nlibrary(limma)           # Statistical analysis\nlibrary(tidyverse)       # Data science tools\nCode# Load proteomics test data from msdata\nproteomics_files &lt;- msdata::proteomics(full.names = TRUE)\ncat(\"Available proteomics files:\\n\")\n\nAvailable proteomics files:\n\nCodefor (i in seq_along(proteomics_files)) {\n  cat(i, \":\", basename(proteomics_files[i]), \"\\n\")\n}\n\n1 : MRM-standmix-5.mzML.gz \n2 : MS3TMT10_01022016_32917-33481.mzML.gz \n3 : MS3TMT11.mzML \n4 : TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01-20141210.mzML.gz \n5 : TMT_Erwinia_1uLSike_Top10HCD_isol2_45stepped_60min_01.mzML.gz \n\nCode# Select a file for analysis\nselected_file &lt;- proteomics_files[1]\ncat(\"\\nSelected file:\", basename(selected_file), \"\\n\")\n\n\nSelected file: MRM-standmix-5.mzML.gz",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-proteomics-analysis.html#understanding-proteomics-workflows",
    "href": "09-proteomics-analysis.html#understanding-proteomics-workflows",
    "title": "\n10  Proteomics Data Analysis\n",
    "section": "\n10.2 Understanding Proteomics Workflows",
    "text": "10.2 Understanding Proteomics Workflows\n\n10.2.1 Bottom-up Proteomics Pipeline\nThe typical bottom-up proteomics workflow involves:\n\n\nSample preparation: Protein extraction, digestion (usually with trypsin)\n\nLC-MS/MS analysis: Liquid chromatography coupled to tandem mass spectrometry\n\nDatabase searching: Matching MS/MS spectra to peptide sequences\n\nProtein inference: Assembling peptides into protein identifications\n\nQuantitative analysis: Comparing protein abundances across samples\n\n\nCodeflowchart TD\n    subgraph Sample[\"Sample Preparation\"]\n        A[Protein Extraction] --&gt; B[Reduction & Alkylation]\n        B --&gt; C[Enzymatic Digestion&lt;br/&gt;Trypsin]\n        C --&gt; D[Peptide Cleanup&lt;br/&gt;Desalting]\n    end\n    \n    subgraph MS[\"LC-MS/MS Analysis\"]\n        D --&gt; E[LC Separation&lt;br/&gt;Reverse Phase]\n        E --&gt; F[MS1 Scan&lt;br/&gt;Precursor Selection]\n        F --&gt; G[MS2 Fragmentation&lt;br/&gt;HCD/CID/ETD]\n        G --&gt; H[Raw Data&lt;br/&gt;mzML Files]\n    end\n    \n    subgraph Search[\"Database Searching\"]\n        H --&gt; I[Spectra Object&lt;br/&gt;R/Spectra]\n        I --&gt; J{Search Engine}\n        J --&gt; K1[Mascot]\n        J --&gt; K2[MaxQuant]\n        J --&gt; K3[MSFragger]\n        K1 --&gt; L[PSM Table&lt;br/&gt;PSMatch]\n        K2 --&gt; L\n        K3 --&gt; L\n    end\n    \n    subgraph Inference[\"Protein Inference\"]\n        L --&gt; M[Filter PSMs&lt;br/&gt;FDR &lt; 1%]\n        M --&gt; N[Peptide Assembly&lt;br/&gt;Unique + Shared]\n        N --&gt; O[Protein Grouping&lt;br/&gt;Parsimony Principle]\n    end\n    \n    subgraph Quant[\"Quantification\"]\n        O --&gt; P{Quant Method?}\n        P --&gt;|Label-Free| Q1[XIC Integration&lt;br/&gt;MS1 Intensity]\n        P --&gt;|TMT/iTRAQ| Q2[Reporter Ions&lt;br/&gt;MS2 Intensity]\n        P --&gt;|SILAC| Q3[Heavy/Light Ratio&lt;br/&gt;MS1 Intensity]\n        Q1 --&gt; R[QFeatures Object]\n        Q2 --&gt; R\n        Q3 --&gt; R\n    end\n    \n    subgraph Analysis[\"Statistical Analysis\"]\n        R --&gt; S[PSM → Peptide&lt;br/&gt;Aggregation]\n        S --&gt; T[Peptide → Protein&lt;br/&gt;Summarization]\n        T --&gt; U[Differential Analysis&lt;br/&gt;limma/DEqMS]\n        U --&gt; V[Results&lt;br/&gt;Volcano/Heatmap]\n    end\n    \n  style Sample fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style MS fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n  style Search fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style Inference fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n  style Quant fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style Analysis fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n\n\n\n\nflowchart TD\n    subgraph Sample[\"Sample Preparation\"]\n        A[Protein Extraction] --&gt; B[Reduction & Alkylation]\n        B --&gt; C[Enzymatic Digestion&lt;br/&gt;Trypsin]\n        C --&gt; D[Peptide Cleanup&lt;br/&gt;Desalting]\n    end\n    \n    subgraph MS[\"LC-MS/MS Analysis\"]\n        D --&gt; E[LC Separation&lt;br/&gt;Reverse Phase]\n        E --&gt; F[MS1 Scan&lt;br/&gt;Precursor Selection]\n        F --&gt; G[MS2 Fragmentation&lt;br/&gt;HCD/CID/ETD]\n        G --&gt; H[Raw Data&lt;br/&gt;mzML Files]\n    end\n    \n    subgraph Search[\"Database Searching\"]\n        H --&gt; I[Spectra Object&lt;br/&gt;R/Spectra]\n        I --&gt; J{Search Engine}\n        J --&gt; K1[Mascot]\n        J --&gt; K2[MaxQuant]\n        J --&gt; K3[MSFragger]\n        K1 --&gt; L[PSM Table&lt;br/&gt;PSMatch]\n        K2 --&gt; L\n        K3 --&gt; L\n    end\n    \n    subgraph Inference[\"Protein Inference\"]\n        L --&gt; M[Filter PSMs&lt;br/&gt;FDR &lt; 1%]\n        M --&gt; N[Peptide Assembly&lt;br/&gt;Unique + Shared]\n        N --&gt; O[Protein Grouping&lt;br/&gt;Parsimony Principle]\n    end\n    \n    subgraph Quant[\"Quantification\"]\n        O --&gt; P{Quant Method?}\n        P --&gt;|Label-Free| Q1[XIC Integration&lt;br/&gt;MS1 Intensity]\n        P --&gt;|TMT/iTRAQ| Q2[Reporter Ions&lt;br/&gt;MS2 Intensity]\n        P --&gt;|SILAC| Q3[Heavy/Light Ratio&lt;br/&gt;MS1 Intensity]\n        Q1 --&gt; R[QFeatures Object]\n        Q2 --&gt; R\n        Q3 --&gt; R\n    end\n    \n    subgraph Analysis[\"Statistical Analysis\"]\n        R --&gt; S[PSM → Peptide&lt;br/&gt;Aggregation]\n        S --&gt; T[Peptide → Protein&lt;br/&gt;Summarization]\n        T --&gt; U[Differential Analysis&lt;br/&gt;limma/DEqMS]\n        U --&gt; V[Results&lt;br/&gt;Volcano/Heatmap]\n    end\n    \n  style Sample fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style MS fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n  style Search fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style Inference fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n  style Quant fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style Analysis fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n\n\n\n\n\n\n\n\n\n\n\n\nKey Proteomics Concepts\n\n\n\n\n\nPSM (Peptide-Spectrum Match): One MS/MS spectrum matched to one peptide sequence\n\nFDR (False Discovery Rate): Typically controlled at 1% using target-decoy approach\n\nProtein Parsimony: Minimal set of proteins explaining observed peptides\n\nMissing Values: Can occur at PSM, peptide, or protein level - handle appropriately\n\n\n\n\n10.2.2 Data Structures in Proteomics\nProteomics data has a hierarchical structure: - Spectra: Raw MS and MS/MS data - PSMs: Peptide-Spectrum Matches from database search - Peptides: Unique peptide sequences - Proteins: Protein groups inferred from peptides",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-proteomics-analysis.html#msms-spectral-data-processing",
    "href": "09-proteomics-analysis.html#msms-spectral-data-processing",
    "title": "\n10  Proteomics Data Analysis\n",
    "section": "\n10.3 MS/MS Spectral Data Processing",
    "text": "10.3 MS/MS Spectral Data Processing\n\n10.3.1 Loading and Examining MS/MS Data\n\nCode# Load MS/MS data with error handling\ntryCatch({\n  ms_data &lt;- Spectra(selected_file, backend = MsBackendMzR())\n  ms_data &lt;- setBackend(ms_data, backend = MsBackendDataFrame())\n  cat(\"Successfully loaded real MS data\\n\")\n}, error = function(e) {\n  cat(\"Note: Using synthetic data due to mzR compatibility issues\\n\")\n  cat(\"Error details:\", conditionMessage(e), \"\\n\\n\")\n  \n  # Create synthetic MS/MS data\n  set.seed(456)\n  n_spectra &lt;- 200\n  \n  library(S4Vectors)\n  library(IRanges)\n  \n  # Generate peak data\n  mz_list &lt;- lapply(1:n_spectra, function(i) {\n    sort(runif(sample(40:120, 1), 200, 1800))\n  })\n  \n  intensity_list &lt;- lapply(mz_list, function(mz_vals) {\n    rlnorm(length(mz_vals), meanlog = 7, sdlog = 2)\n  })\n  \n  # Generate MS levels (80% MS2, 20% MS1)\n  ms_levels &lt;- sample(1:2, n_spectra, replace = TRUE, prob = c(0.2, 0.8))\n  \n  # Create metadata DataFrame\n  spd &lt;- DataFrame(\n    msLevel = ms_levels,\n    rtime = seq(100, 4500, length.out = n_spectra),\n    acquisitionNum = 1:n_spectra,\n    precursorMz = ifelse(ms_levels == 1, NA_real_, runif(n_spectra, 400, 1600)),\n    precursorCharge = ifelse(ms_levels == 1, NA_integer_, sample(2:4, n_spectra, replace = TRUE)),\n    precursorIntensity = ifelse(ms_levels == 1, NA_real_, rlnorm(n_spectra, meanlog = 10, sdlog = 1.5)),\n    collisionEnergy = ifelse(ms_levels == 1, NA_real_, runif(n_spectra, 25, 45)),\n    polarity = rep(1L, n_spectra)\n  )\n  \n  # Add peak data as list columns\n  spd$mz &lt;- NumericList(mz_list)\n  spd$intensity &lt;- NumericList(intensity_list)\n  \n  # Initialize backend and create Spectra object\n  backend &lt;- MsBackendDataFrame()\n  backend &lt;- backendInitialize(backend, spd)\n  ms_data &lt;&lt;- Spectra(backend)\n})\n\nNote: Using synthetic data due to mzR compatibility issues\nError details: BiocParallel errors\n  1 remote errors, element index: 1\n  0 unevaluated and other errors\n  first remote error:\nError in DataFrame(..., check.names = FALSE): different row counts implied by arguments\n \n\nCode# Basic information about the data\ncat(\"\\nDataset summary:\\n\")\n\n\nDataset summary:\n\nCodecat(\"Total spectra:\", length(ms_data), \"\\n\")\n\nTotal spectra: 200 \n\nCodecat(\"MS levels:\", paste(unique(msLevel(ms_data)), collapse = \", \"), \"\\n\")\n\nMS levels: 2, 1 \n\nCodecat(\"Scan range:\", range(acquisitionNum(ms_data)), \"\\n\")\n\nScan range: 1 200 \n\nCodecat(\"RT range:\", round(range(rtime(ms_data)), 2), \"seconds\\n\")\n\nRT range: 100 4500 seconds\n\nCode# MS2 spectra information\nms2_data &lt;- filterMsLevel(ms_data, msLevel = 2)\ncat(\"\\nMS2 spectra:\", length(ms2_data), \"\\n\")\n\n\nMS2 spectra: 164 \n\nCodeif (length(ms2_data) &gt; 0) {\n  cat(\"Precursor m/z range:\", round(range(precursorMz(ms2_data), na.rm = TRUE), 2), \"\\n\")\n  cat(\"Charge state distribution:\\n\")\n  print(table(precursorCharge(ms2_data)))\n}\n\nPrecursor m/z range: 400.4 1595.69 \nCharge state distribution:\n\n 2  3  4 \n49 58 57 \n\n\n\n10.3.2 MS/MS Spectrum Quality Assessment\n\nCode# Function to assess MS/MS spectrum quality\nassess_ms2_quality &lt;- function(ms2_spectra) {\n  quality_metrics &lt;- data.frame(\n    spectrum_id = seq_along(ms2_spectra),\n    precursor_mz = precursorMz(ms2_spectra),\n    precursor_charge = precursorCharge(ms2_spectra),\n    precursor_intensity = precursorIntensity(ms2_spectra),\n    retention_time = rtime(ms2_spectra),\n    total_ion_current = sapply(seq_along(ms2_spectra), function(i) {\n      sum(intensity(ms2_spectra[i])[[1]], na.rm = TRUE)\n    }),\n    peak_count = sapply(seq_along(ms2_spectra), function(i) {\n      length(intensity(ms2_spectra[i])[[1]])\n    }),\n    base_peak_intensity = sapply(seq_along(ms2_spectra), function(i) {\n      max(intensity(ms2_spectra[i])[[1]], na.rm = TRUE)\n    })\n  )\n  \n  # Calculate signal-to-noise metrics\n  quality_metrics$snr_estimate &lt;- quality_metrics$base_peak_intensity / \n                                  (quality_metrics$total_ion_current / quality_metrics$peak_count)\n  \n  return(quality_metrics)\n}\n\n# Assess quality for first 100 MS2 spectra\nms2_quality &lt;- assess_ms2_quality(ms2_data[1:min(100, length(ms2_data))])\n\n# Visualize quality metrics\nquality_plots &lt;- list()\n\nquality_plots[[1]] &lt;- ggplot(ms2_quality, aes(x = peak_count)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", alpha = 0.7) +\n  labs(title = \"Distribution of Peak Counts\", x = \"Peak Count\", y = \"Frequency\") +\n  theme_minimal()\n\nquality_plots[[2]] &lt;- ggplot(ms2_quality, aes(x = precursor_charge, y = peak_count)) +\n  geom_boxplot(aes(group = precursor_charge), fill = \"lightcoral\", alpha = 0.7) +\n  labs(title = \"Peak Count vs Charge State\", x = \"Charge State\", y = \"Peak Count\") +\n  theme_minimal()\n\n# Print plots\nprint(quality_plots[[1]])\n\n\n\n\n\n\nCodeprint(quality_plots[[2]])\n\n\n\n\n\n\n\n\n10.3.3 Spectrum Preprocessing\n\nCode# Function to preprocess MS/MS spectra\npreprocess_ms2_spectrum &lt;- function(spectrum_obj, \n                                   min_intensity = 100,\n                                   top_n_peaks = 150,\n                                   remove_precursor = TRUE,\n                                   precursor_tolerance = 1.5) {\n  \n  processed_spectra &lt;- list()\n  \n  for (i in seq_along(spectrum_obj)) {\n    mz_vals &lt;- mz(spectrum_obj[i])[[1]]\n    int_vals &lt;- intensity(spectrum_obj[i])[[1]]\n    precursor_mz_val &lt;- precursorMz(spectrum_obj[i])\n    \n    if (length(mz_vals) == 0 || length(int_vals) == 0) {\n      next\n    }\n    \n    # Remove low-intensity peaks\n    intensity_filter &lt;- int_vals &gt;= min_intensity\n    mz_vals &lt;- mz_vals[intensity_filter]\n    int_vals &lt;- int_vals[intensity_filter]\n    \n    # Remove precursor ion if requested\n    if (remove_precursor && !is.na(precursor_mz_val)) {\n      precursor_filter &lt;- abs(mz_vals - precursor_mz_val) &gt; precursor_tolerance\n      mz_vals &lt;- mz_vals[precursor_filter]\n      int_vals &lt;- int_vals[precursor_filter]\n    }\n    \n    # Keep only top N peaks\n    if (length(int_vals) &gt; top_n_peaks) {\n      top_indices &lt;- order(int_vals, decreasing = TRUE)[1:top_n_peaks]\n      mz_vals &lt;- mz_vals[top_indices]\n      int_vals &lt;- int_vals[top_indices]\n      \n      # Re-order by m/z\n      order_indices &lt;- order(mz_vals)\n      mz_vals &lt;- mz_vals[order_indices]\n      int_vals &lt;- int_vals[order_indices]\n    }\n    \n    # Normalize intensities\n    int_vals &lt;- int_vals / max(int_vals) * 100\n    \n    processed_spectra[[i]] &lt;- list(\n      spectrum_index = i,\n      mz = mz_vals,\n      intensity = int_vals,\n      precursor_mz = precursor_mz_val,\n      precursor_charge = precursorCharge(spectrum_obj[i]),\n      retention_time = rtime(spectrum_obj[i]),\n      original_peak_count = length(intensity(spectrum_obj[i])[[1]]),\n      processed_peak_count = length(int_vals)\n    )\n  }\n  \n  return(processed_spectra)\n}\n\n# Preprocess first 50 MS2 spectra\nprocessed_ms2 &lt;- preprocess_ms2_spectrum(ms2_data[1:50])\n\n# Remove NULL entries\nprocessed_ms2 &lt;- processed_ms2[!sapply(processed_ms2, is.null)]\n\ncat(\"Processed\", length(processed_ms2), \"MS/MS spectra\\n\")\n\nProcessed 50 MS/MS spectra\n\nCode# Example: visualize a processed spectrum\nif (length(processed_ms2) &gt; 0) {\n  example_spectrum &lt;- processed_ms2[[1]]\n  \n  spectrum_df &lt;- data.frame(\n    mz = example_spectrum$mz,\n    intensity = example_spectrum$intensity\n  )\n  \n  ggplot(spectrum_df, aes(x = mz, y = intensity)) +\n    geom_segment(aes(xend = mz, yend = 0), color = \"blue\", alpha = 0.7) +\n    labs(title = paste(\"Processed MS/MS Spectrum - Precursor m/z:\", \n                      round(example_spectrum$precursor_mz, 2)),\n         x = \"m/z\", y = \"Relative Intensity (%)\") +\n    theme_minimal()\n}",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-proteomics-analysis.html#protein-identification",
    "href": "09-proteomics-analysis.html#protein-identification",
    "title": "\n10  Proteomics Data Analysis\n",
    "section": "\n10.4 Protein Identification",
    "text": "10.4 Protein Identification\n\n10.4.1 Peptide Spectral Matching\n\nCode# Simulate protein database and peptide identification results\ncreate_protein_database &lt;- function() {\n  # Create a simplified protein database\n  proteins &lt;- data.frame(\n    protein_id = paste0(\"PROT_\", 1:100),\n    protein_name = paste0(\"Protein_\", 1:100),\n    gene_name = paste0(\"GENE_\", 1:100),\n    organism = \"Homo sapiens\",\n    sequence_length = sample(100:2000, 100),\n    stringsAsFactors = FALSE\n  )\n  \n  # Generate theoretical peptides for each protein\n  peptide_db &lt;- data.frame()\n  peptide_counter &lt;- 1\n  \n  for (i in 1:nrow(proteins)) {\n    # Simulate 5-15 peptides per protein\n    n_peptides &lt;- sample(5:15, 1)\n    \n    for (j in 1:n_peptides) {\n      # Generate random peptide sequence (simplified)\n      aa_codes &lt;- c(\"A\", \"R\", \"N\", \"D\", \"C\", \"E\", \"Q\", \"G\", \"H\", \"I\", \n                   \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\")\n      peptide_length &lt;- sample(7:25, 1)\n      sequence &lt;- paste(sample(aa_codes, peptide_length, replace = TRUE), collapse = \"\")\n      \n      # Calculate theoretical m/z (simplified calculation)\n      theoretical_mass &lt;- peptide_length * 110  # Rough average AA mass\n      charge &lt;- sample(2:4, 1)\n      theoretical_mz &lt;- (theoretical_mass + charge * 1.007276) / charge\n      \n      peptide_db &lt;- rbind(peptide_db, data.frame(\n        peptide_id = paste0(\"PEP_\", sprintf(\"%04d\", peptide_counter)),\n        protein_id = proteins$protein_id[i],\n        sequence = sequence,\n        theoretical_mz = theoretical_mz,\n        charge = charge,\n        peptide_counter = peptide_counter\n      ))\n      \n      peptide_counter &lt;- peptide_counter + 1\n    }\n  }\n  \n  return(list(proteins = proteins, peptides = peptide_db))\n}\n\ndb_info &lt;- create_protein_database()\ncat(\"Created database with\", nrow(db_info$proteins), \"proteins and\", \n    nrow(db_info$peptides), \"peptides\\n\")\n\nCreated database with 100 proteins and 995 peptides\n\n\n\n10.4.2 Simulate Peptide-Spectrum Matches (PSMs)\n\nCode# Simulate PSM results\nsimulate_psm_results &lt;- function(processed_spectra, peptide_db, match_probability = 0.3) {\n  psm_results &lt;- data.frame()\n  \n  for (i in seq_along(processed_spectra)) {\n    spectrum &lt;- processed_spectra[[i]]\n    \n    # Simulate whether this spectrum gets identified\n    if (runif(1) &lt; match_probability) {\n      # Find potential peptide matches based on precursor m/z\n      mz_tolerance &lt;- 0.01  # 10 ppm at m/z 1000\n      \n      potential_matches &lt;- which(\n        abs(peptide_db$theoretical_mz - spectrum$precursor_mz) &lt; mz_tolerance &\n        peptide_db$charge == spectrum$precursor_charge\n      )\n      \n      if (length(potential_matches) &gt; 0) {\n        # Select best match (random for simulation)\n        best_match &lt;- sample(potential_matches, 1)\n        \n        # Simulate scoring metrics\n        xcorr_score &lt;- runif(1, 1.5, 4.5)\n        delta_cn &lt;- runif(1, 0.1, 0.8)\n        sp_score &lt;- sample(200:800, 1)\n        mass_error_ppm &lt;- runif(1, -5, 5)\n        \n        # Calculate q-value based on score (simplified)\n        q_value &lt;- 1 / (1 + exp((xcorr_score - 2) * 3))  # Sigmoid function\n        \n        psm_results &lt;- rbind(psm_results, data.frame(\n          spectrum_index = i,\n          scan_number = i,\n          peptide_id = peptide_db$peptide_id[best_match],\n          protein_id = peptide_db$protein_id[best_match],\n          sequence = peptide_db$sequence[best_match],\n          charge = spectrum$precursor_charge,\n          theoretical_mz = peptide_db$theoretical_mz[best_match],\n          observed_mz = spectrum$precursor_mz,\n          mass_error_ppm = mass_error_ppm,\n          retention_time = spectrum$retention_time,\n          xcorr = xcorr_score,\n          delta_cn = delta_cn,\n          sp_score = sp_score,\n          q_value = q_value\n        ))\n      }\n    }\n  }\n  \n  return(psm_results)\n}\n\n# Generate PSM results\npsm_results &lt;- simulate_psm_results(processed_ms2, db_info$peptides)\ncat(\"Generated\", nrow(psm_results), \"PSMs\\n\")\n\nGenerated 0 PSMs\n\nCodeif (nrow(psm_results) &gt; 0) {\n  head(psm_results)\n}\n\n\n\n10.4.3 PSM Quality Assessment and Filtering\n\nCode# Assess PSM quality\nassess_psm_quality &lt;- function(psm_data) {\n  # Quality distribution plots\n  quality_plots &lt;- list()\n  \n  # XCorr distribution\n  quality_plots[[1]] &lt;- ggplot(psm_data, aes(x = xcorr)) +\n    geom_histogram(bins = 30, fill = \"lightblue\", alpha = 0.7) +\n    labs(title = \"XCorr Score Distribution\", x = \"XCorr\", y = \"Count\") +\n    theme_minimal()\n  \n  # q-value distribution\n  quality_plots[[2]] &lt;- ggplot(psm_data, aes(x = q_value)) +\n    geom_histogram(bins = 30, fill = \"lightcoral\", alpha = 0.7) +\n    scale_x_log10() +\n    labs(title = \"Q-value Distribution\", x = \"Q-value (log10)\", y = \"Count\") +\n    theme_minimal()\n  \n  # Mass error distribution\n  quality_plots[[3]] &lt;- ggplot(psm_data, aes(x = mass_error_ppm)) +\n    geom_histogram(bins = 30, fill = \"lightgreen\", alpha = 0.7) +\n    labs(title = \"Mass Error Distribution\", x = \"Mass Error (ppm)\", y = \"Count\") +\n    theme_minimal()\n  \n  return(quality_plots)\n}\n\nif (nrow(psm_results) &gt; 0) {\n  quality_plots &lt;- assess_psm_quality(psm_results)\n  print(quality_plots[[1]])\n  print(quality_plots[[2]])\n  print(quality_plots[[3]])\n}\n\n\n\n10.4.4 PSM Filtering and FDR Control\n\nCode# Apply PSM filters\nfilter_psms &lt;- function(psm_data, \n                       xcorr_threshold = 2.0,\n                       qvalue_threshold = 0.01,\n                       mass_error_threshold = 10) {\n  \n  # Apply filters\n  filtered_psms &lt;- psm_data %&gt;%\n    filter(\n      xcorr &gt;= xcorr_threshold,\n      q_value &lt;= qvalue_threshold,\n      abs(mass_error_ppm) &lt;= mass_error_threshold\n    )\n  \n  cat(\"PSM filtering results:\\n\")\n  cat(\"  Original PSMs:\", nrow(psm_data), \"\\n\")\n  cat(\"  XCorr filter (&gt;=\", xcorr_threshold, \"):\", \n      sum(psm_data$xcorr &gt;= xcorr_threshold), \"\\n\")\n  cat(\"  Q-value filter (&lt;=\", qvalue_threshold, \"):\", \n      sum(psm_data$q_value &lt;= qvalue_threshold), \"\\n\")\n  cat(\"  Mass error filter (&lt;=\", mass_error_threshold, \"ppm):\", \n      sum(abs(psm_data$mass_error_ppm) &lt;= mass_error_threshold), \"\\n\")\n  cat(\"  Final filtered PSMs:\", nrow(filtered_psms), \"\\n\")\n  cat(\"  PSM-level FDR:\", round(mean(filtered_psms$q_value) * 100, 2), \"%\\n\")\n  \n  return(filtered_psms)\n}\n\nif (nrow(psm_results) &gt; 0) {\n  filtered_psms &lt;- filter_psms(psm_results)\n}",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-proteomics-analysis.html#protein-inference-and-quantification",
    "href": "09-proteomics-analysis.html#protein-inference-and-quantification",
    "title": "\n10  Proteomics Data Analysis\n",
    "section": "\n10.5 Protein Inference and Quantification",
    "text": "10.5 Protein Inference and Quantification\n\n10.5.1 Protein Grouping\n\nCode# Perform protein inference\nprotein_inference &lt;- function(filtered_psms, protein_db) {\n  if (nrow(filtered_psms) == 0) {\n    return(data.frame())\n  }\n  \n  # Group PSMs by protein\n  protein_groups &lt;- filtered_psms %&gt;%\n    group_by(protein_id) %&gt;%\n    summarise(\n      peptide_count = n_distinct(sequence),\n      psm_count = n(),\n      unique_peptide_count = n_distinct(sequence),  # Simplified - assume all peptides are unique\n      sequence_coverage = peptide_count * 10,  # Rough estimate\n      best_xcorr = max(xcorr),\n      mean_mass_error = mean(mass_error_ppm),\n      .groups = 'drop'\n    ) %&gt;%\n    filter(peptide_count &gt;= 2)  # Require at least 2 peptides\n  \n  # Add protein information\n  protein_groups &lt;- protein_groups %&gt;%\n    left_join(protein_db, by = \"protein_id\")\n  \n  return(protein_groups)\n}\n\nif (exists(\"filtered_psms\") && nrow(filtered_psms) &gt; 0) {\n  protein_groups &lt;- protein_inference(filtered_psms, db_info$proteins)\n  cat(\"Identified\", nrow(protein_groups), \"protein groups\\n\")\n  \n  if (nrow(protein_groups) &gt; 0) {\n    head(protein_groups)\n  }\n}\n\n\n\n10.5.2 Label-Free Quantification\n\nCode# Simulate label-free quantification data\nsimulate_lfq_data &lt;- function(protein_groups, n_samples = 12) {\n  if (nrow(protein_groups) == 0) {\n    return(list())\n  }\n  \n  # Create sample information\n  sample_info &lt;- data.frame(\n    sample_id = paste0(\"Sample_\", 1:n_samples),\n    condition = rep(c(\"Control\", \"Treatment\"), each = n_samples/2),\n    batch = rep(1:3, each = n_samples/3),\n    injection_order = 1:n_samples\n  )\n  \n  # Create intensity matrix\n  intensity_matrix &lt;- matrix(0, nrow = nrow(protein_groups), ncol = n_samples)\n  rownames(intensity_matrix) &lt;- protein_groups$protein_id\n  colnames(intensity_matrix) &lt;- sample_info$sample_id\n  \n  # Simulate protein abundances\n  for (i in 1:nrow(protein_groups)) {\n    base_abundance &lt;- rlnorm(1, meanlog = 20, sdlog = 2)\n    \n    for (j in 1:n_samples) {\n      # Add condition effect for some proteins\n      condition_effect &lt;- ifelse(sample_info$condition[j] == \"Treatment\" & \n                                i &lt;= nrow(protein_groups) * 0.2, \n                                log2(1.5), 0)  # 1.5-fold change for 20% of proteins\n      \n      # Add batch effect\n      batch_effect &lt;- rnorm(1, 0, 0.1) * sample_info$batch[j]\n      \n      # Add biological and technical variation\n      log_intensity &lt;- log2(base_abundance) + condition_effect + batch_effect + rnorm(1, 0, 0.3)\n      \n      # Convert back to linear scale with some probability of missing values\n      if (runif(1) &gt; 0.1) {  # 90% detection rate\n        intensity_matrix[i, j] &lt;- 2^log_intensity\n      }\n    }\n  }\n  \n  # Convert zero values to NA\n  intensity_matrix[intensity_matrix == 0] &lt;- NA\n  \n  return(list(\n    intensity_matrix = intensity_matrix,\n    sample_info = sample_info,\n    protein_info = protein_groups\n  ))\n}\n\nif (exists(\"protein_groups\") && nrow(protein_groups) &gt; 0) {\n  lfq_data &lt;- simulate_lfq_data(protein_groups)\n  \n  cat(\"Created LFQ dataset:\\n\")\n  cat(\"  Proteins:\", nrow(lfq_data$intensity_matrix), \"\\n\")\n  cat(\"  Samples:\", ncol(lfq_data$intensity_matrix), \"\\n\")\n  cat(\"  Missing values:\", \n      round(sum(is.na(lfq_data$intensity_matrix)) / length(lfq_data$intensity_matrix) * 100, 1), \"%\\n\")\n}\n\n\n\n10.5.3 Data Normalization and Preprocessing\n\nCode# Normalize proteomics data\nnormalize_proteomics_data &lt;- function(intensity_matrix, method = \"median\") {\n  log_matrix &lt;- log2(intensity_matrix)\n  \n  if (method == \"median\") {\n    # Median normalization\n    sample_medians &lt;- apply(log_matrix, 2, median, na.rm = TRUE)\n    global_median &lt;- median(sample_medians, na.rm = TRUE)\n    normalization_factors &lt;- global_median - sample_medians\n    \n    for (i in 1:ncol(log_matrix)) {\n      log_matrix[, i] &lt;- log_matrix[, i] + normalization_factors[i]\n    }\n    \n  } else if (method == \"quantile\") {\n    # Quantile normalization (simplified)\n    for (i in 1:ncol(log_matrix)) {\n      log_matrix[, i] &lt;- scale(log_matrix[, i])[, 1]\n    }\n  }\n  \n  return(2^log_matrix)  # Convert back to linear scale\n}\n\nif (exists(\"lfq_data\")) {\n  # Normalize data\n  normalized_intensities &lt;- normalize_proteomics_data(lfq_data$intensity_matrix)\n  \n  # Visualize normalization effect\n  # Before normalization\n  sample_medians_before &lt;- apply(log2(lfq_data$intensity_matrix), 2, median, na.rm = TRUE)\n  sample_medians_after &lt;- apply(log2(normalized_intensities), 2, median, na.rm = TRUE)\n  \n  normalization_df &lt;- data.frame(\n    sample = rep(colnames(lfq_data$intensity_matrix), 2),\n    median_intensity = c(sample_medians_before, sample_medians_after),\n    normalization = rep(c(\"Before\", \"After\"), each = length(sample_medians_before))\n  )\n  \n  ggplot(normalization_df, aes(x = sample, y = median_intensity, fill = normalization)) +\n    geom_bar(stat = \"identity\", position = \"dodge\", alpha = 0.7) +\n    labs(title = \"Effect of Median Normalization\",\n         x = \"Sample\", y = \"Median log2 Intensity\",\n         fill = \"Normalization\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n}",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-proteomics-analysis.html#differential-expression-analysis",
    "href": "09-proteomics-analysis.html#differential-expression-analysis",
    "title": "\n10  Proteomics Data Analysis\n",
    "section": "\n10.6 Differential Expression Analysis",
    "text": "10.6 Differential Expression Analysis\n\n10.6.1 Statistical Testing with limma\n\nCode# Perform differential expression analysis\nperform_limma_analysis &lt;- function(intensity_matrix, sample_info) {\n  # Convert to log2 scale\n  log_matrix &lt;- log2(intensity_matrix)\n  \n  # Create design matrix\n  condition &lt;- factor(sample_info$condition)\n  batch &lt;- factor(sample_info$batch)\n  design &lt;- model.matrix(~ 0 + condition + batch)\n  colnames(design)[1:2] &lt;- levels(condition)\n  \n  # Fit linear model\n  fit &lt;- lmFit(log_matrix, design)\n  \n  # Create contrast matrix\n  contrast_matrix &lt;- makeContrasts(\n    TreatmentVsControl = Treatment - Control,\n    levels = design\n  )\n  \n  # Apply contrasts\n  fit2 &lt;- contrasts.fit(fit, contrast_matrix)\n  fit2 &lt;- eBayes(fit2)\n  \n  # Extract results\n  results &lt;- topTable(fit2, coef = \"TreatmentVsControl\", \n                     number = Inf, adjust.method = \"BH\")\n  \n  return(list(fit = fit2, results = results))\n}\n\nif (exists(\"normalized_intensities\")) {\n  limma_results &lt;- perform_limma_analysis(normalized_intensities, lfq_data$sample_info)\n  \n  cat(\"Differential expression results:\\n\")\n  cat(\"  Significant proteins (p &lt; 0.05):\", \n      sum(limma_results$results$P.Value &lt; 0.05, na.rm = TRUE), \"\\n\")\n  cat(\"  Significant proteins (FDR &lt; 0.05):\", \n      sum(limma_results$results$adj.P.Val &lt; 0.05, na.rm = TRUE), \"\\n\")\n  \n  head(limma_results$results)\n}\n\n\n\n10.6.2 Volcano Plot\n\nCode# Create volcano plot\nif (exists(\"limma_results\")) {\n  volcano_data &lt;- limma_results$results %&gt;%\n    mutate(\n      protein_id = rownames(.),\n      significant = adj.P.Val &lt; 0.05 & abs(logFC) &gt; log2(1.2),\n      direction = case_when(\n        logFC &gt; log2(1.2) & adj.P.Val &lt; 0.05 ~ \"Up\",\n        logFC &lt; -log2(1.2) & adj.P.Val &lt; 0.05 ~ \"Down\",\n        TRUE ~ \"NS\"\n      )\n    )\n  \n  ggplot(volcano_data, aes(x = logFC, y = -log10(P.Value))) +\n    geom_point(aes(color = direction), alpha = 0.7) +\n    scale_color_manual(values = c(\"Up\" = \"red\", \"Down\" = \"blue\", \"NS\" = \"gray\")) +\n    geom_hline(yintercept = -log10(0.05), linetype = \"dashed\") +\n    geom_vline(xintercept = c(-log2(1.2), log2(1.2)), linetype = \"dashed\") +\n    labs(title = \"Volcano Plot - Proteomics Differential Expression\",\n         x = \"log2 Fold Change\", y = \"-log10 P-value\",\n         color = \"Regulation\") +\n    theme_minimal()\n}\n\n\n\n10.6.3 Protein Set Analysis\n\nCode# Simulate gene ontology enrichment\nsimulate_go_enrichment &lt;- function(significant_proteins, all_proteins) {\n  # Create mock GO terms\n  go_terms &lt;- c(\"Protein Binding\", \"Metabolic Process\", \"Transport\", \n                \"Cell Division\", \"DNA Repair\", \"Signal Transduction\")\n  \n  enrichment_results &lt;- data.frame()\n  \n  for (go_term in go_terms) {\n    # Randomly assign proteins to GO terms\n    go_proteins &lt;- sample(all_proteins, size = sample(20:100, 1))\n    \n    # Calculate overlap with significant proteins\n    overlap &lt;- intersect(significant_proteins, go_proteins)\n    \n    # Fisher's exact test\n    contingency &lt;- matrix(c(\n      length(overlap),\n      length(significant_proteins) - length(overlap),\n      length(go_proteins) - length(overlap),\n      length(all_proteins) - length(significant_proteins) - \n        length(go_proteins) + length(overlap)\n    ), nrow = 2)\n    \n    fisher_result &lt;- fisher.test(contingency, alternative = \"greater\")\n    \n    enrichment_results &lt;- rbind(enrichment_results, data.frame(\n      go_term = go_term,\n      overlap_size = length(overlap),\n      go_size = length(go_proteins),\n      p_value = fisher_result$p.value,\n      odds_ratio = fisher_result$estimate\n    ))\n  }\n  \n  enrichment_results$adj_p_value &lt;- p.adjust(enrichment_results$p_value, method = \"BH\")\n  return(enrichment_results[order(enrichment_results$p_value), ])\n}\n\nif (exists(\"volcano_data\")) {\n  significant_proteins &lt;- volcano_data$protein_id[volcano_data$significant]\n  all_proteins &lt;- volcano_data$protein_id\n  \n  if (length(significant_proteins) &gt; 0) {\n    go_results &lt;- simulate_go_enrichment(significant_proteins, all_proteins)\n    \n    cat(\"GO enrichment analysis:\\n\")\n    print(go_results)\n    \n    # Plot enrichment results\n    ggplot(go_results, aes(x = reorder(go_term, -log10(p_value)), \n                          y = -log10(p_value))) +\n      geom_bar(stat = \"identity\", fill = \"steelblue\", alpha = 0.7) +\n      geom_hline(yintercept = -log10(0.05), linetype = \"dashed\", color = \"red\") +\n      coord_flip() +\n      labs(title = \"GO Term Enrichment Analysis\",\n           x = \"GO Term\", y = \"-log10 P-value\") +\n      theme_minimal()\n  }\n}",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-proteomics-analysis.html#data-visualization-and-reporting",
    "href": "09-proteomics-analysis.html#data-visualization-and-reporting",
    "title": "\n10  Proteomics Data Analysis\n",
    "section": "\n10.7 Data Visualization and Reporting",
    "text": "10.7 Data Visualization and Reporting\n\n10.7.1 Heat Map of Significant Proteins\n\nCode# Create heat map for significant proteins\nif (exists(\"volcano_data\") && exists(\"normalized_intensities\")) {\n  significant_proteins &lt;- volcano_data$protein_id[volcano_data$significant]\n  \n  if (length(significant_proteins) &gt; 5) {  # Need at least 5 proteins for meaningful heatmap\n    # Select top significant proteins\n    top_proteins &lt;- head(significant_proteins, 20)\n    heatmap_data &lt;- log2(normalized_intensities[top_proteins, ])\n    \n    # Create sample annotation\n    annotation_col &lt;- data.frame(\n      Condition = lfq_data$sample_info$condition,\n      Batch = factor(lfq_data$sample_info$batch),\n      row.names = lfq_data$sample_info$sample_id\n    )\n    \n    # Generate heat map\n    pheatmap(heatmap_data,\n             annotation_col = annotation_col,\n             scale = \"row\",\n             clustering_distance_rows = \"euclidean\",\n             clustering_distance_cols = \"euclidean\",\n             show_rownames = TRUE,\n             show_colnames = TRUE,\n             main = \"Heat Map of Significantly Changed Proteins\")\n  }\n}",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-proteomics-analysis.html#exercises",
    "href": "09-proteomics-analysis.html#exercises",
    "title": "\n10  Proteomics Data Analysis\n",
    "section": "\n10.8 Exercises",
    "text": "10.8 Exercises\n\nAnalyze real proteomics data from a public repository\nImplement different protein inference algorithms\nCompare various normalization methods for label-free quantification\nPerform time-course proteomics analysis\nIntegrate proteomics with other omics data types",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "09-proteomics-analysis.html#summary",
    "href": "09-proteomics-analysis.html#summary",
    "title": "\n10  Proteomics Data Analysis\n",
    "section": "\n10.9 Summary",
    "text": "10.9 Summary\nThis chapter covered comprehensive proteomics data analysis workflows, including MS/MS data processing, protein identification, quantification, and differential expression analysis. These methods are essential for extracting biological insights from bottom-up proteomics experiments.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Proteomics Data Analysis</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html",
    "href": "10-qfeatures-quantitative.html",
    "title": "\n11  Quantitative Proteomics with QFeatures\n",
    "section": "",
    "text": "11.1 Understanding Quantitative MS Data\nQuantitative proteomics involves the measurement and comparison of protein abundances across different conditions. This chapter introduces the QFeatures infrastructure for handling quantitative MS data.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#understanding-quantitative-ms-data",
    "href": "10-qfeatures-quantitative.html#understanding-quantitative-ms-data",
    "title": "\n11  Quantitative Proteomics with QFeatures\n",
    "section": "",
    "text": "11.1.1 Quantitation Methodologies\nThere are several approaches to quantitative proteomics, each with distinct advantages:\n\nCodelibrary(QFeatures)\nlibrary(tidyverse)\nlibrary(limma)\nlibrary(ggplot2)\nlibrary(pheatmap)\n\n\nLabel-free MS1: Extracted Ion Chromatograms (XIC)\nIn label-free quantitation, precursor peaks matching identified peptides are integrated over retention time.\nLabelled MS2: Isobaric Tagging (TMT/iTRAQ)\nIsobaric tags allow multiplexed quantitation where peptides from different samples are chemically labeled and analyzed together.\nLabel-free MS2: Spectral Counting\nSimple counting of peptide-spectrum matches assigned to each protein.\nLabelled MS1: SILAC\nStable isotope labeling allows direct comparison between heavy and light labeled samples.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#the-qfeatures-framework",
    "href": "10-qfeatures-quantitative.html#the-qfeatures-framework",
    "title": "\n11  Quantitative Proteomics with QFeatures\n",
    "section": "\n11.2 The QFeatures Framework",
    "text": "11.2 The QFeatures Framework\n\n11.2.1 QFeatures Class Structure\nQFeatures extends the MultiAssayExperiment class to handle the hierarchical nature of MS data (spectra → peptides → proteins).\n\nCodeflowchart TD\n    subgraph QF[\"QFeatures Object Structure\"]\n        direction TB\n        A[QFeatures Container] --&gt; B[colData&lt;br/&gt;Sample Metadata]\n        A --&gt; C[Assays&lt;br/&gt;Hierarchical Levels]\n        A --&gt; D[rowData&lt;br/&gt;Feature Annotations]\n        \n        C --&gt; E1[PSMs Assay&lt;br/&gt;Rows: 5000 PSMs&lt;br/&gt;Cols: 10 Samples]\n        C --&gt; E2[Peptides Assay&lt;br/&gt;Rows: 2500 Peptides&lt;br/&gt;Cols: 10 Samples]\n        C --&gt; E3[Proteins Assay&lt;br/&gt;Rows: 800 Proteins&lt;br/&gt;Cols: 10 Samples]\n        \n        E1 --&gt;|aggregateFeatures&lt;br/&gt;by Sequence| E2\n        E2 --&gt;|aggregateFeatures&lt;br/&gt;by Protein| E3\n    end\n    \n    subgraph Meta[\"Metadata Propagation\"]\n        F[Sample Info&lt;br/&gt;Condition, Batch, etc.] --&gt; B\n        G1[PSM Annotations&lt;br/&gt;Scores, RT, m/z] --&gt; D\n        G2[Peptide Info&lt;br/&gt;Sequence, Modifications] --&gt; D\n        G3[Protein Info&lt;br/&gt;Accession, Gene] --&gt; D\n    end\n    \n    subgraph Process[\"Data Processing\"]\n        E3 --&gt; H[filterNA&lt;br/&gt;Remove Missing]\n        H --&gt; I[normalize&lt;br/&gt;Median/Quantile]\n        I --&gt; J[impute&lt;br/&gt;KNN/MinProb]\n        J --&gt; K[logTransform&lt;br/&gt;log2]\n        K --&gt; L[limma Analysis&lt;br/&gt;Differential Expression]\n    end\n    \n  style QF fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style Meta fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n  style Process fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n\n\n\n\nflowchart TD\n    subgraph QF[\"QFeatures Object Structure\"]\n        direction TB\n        A[QFeatures Container] --&gt; B[colData&lt;br/&gt;Sample Metadata]\n        A --&gt; C[Assays&lt;br/&gt;Hierarchical Levels]\n        A --&gt; D[rowData&lt;br/&gt;Feature Annotations]\n        \n        C --&gt; E1[PSMs Assay&lt;br/&gt;Rows: 5000 PSMs&lt;br/&gt;Cols: 10 Samples]\n        C --&gt; E2[Peptides Assay&lt;br/&gt;Rows: 2500 Peptides&lt;br/&gt;Cols: 10 Samples]\n        C --&gt; E3[Proteins Assay&lt;br/&gt;Rows: 800 Proteins&lt;br/&gt;Cols: 10 Samples]\n        \n        E1 --&gt;|aggregateFeatures&lt;br/&gt;by Sequence| E2\n        E2 --&gt;|aggregateFeatures&lt;br/&gt;by Protein| E3\n    end\n    \n    subgraph Meta[\"Metadata Propagation\"]\n        F[Sample Info&lt;br/&gt;Condition, Batch, etc.] --&gt; B\n        G1[PSM Annotations&lt;br/&gt;Scores, RT, m/z] --&gt; D\n        G2[Peptide Info&lt;br/&gt;Sequence, Modifications] --&gt; D\n        G3[Protein Info&lt;br/&gt;Accession, Gene] --&gt; D\n    end\n    \n    subgraph Process[\"Data Processing\"]\n        E3 --&gt; H[filterNA&lt;br/&gt;Remove Missing]\n        H --&gt; I[normalize&lt;br/&gt;Median/Quantile]\n        I --&gt; J[impute&lt;br/&gt;KNN/MinProb]\n        J --&gt; K[logTransform&lt;br/&gt;log2]\n        K --&gt; L[limma Analysis&lt;br/&gt;Differential Expression]\n    end\n    \n  style QF fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style Meta fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n  style Process fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n\n\n\n\n\n\n\n\n\n\n\n\nQFeatures Advantages\n\n\n\n\n\nTraceability: Links between PSMs, peptides, and proteins maintained throughout\n\nFlexibility: Multiple assays can coexist (different processing strategies)\n\nMetadata: Sample and feature annotations travel with the data\n\nReproducibility: Complete processing pipeline encoded in object\n\n\n\n\nCode# Load example data\ndata(feat1)\nfeat1\n\nAn instance of class QFeatures containing 1 set(s):\n [1] psms: SummarizedExperiment with 10 rows and 2 columns \n\n\n\nCode# Examine the structure\ncolData(feat1)\n\nDataFrame with 2 rows and 1 column\n       Group\n   &lt;integer&gt;\nS1         1\nS2         2\n\n\n\nCode# Access the PSM-level assay\npsms_assay &lt;- feat1[[\"psms\"]]\npsms_assay\n\nclass: SummarizedExperiment \ndim: 10 2 \nmetadata(0):\nassays(1): ''\nrownames(10): PSM1 PSM2 ... PSM9 PSM10\nrowData names(5): Sequence Protein Var location pval\ncolnames(2): S1 S2\ncolData names(0):\n\n\n\nCode# View quantitative data\nassay(psms_assay)\n\n      S1 S2\nPSM1   1 11\nPSM2   2 12\nPSM3   3 13\nPSM4   4 14\nPSM5   5 15\nPSM6   6 16\nPSM7   7 17\nPSM8   8 18\nPSM9   9 19\nPSM10 10 20\n\n\n\nCode# Examine row annotations\nrowData(psms_assay)\n\nDataFrame with 10 rows and 5 columns\n           Sequence     Protein       Var      location      pval\n        &lt;character&gt; &lt;character&gt; &lt;integer&gt;   &lt;character&gt; &lt;numeric&gt;\nPSM1       SYGFNAAR       ProtA         1 Mitochondr...     0.084\nPSM2       SYGFNAAR       ProtA         2 Mitochondr...     0.077\nPSM3       SYGFNAAR       ProtA         3 Mitochondr...     0.063\nPSM4       ELGNDAYK       ProtA         4 Mitochondr...     0.073\nPSM5       ELGNDAYK       ProtA         5 Mitochondr...     0.012\nPSM6       ELGNDAYK       ProtA         6 Mitochondr...     0.011\nPSM7  IAEESNFPFI...       ProtB         7       unknown     0.075\nPSM8  IAEESNFPFI...       ProtB         8       unknown     0.038\nPSM9  IAEESNFPFI...       ProtB         9       unknown     0.028\nPSM10 IAEESNFPFI...       ProtB        10       unknown     0.097\n\n\n\n11.2.2 Feature Aggregation\nA key feature of QFeatures is the ability to aggregate features from lower to higher levels while maintaining traceability.\n\nCode# Aggregate PSMs to peptides based on sequence\nfeat1 &lt;- aggregateFeatures(feat1, \n                          i = \"psms\",\n                          fcol = \"Sequence\", \n                          name = \"peptides\",\n                          fun = colMeans)\nfeat1\n\nAn instance of class QFeatures containing 2 set(s):\n [1] psms: SummarizedExperiment with 10 rows and 2 columns \n [2] peptides: SummarizedExperiment with 3 rows and 2 columns \n\n\n\nCode# Examine the peptide-level data\nassay(feat1[[\"peptides\"]])\n\n             S1   S2\nELGNDAYK    5.0 15.0\nIAEESNFPFIK 8.5 18.5\nSYGFNAAR    2.0 12.0\n\n\n\nCode# Check aggregation statistics\nrowData(feat1[[\"peptides\"]])\n\nDataFrame with 3 rows and 4 columns\n                 Sequence     Protein      location        .n\n              &lt;character&gt; &lt;character&gt;   &lt;character&gt; &lt;integer&gt;\nELGNDAYK         ELGNDAYK       ProtA Mitochondr...         3\nIAEESNFPFIK IAEESNFPFI...       ProtB       unknown         4\nSYGFNAAR         SYGFNAAR       ProtA Mitochondr...         3\n\n\n\nCode# Aggregate peptides to proteins\nfeat1 &lt;- aggregateFeatures(feat1,\n                          i = \"peptides\", \n                          fcol = \"Protein\",\n                          name = \"proteins\",\n                          fun = colMedians)\nfeat1\n\nAn instance of class QFeatures containing 3 set(s):\n [1] psms: SummarizedExperiment with 10 rows and 2 columns \n [2] peptides: SummarizedExperiment with 3 rows and 2 columns \n [3] proteins: SummarizedExperiment with 2 rows and 2 columns \n\n\n\nCode# View final protein quantification\nassay(feat1[[\"proteins\"]])\n\n       S1   S2\nProtA 3.5 13.5\nProtB 8.5 18.5\n\n\n\n11.2.3 Subsetting and Filtering\nQFeatures maintains relationships between assays during subsetting operations.\n\nCode# Subset for a specific protein\nprotein_a &lt;- feat1[\"ProtA\", , ]\nprotein_a\n\nAn instance of class QFeatures containing 3 set(s):\n [1] psms: SummarizedExperiment with 6 rows and 2 columns \n [2] peptides: SummarizedExperiment with 2 rows and 2 columns \n [3] proteins: SummarizedExperiment with 1 rows and 2 columns \n\n\n\nCode# Filter features based on quality criteria\nfeat1_filtered &lt;- filterFeatures(feat1, ~ pval &lt; 0.05)\nfeat1_filtered\n\nAn instance of class QFeatures containing 3 set(s):\n [1] psms: SummarizedExperiment with 4 rows and 2 columns \n [2] peptides: SummarizedExperiment with 0 rows and 2 columns \n [3] proteins: SummarizedExperiment with 0 rows and 2 columns",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#working-with-real-data-cptac-dataset",
    "href": "10-qfeatures-quantitative.html#working-with-real-data-cptac-dataset",
    "title": "\n11  Quantitative Proteomics with QFeatures\n",
    "section": "\n11.3 Working with Real Data: CPTAC Dataset",
    "text": "11.3 Working with Real Data: CPTAC Dataset\n\n11.3.1 Data Import\n\nCodelibrary(MsDataHub)\n\n# Load CPTAC peptide data\n# Note: This is a simulated example based on the CPTAC study design\nset.seed(123)\n\n# Create sample metadata\nsample_info &lt;- data.frame(\n  sample = paste0(\"Sample_\", 1:6),\n  condition = rep(c(\"6A\", \"6B\"), each = 3),\n  replicate = rep(1:3, 2),\n  row.names = paste0(\"Sample_\", 1:6)\n)\n\n# Simulate peptide quantification data\nn_peptides &lt;- 1000\npeptide_data &lt;- matrix(\n  rlnorm(n_peptides * 6, meanlog = 10, sdlog = 1),\n  nrow = n_peptides,\n  ncol = 6,\n  dimnames = list(\n    paste0(\"Peptide_\", 1:n_peptides),\n    rownames(sample_info)\n  )\n)\n\n# Add differential expression signal\nde_peptides &lt;- 1:100  # First 100 peptides are differentially expressed\npeptide_data[de_peptides, 4:6] &lt;- peptide_data[de_peptides, 4:6] * 1.5\n\n# Create row annotations\npeptide_annotations &lt;- data.frame(\n  Sequence = paste0(\"SEQ\", 1:n_peptides),\n  Proteins = sample(paste0(\"PROT\", 1:200), n_peptides, replace = TRUE),\n  PEP = runif(n_peptides, 0, 0.1),\n  Score = runif(n_peptides, 20, 100),\n  row.names = rownames(peptide_data)\n)\n\n# Create SummarizedExperiment\nlibrary(SummarizedExperiment)\ncptac_se &lt;- SummarizedExperiment(\n  assays = list(peptides = peptide_data),\n  rowData = peptide_annotations,\n  colData = sample_info\n)\n\n# Create QFeatures object\ncptac_qf &lt;- QFeatures(list(peptides = cptac_se))\ncptac_qf\n\nAn instance of class QFeatures containing 1 set(s):\n [1] peptides: SummarizedExperiment with 1000 rows and 6 columns \n\n\n\n11.3.2 Data Preprocessing Pipeline\n\nCode# Log transformation\ncptac_qf &lt;- logTransform(cptac_qf, \n                        i = \"peptides\",\n                        name = \"log_peptides\")\n\n# Normalization (median centering)\ncptac_qf &lt;- normalize(cptac_qf,\n                     i = \"log_peptides\", \n                     name = \"norm_peptides\",\n                     method = \"center.median\")\n\ncptac_qf\n\nAn instance of class QFeatures containing 3 set(s):\n [1] peptides: SummarizedExperiment with 1000 rows and 6 columns \n [2] log_peptides: SummarizedExperiment with 1000 rows and 6 columns \n [3] norm_peptides: SummarizedExperiment with 1000 rows and 6 columns \n\n\n\n11.3.3 Missing Value Analysis\n\nCode# Introduce some missing values for demonstration\nassay_data &lt;- assay(cptac_qf[[\"norm_peptides\"]])\n# Set 10% of values to NA randomly\nmissing_indices &lt;- sample(length(assay_data), length(assay_data) * 0.1)\nassay_data[missing_indices] &lt;- NA\nassay(cptac_qf[[\"norm_peptides\"]]) &lt;- assay_data\n\n# Analyze missing value patterns\nna_stats &lt;- nNA(cptac_qf[[\"norm_peptides\"]])\ncat(\"Overall missing values:\", na_stats$nNA$pNA * 100, \"%\\n\")\n\nOverall missing values: 10 %\n\n\n\nCode# Visualize missing value patterns\nmissing_pattern &lt;- na_stats$nNArows\nhead(missing_pattern, 10)\n\nDataFrame with 10 rows and 3 columns\n          name       nNA       pNA\n   &lt;character&gt; &lt;integer&gt; &lt;numeric&gt;\n1    Peptide_1         1  0.166667\n2    Peptide_2         2  0.333333\n3    Peptide_3         1  0.166667\n4    Peptide_4         2  0.333333\n5    Peptide_5         2  0.333333\n6    Peptide_6         1  0.166667\n7    Peptide_7         1  0.166667\n8    Peptide_8         0  0.000000\n9    Peptide_9         1  0.166667\n10  Peptide_10         0  0.000000\n\n\n\nCode# Filter peptides with too many missing values\ncptac_qf_clean &lt;- filterNA(cptac_qf, i = \"norm_peptides\", pNA = 0.5)\ncat(\"Peptides after filtering:\", nrow(cptac_qf_clean[[\"norm_peptides\"]]), \"\\n\")\n\nPeptides after filtering: 1000 \n\n\n\n11.3.4 Protein Aggregation\n\nCode# Aggregate to protein level using median\ncptac_qf_clean &lt;- aggregateFeatures(cptac_qf_clean,\n                                   i = \"norm_peptides\",\n                                   fcol = \"Proteins\", \n                                   name = \"proteins\",\n                                   fun = colMedians,\n                                   na.rm = TRUE)\n\ncptac_qf_clean\n\nAn instance of class QFeatures containing 4 set(s):\n [1] peptides: SummarizedExperiment with 1000 rows and 6 columns \n [2] log_peptides: SummarizedExperiment with 1000 rows and 6 columns \n [3] norm_peptides: SummarizedExperiment with 1000 rows and 6 columns \n [4] proteins: SummarizedExperiment with 197 rows and 6 columns \n\n\n\nCode# Examine aggregation results\naggregation_stats &lt;- rowData(cptac_qf_clean[[\"proteins\"]])[\".n\"]\ntable(aggregation_stats)\n\n.n\n 1  2  3  4  5  6  7  8  9 10 11 13 14 \n 6 16 27 42 32 28 18 13  6  5  1  2  1",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#quality-control-and-visualization",
    "href": "10-qfeatures-quantitative.html#quality-control-and-visualization",
    "title": "\n11  Quantitative Proteomics with QFeatures\n",
    "section": "\n11.4 Quality Control and Visualization",
    "text": "11.4 Quality Control and Visualization\n\n11.4.1 Principal Component Analysis\n\nCodelibrary(factoextra)\n\n# PCA on peptide-level data\npeptide_pca &lt;- cptac_qf_clean[[\"norm_peptides\"]] %&gt;%\n  filterNA() %&gt;%\n  assay() %&gt;%\n  t() %&gt;%\n  prcomp(scale = TRUE, center = TRUE)\n\n# Create PCA plot\nfviz_pca_ind(peptide_pca, \n             habillage = colData(cptac_qf_clean)$condition,\n             title = \"Peptide-level PCA\")\n\n\n\n\n\n\n\n\nCode# PCA on protein-level data  \nprotein_pca &lt;- cptac_qf_clean[[\"proteins\"]] %&gt;%\n  filterNA() %&gt;%\n  assay() %&gt;%\n  t() %&gt;%\n  prcomp(scale = TRUE, center = TRUE)\n\nfviz_pca_ind(protein_pca,\n             habillage = colData(cptac_qf_clean)$condition, \n             title = \"Protein-level PCA\")\n\n\n\n\n\n\n\n\n11.4.2 Expression Profile Visualization\n\nCode# Extract data for a specific protein using longForm\nexample_protein &lt;- rownames(assay(cptac_qf_clean[[\"proteins\"]]))[1]\n\nprofile_data &lt;- longForm(cptac_qf_clean[example_protein, , \n                                       c(\"norm_peptides\", \"proteins\")]) %&gt;%\n  as_tibble()\n\n# Get column data and ensure unique column names\ncol_data &lt;- as_tibble(colData(cptac_qf_clean), rownames = \"sample_id\")\n\n# Join the data\nprofile_data &lt;- profile_data %&gt;%\n  left_join(col_data, by = c(\"colname\" = \"sample_id\"))\n\nggplot(profile_data, aes(x = colname, y = value, color = condition)) +\n  geom_point(size = 3) +\n  geom_line(aes(group = rowname)) +\n  facet_wrap(~assay, scales = \"free_y\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = paste(\"Expression Profile:\", example_protein),\n       x = \"Sample\", y = \"Normalized Intensity\")",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#statistical-analysis",
    "href": "10-qfeatures-quantitative.html#statistical-analysis",
    "title": "\n11  Quantitative Proteomics with QFeatures\n",
    "section": "\n11.5 Statistical Analysis",
    "text": "11.5 Statistical Analysis\n\n11.5.1 Differential Expression with limma\n\nCode# Extract protein data for statistical analysis\nprotein_data &lt;- getWithColData(cptac_qf_clean, \"proteins\")\n\n# Set up design matrix\ndesign &lt;- model.matrix(~ condition, data = colData(protein_data))\ncolnames(design) &lt;- c(\"Intercept\", \"Condition_6B_vs_6A\")\n\n# Fit linear model\nfit &lt;- lmFit(assay(protein_data), design)\nfit &lt;- eBayes(fit)\n\n# Extract results\nresults &lt;- topTable(fit, coef = \"Condition_6B_vs_6A\", number = Inf) %&gt;%\n  rownames_to_column(\"protein\") %&gt;%\n  as_tibble()\n\nhead(results)\n\n# A tibble: 6 × 7\n  protein logFC AveExpr     t P.Value adj.P.Val     B\n  &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 PROT157 -2.27   0.226 -3.88 0.00153     0.301 -1.33\n2 PROT137 -1.83   0.372 -3.24 0.00558     0.549 -2.22\n3 PROT140  1.70   0.859  2.94 0.0102      0.650 -2.64\n4 PROT188 -1.84  -0.551 -2.77 0.0144      0.650 -2.88\n5 PROT78   1.51  -0.224  2.71 0.0165      0.650 -2.97\n6 PROT187  1.93   0.220  2.46 0.0277      0.824 -3.36\n\n\n\nCode# Volcano plot\nresults %&gt;%\n  ggplot(aes(x = logFC, y = -log10(P.Value))) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = -log10(0.05), linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = c(-1, 1), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Volcano Plot\",\n       x = \"log2 Fold Change\", \n       y = \"-log10 P-value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCode# Summary of differential expression\nsignificant_proteins &lt;- results %&gt;%\n  filter(adj.P.Val &lt; 0.05, abs(logFC) &gt; 1)\n\ncat(\"Significantly changed proteins:\", nrow(significant_proteins), \"\\n\")\n\nSignificantly changed proteins: 0 \n\nCodecat(\"Up-regulated:\", sum(significant_proteins$logFC &gt; 1), \"\\n\")\n\nUp-regulated: 0 \n\nCodecat(\"Down-regulated:\", sum(significant_proteins$logFC &lt; -1), \"\\n\")\n\nDown-regulated: 0 \n\n\n\n11.5.2 Heatmap of Significant Proteins\n\nCodeif (nrow(significant_proteins) &gt; 5) {\n  # Select top 20 most significant proteins\n  top_proteins &lt;- head(significant_proteins, 20)$protein\n  \n  # Create heatmap data\n  heatmap_data &lt;- assay(protein_data)[top_proteins, ]\n  \n  # Sample annotations\n  annotation_col &lt;- data.frame(\n    Condition = colData(protein_data)$condition,\n    row.names = colnames(heatmap_data)\n  )\n  \n  # Generate heatmap\n  pheatmap(heatmap_data,\n           annotation_col = annotation_col,\n           scale = \"row\",\n           clustering_distance_rows = \"euclidean\",\n           clustering_distance_cols = \"euclidean\",\n           main = \"Top Differentially Expressed Proteins\")\n}",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#advanced-aggregation-methods",
    "href": "10-qfeatures-quantitative.html#advanced-aggregation-methods",
    "title": "\n11  Quantitative Proteomics with QFeatures\n",
    "section": "\n11.6 Advanced Aggregation Methods",
    "text": "11.6 Advanced Aggregation Methods\n\n11.6.1 Robust Summarization\n\nCodelibrary(MsCoreUtils)\n\n# Robust aggregation using robust summarization\ncptac_robust &lt;- aggregateFeatures(cptac_qf_clean,\n                                 i = \"norm_peptides\",\n                                 fcol = \"Proteins\",\n                                 name = \"proteins_robust\", \n                                 fun = MsCoreUtils::robustSummary,\n                                 na.rm = TRUE)\n\n# Compare standard vs robust aggregation\ncomparison_data &lt;- data.frame(\n  standard = assay(cptac_qf_clean[[\"proteins\"]])[, 1],\n  robust = assay(cptac_robust[[\"proteins_robust\"]])[, 1]\n) %&gt;%\n  na.omit()\n\nggplot(comparison_data, aes(x = standard, y = robust)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, color = \"red\") +\n  labs(title = \"Standard vs Robust Aggregation\",\n       x = \"Standard (Median)\",\n       y = \"Robust Summarization\") +\n  theme_minimal()",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#working-with-qfeatures-workflows",
    "href": "10-qfeatures-quantitative.html#working-with-qfeatures-workflows",
    "title": "\n11  Quantitative Proteomics with QFeatures\n",
    "section": "\n11.7 Working with QFeatures Workflows",
    "text": "11.7 Working with QFeatures Workflows\n\n11.7.1 Visualization of Data Relationships\n\nCode# Visualize the relationship between assays\nplot(cptac_robust)\n\n\n\n\n\n\n\n\n11.7.2 Custom Processing Functions\n\nCode# Example: Custom normalization using addAssay\nquantile_normalize &lt;- function(x) {\n  # Simple quantile normalization implementation\n  x_sorted &lt;- apply(x, 2, sort, na.last = TRUE)\n  x_mean &lt;- rowMeans(x_sorted, na.rm = TRUE)\n  \n  for (i in 1:ncol(x)) {\n    ranks &lt;- rank(x[, i], na.last = \"keep\")\n    x[, i] &lt;- x_mean[ranks]\n  }\n  return(x)\n}\n\n# Apply custom normalization\nlog_peptides_se &lt;- cptac_qf_clean[[\"log_peptides\"]]\nquantile_norm_assay &lt;- quantile_normalize(assay(log_peptides_se))\n\n# Create a new SummarizedExperiment with the normalized data\nlibrary(SummarizedExperiment)\nquantile_norm_se &lt;- SummarizedExperiment(\n  assays = list(quantile_norm = quantile_norm_assay),\n  rowData = rowData(log_peptides_se),\n  colData = colData(log_peptides_se)\n)\n\n# Add to QFeatures object\ncptac_custom &lt;- addAssay(cptac_qf_clean,\n                        quantile_norm_se,\n                        name = \"quantile_norm\")\n\n# Add assay link - for a transformation, use simple string-based linking\n# This indicates that all features in quantile_norm come from log_peptides\ncptac_custom &lt;- addAssayLink(cptac_custom,\n                            from = \"log_peptides\",\n                            to = \"quantile_norm\")\n\ncat(\"Custom normalization applied successfully\\n\")\n\nCustom normalization applied successfully\n\nCodecat(\"Available assays:\", names(cptac_custom), \"\\n\")\n\nAvailable assays: peptides log_peptides norm_peptides proteins quantile_norm",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#exercises",
    "href": "10-qfeatures-quantitative.html#exercises",
    "title": "\n11  Quantitative Proteomics with QFeatures\n",
    "section": "\n11.8 Exercises",
    "text": "11.8 Exercises\n\nLoad the CPTAC dataset and perform complete preprocessing pipeline\nCompare different aggregation methods (mean, median, robust)\nImplement missing value imputation strategies\nPerform differential expression analysis with multiple comparisons\nCreate custom visualization functions for QFeatures objects",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "10-qfeatures-quantitative.html#summary",
    "href": "10-qfeatures-quantitative.html#summary",
    "title": "\n11  Quantitative Proteomics with QFeatures\n",
    "section": "\n11.9 Summary",
    "text": "11.9 Summary\nThis chapter introduced the QFeatures framework for quantitative proteomics analysis. Key concepts covered include:\n\nDifferent quantitation methodologies in proteomics\nThe hierarchical structure of MS quantitative data\nFeature aggregation strategies\nQuality control and missing value handling\nStatistical analysis workflows\nVisualization of quantitative proteomics data\n\nThe QFeatures infrastructure provides a robust foundation for reproducible quantitative proteomics analysis in R.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Quantitative Proteomics with QFeatures</span>"
    ]
  },
  {
    "objectID": "11-advanced-topics.html",
    "href": "11-advanced-topics.html",
    "title": "\n12  Advanced Topics and Applications\n",
    "section": "",
    "text": "12.1 Advanced Spectra Backends\nThis chapter covers advanced techniques and specialized applications in mass spectrometry data analysis using R, including backend management, computational considerations, and specialized workflows inspired by the R for Mass Spectrometry ecosystem.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "11-advanced-topics.html#advanced-spectra-backends",
    "href": "11-advanced-topics.html#advanced-spectra-backends",
    "title": "\n12  Advanced Topics and Applications\n",
    "section": "",
    "text": "12.1.1 Understanding Backend Architecture\nThe Spectra package uses different backends to store and access MS data efficiently. Understanding these backends is crucial for handling large-scale datasets.\n\nCodelibrary(Spectra)\nlibrary(tidyverse)\nlibrary(BiocParallel)\n\n# Optional packages (load if available)\nif (requireNamespace(\"MsDataHub\", quietly = TRUE)) library(MsDataHub)\nif (requireNamespace(\"HDF5Array\", quietly = TRUE)) library(HDF5Array)\n\n\n\nCode# Load example data to demonstrate backends with error handling\nlibrary(msdata)\nms_file &lt;- msdata::proteomics(full.names = TRUE)[1]\n\n# Create Spectra with error handling for mzR compatibility\ntryCatch({\n  sps_mzr &lt;- Spectra(ms_file, backend = MsBackendMzR())\n  sps_mzr &lt;- setBackend(sps_mzr, backend = MsBackendDataFrame())\n  cat(\"Successfully loaded real MS data\\n\")\n}, error = function(e) {\n  cat(\"Note: Using synthetic data due to mzR compatibility issues\\n\\n\")\n  \n  # Create synthetic data\n  set.seed(123)\n  n_spectra &lt;- 100\n  \n  library(S4Vectors)\n  library(IRanges)\n  \n  mz_list &lt;- lapply(1:n_spectra, function(i) {\n    sort(runif(sample(50:150, 1), 150, 2000))\n  })\n  \n  intensity_list &lt;- lapply(mz_list, function(mz_vals) {\n    rlnorm(length(mz_vals), meanlog = 8, sdlog = 2)\n  })\n  \n  ms_levels &lt;- sample(1:2, n_spectra, replace = TRUE, prob = c(0.3, 0.7))\n  \n  spd &lt;- DataFrame(\n    msLevel = ms_levels,\n    rtime = seq(100, 3800, length.out = n_spectra),\n    acquisitionNum = 1:n_spectra,\n    precursorMz = ifelse(ms_levels == 1, NA_real_, runif(n_spectra, 400, 1500)),\n    precursorCharge = ifelse(ms_levels == 1, NA_integer_, sample(2:3, n_spectra, replace = TRUE)),\n    polarity = rep(1L, n_spectra),\n    dataOrigin = rep(\"synthetic_data.mzML\", n_spectra)\n  )\n  \n  spd$mz &lt;- NumericList(mz_list)\n  spd$intensity &lt;- NumericList(intensity_list)\n  \n  backend &lt;- MsBackendDataFrame()\n  backend &lt;- backendInitialize(backend, spd)\n  sps_mzr &lt;&lt;- Spectra(backend)\n})\n\nNote: Using synthetic data due to mzR compatibility issues\n\nCodecat(\"Backend class:\", class(sps_mzr@backend)[1], \"\\n\")\n\nBackend class: MsBackendDataFrame \n\nCodecat(\"Total spectra:\", length(sps_mzr), \"\\n\")\n\nTotal spectra: 100 \n\nCodecat(\"Data access type: In-memory\\n\")\n\nData access type: In-memory\n\n\nMsBackendMzR: On-disk Storage\n\nCode# Note: Demonstrating backend concepts with in-memory data\n# (MsBackendMzR requires compatible mzR version)\ncat(\"Backend information:\\n\")\n\nBackend information:\n\nCodecat(\"Backend class:\", class(sps_mzr@backend)[1], \"\\n\")\n\nBackend class: MsBackendDataFrame \n\nCodecat(\"Data origin:\", unique(dataOrigin(sps_mzr))[1], \"\\n\")\n\nData origin: synthetic_data.mzML \n\nCode# Data access demonstration\npeak_data &lt;- peaksData(sps_mzr[1:5])\ncat(\"\\nRetrieved peaks for 5 spectra\\n\")\n\n\nRetrieved peaks for 5 spectra\n\nCodecat(\"First spectrum has\", length(peak_data[[1]][,1]), \"peaks\\n\")\n\nFirst spectrum has 80 peaks\n\n\nMsBackendDataFrame: In-memory Storage\n\nCode# Our data is already in-memory using MsBackendDataFrame\ncat(\"Current backend class:\", class(sps_mzr@backend)[1], \"\\n\")\n\nCurrent backend class: MsBackendDataFrame \n\nCodecat(\"Data is in memory for fast access\\n\")\n\nData is in memory for fast access\n\nCode# Demonstrate fast access to data\nsystem.time({\n  for(i in 1:10) {\n    temp &lt;- intensity(sps_mzr[1:min(50, length(sps_mzr))])\n  }\n})\n\n   user  system elapsed \n   0.03    0.00    0.03 \n\nCodecat(\"\\nIn-memory backend provides fast repeated access\\n\")\n\n\nIn-memory backend provides fast repeated access\n\n\nMsBackendHdf5Peaks: HDF5 Storage\n\nCode# For very large datasets, HDF5 backend provides efficient storage\n# This is particularly useful for processed data\n# Note: MsBackendHdf5Peaks package not available in this environment\n\n# Conceptual demonstration (requires MsBackendHdf5Peaks package)\n# library(MsBackendHdf5Peaks)\n# hdf5_file &lt;- tempfile(fileext = \".h5\")\n# sps_hdf5 &lt;- setBackend(sps_mzr, \n#                       backend = MsBackendHdf5Peaks(),\n#                       hdf5path = hdf5_file)\n\ncat(\"HDF5 backend benefits:\\n\")\n\nHDF5 backend benefits:\n\nCodecat(\"- Efficient storage for large datasets\\n\")\n\n- Efficient storage for large datasets\n\nCodecat(\"- Fast partial data loading\\n\") \n\n- Fast partial data loading\n\nCodecat(\"- Cross-platform compatibility\\n\")\n\n- Cross-platform compatibility\n\nCodecat(\"- Reduced memory footprint\\n\")\n\n- Reduced memory footprint\n\nCodecat(\"\\nNote: Install with BiocManager::install('MsBackendHdf5Peaks')\\n\")\n\n\nNote: Install with BiocManager::install('MsBackendHdf5Peaks')",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "11-advanced-topics.html#computational-considerations",
    "href": "11-advanced-topics.html#computational-considerations",
    "title": "\n12  Advanced Topics and Applications\n",
    "section": "\n12.2 Computational Considerations",
    "text": "12.2 Computational Considerations\n\n12.2.1 Parallel Processing with BiocParallel\n\nCode# Configure parallel processing for large datasets\nlibrary(BiocParallel)\nlibrary(parallel)  # For detectCores()\n\n# Check available cores\ncat(\"Available cores:\", detectCores(), \"\\n\")\n\nAvailable cores: 24 \n\nCode# Set up parallel backend\nparam &lt;- MulticoreParam(workers = 2)  # Use 2 cores for demonstration\nregister(param)\n\n# Parallel processing example with spectral processing\nprocess_spectra_parallel &lt;- function(sps, param = bpparam()) {\n  # Example: Calculate TIC for all spectra in parallel\n  bplapply(seq_along(sps), function(i) {\n    sum(intensity(sps[i])[[1]], na.rm = TRUE)\n  }, BPPARAM = param)\n}\n\n# Use the available spectra data (sps_mzr)\nn_spectra &lt;- min(100, length(sps_mzr))\n\n# Time comparison - Sequential processing\nsystem.time({\n  tic_sequential &lt;- sapply(1:n_spectra, function(i) {\n    sum(intensity(sps_mzr[i])[[1]], na.rm = TRUE)\n  })\n})\n\n   user  system elapsed \n   0.25    0.02    0.28 \n\nCode# Parallel processing\nsystem.time({\n  tic_parallel &lt;- unlist(process_spectra_parallel(sps_mzr[1:n_spectra], param))\n})\n\n   user  system elapsed \n   0.25    0.00    0.27 \n\nCodecat(\"Parallel processing can significantly speed up large-scale operations\\n\")\n\nParallel processing can significantly speed up large-scale operations\n\n\n\n12.2.2 Memory Management Strategies\n\nCode# Memory monitoring functions\ncheck_memory_usage &lt;- function(label = \"\") {\n  if (label != \"\") cat(label, \":\\n\")\n  mem_used &lt;- sum(sapply(ls(envir = .GlobalEnv), function(x) {\n    object.size(get(x, envir = .GlobalEnv))\n  }))\n  cat(\"  Memory used:\", format(mem_used, units = \"MB\"), \"\\n\")\n}\n\n# Demonstrate memory efficient workflows\nlarge_dataset_workflow &lt;- function(sps_data) {\n  # Process spectra in batches without loading all into memory at once\n  batch_size &lt;- 20\n  n_spectra &lt;- length(sps_data)\n  n_batches &lt;- ceiling(n_spectra / batch_size)\n  \n  results &lt;- list()\n  \n  for (i in 1:n_batches) {\n    start_idx &lt;- (i - 1) * batch_size + 1\n    end_idx &lt;- min(i * batch_size, n_spectra)\n    \n    # Process batch\n    batch_sps &lt;- sps_data[start_idx:end_idx]\n    \n    # Extract summary statistics\n    results[[i]] &lt;- data.frame(\n      batch = i,\n      n_spectra = length(batch_sps),\n      rt_range = paste(round(range(rtime(batch_sps)), 2), collapse = \"-\"),\n      ms_levels = paste(unique(msLevel(batch_sps)), collapse = \",\"),\n      mean_peaks = round(mean(lengths(mz(batch_sps))))\n    )\n    \n    # Explicitly remove batch objects\n    rm(batch_sps)\n    gc()  # Force garbage collection\n  }\n  \n  do.call(rbind, results)\n}\n\n# Example usage with existing data\nresult &lt;- large_dataset_workflow(sps_mzr)\nprint(result)\n\n  batch n_spectra        rt_range ms_levels mean_peaks\n1     1        20       100-810.1       2,1         97\n2     2        20  847.47-1557.58       1,2        104\n3     3        20 1594.95-2305.05       2,1        103\n4     4        20 2342.42-3052.53       2,1        106\n5     5        20     3089.9-3800       2,1        106\n\nCodecat(\"\\nBatch processing helps manage memory for large datasets\\n\")\n\n\nBatch processing helps manage memory for large datasets",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "11-advanced-topics.html#advanced-spectral-processing",
    "href": "11-advanced-topics.html#advanced-spectral-processing",
    "title": "\n12  Advanced Topics and Applications\n",
    "section": "\n12.3 Advanced Spectral Processing",
    "text": "12.3 Advanced Spectral Processing\n\n12.3.1 Custom Backend Development\n\nCode# Example of extending Spectra with custom functionality\ncustom_spectral_processing &lt;- function(sps) {\n  # Custom processing pipeline\n  processed_sps &lt;- sps\n  \n  # 1. Noise reduction with Savitzky-Golay smoothing\n  processed_sps &lt;- smooth(processed_sps, method = \"SavitzkyGolay\")\n  \n  # 2. Peak picking with custom parameters\n  processed_sps &lt;- pickPeaks(processed_sps, \n                            method = \"MAD\",\n                            snr = 3)\n  \n  # 3. Normalize intensities by dividing by max intensity\n  processed_sps &lt;- addProcessing(processed_sps, function(x) {\n    x[, \"intensity\"] &lt;- x[, \"intensity\"] / max(x[, \"intensity\"], na.rm = TRUE)\n    x\n  })\n  \n  return(processed_sps)\n}\n\n# Apply custom processing\nprocessed_spectra &lt;- custom_spectral_processing(sps_mzr[1:10])\ncat(\"Applied custom processing pipeline to\", length(processed_spectra), \"spectra\\n\")\n\nApplied custom processing pipeline to 10 spectra\n\nCodecat(\"Processing steps include: smoothing, peak picking, and normalization\\n\")\n\nProcessing steps include: smoothing, peak picking, and normalization\n\n\n\n12.3.2 Spectral Similarity Networks\n\nCode# Advanced spectral comparison and networking\nlibrary(igraph)\n\ncalculate_spectral_similarity_matrix &lt;- function(sps, method = \"cosine\") {\n  n &lt;- length(sps)\n  similarity_matrix &lt;- matrix(0, nrow = n, ncol = n)\n  \n  for (i in 1:n) {\n    for (j in i:n) {\n      if (i == j) {\n        similarity_matrix[i, j] &lt;- 1.0\n      } else {\n        # Calculate pairwise similarity\n        sim &lt;- compareSpectra(sps[i], sps[j], \n                             FUN = MsCoreUtils::gnps,\n                             ppm = 20)\n        similarity_matrix[i, j] &lt;- sim\n        similarity_matrix[j, i] &lt;- sim\n      }\n    }\n  }\n  \n  return(similarity_matrix)\n}\n\n# Create similarity network (with subset for demonstration)\nsubset_spectra &lt;- sps_mzr[1:20]\nsim_matrix &lt;- calculate_spectral_similarity_matrix(subset_spectra)\n\n# Create network from similarity matrix\nthreshold &lt;- 0.6\nadjacency_matrix &lt;- ifelse(sim_matrix &gt;= threshold, 1, 0)\ndiag(adjacency_matrix) &lt;- 0  # Remove self-loops\n\nnetwork &lt;- graph_from_adjacency_matrix(adjacency_matrix, mode = \"undirected\")\n\n# Network statistics\ncat(\"Network statistics:\\n\")\n\nNetwork statistics:\n\nCodecat(\"  Nodes:\", vcount(network), \"\\n\")\n\n  Nodes: 20 \n\nCodecat(\"  Edges:\", ecount(network), \"\\n\")\n\n  Edges: 0 \n\nCodecat(\"  Connected components:\", components(network)$no, \"\\n\")\n\n  Connected components: 20 \n\nCode# Visualize network (if small enough)\nif (vcount(network) &lt;= 50) {\n  plot(network,\n       vertex.size = 8,\n       vertex.color = \"lightblue\",\n       edge.width = 2,\n       main = \"Spectral Similarity Network\")\n}",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "11-advanced-topics.html#integration-with-external-tools",
    "href": "11-advanced-topics.html#integration-with-external-tools",
    "title": "\n12  Advanced Topics and Applications\n",
    "section": "\n12.4 Integration with External Tools",
    "text": "12.4 Integration with External Tools\n\n12.4.1 Connecting to Online Resources\n\nCode# Example: Integration with online databases\naccess_online_resources &lt;- function() {\n  cat(\"Strategies for integrating external resources:\\n\\n\")\n  \n  cat(\"1. GNPS (Global Natural Products Social Molecular Networking):\\n\")\n  cat(\"   - Use GNPS REST API for spectral library matching\\n\")\n  cat(\"   - Export data in GNPS-compatible formats\\n\\n\")\n  \n  cat(\"2. MassBank:\\n\") \n  cat(\"   - Access curated reference spectra\\n\")\n  cat(\"   - Use RMassBank for compound identification\\n\\n\")\n  \n  cat(\"3. ChemSpider/PubChem:\\n\")\n  cat(\"   - Retrieve compound information\\n\")\n  cat(\"   - Use webchem package for programmatic access\\n\\n\")\n  \n  cat(\"4. MetaboLights/PRIDE:\\n\")\n  cat(\"   - Access public datasets\\n\")\n  cat(\"   - Use appropriate R packages for data retrieval\\n\")\n}\n\naccess_online_resources()\n\nStrategies for integrating external resources:\n\n1. GNPS (Global Natural Products Social Molecular Networking):\n   - Use GNPS REST API for spectral library matching\n   - Export data in GNPS-compatible formats\n\n2. MassBank:\n   - Access curated reference spectra\n   - Use RMassBank for compound identification\n\n3. ChemSpider/PubChem:\n   - Retrieve compound information\n   - Use webchem package for programmatic access\n\n4. MetaboLights/PRIDE:\n   - Access public datasets\n   - Use appropriate R packages for data retrieval\n\n\n\n12.4.2 Export and Interoperability\n\nCode# Data export functions for different formats\nexport_spectra_formats &lt;- function(sps, base_name) {\n  # 1. Export to mzML (standard format)\n  # export(sps, file = paste0(base_name, \".mzML\"))\n  \n  # 2. Export to MGF (for GNPS)\n  # export(sps, file = paste0(base_name, \".mgf\"))\n  \n  # 3. Export metadata as CSV\n  metadata_df &lt;- spectraData(sps) %&gt;%\n    as.data.frame() %&gt;%\n    select(msLevel, rtime, precursorMz, precursorCharge, collisionEnergy)\n  \n  write.csv(metadata_df, file = paste0(base_name, \"_metadata.csv\"), row.names = FALSE)\n  \n  cat(\"Exported spectra in multiple formats:\\n\")\n  cat(\"  - Metadata: CSV format\\n\")\n  cat(\"  - Spectral data: mzML/MGF (commented out)\\n\")\n}\n\n# Example export (metadata only)\nexport_spectra_formats(subset_spectra, \"example_export\")\n\nExported spectra in multiple formats:\n  - Metadata: CSV format\n  - Spectral data: mzML/MGF (commented out)",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "11-advanced-topics.html#machine-learning-integration",
    "href": "11-advanced-topics.html#machine-learning-integration",
    "title": "\n12  Advanced Topics and Applications\n",
    "section": "\n12.5 Machine Learning Integration",
    "text": "12.5 Machine Learning Integration\n\n12.5.1 Feature Engineering for ML\n\nCode# Create synthetic feature matrix and metadata for ML demonstration\nset.seed(123)\nn_samples &lt;- 50\nn_features &lt;- 100\n\n# Create synthetic feature matrix\nspectral_features &lt;- matrix(\n  rlnorm(n_samples * n_features, meanlog = 5, sdlog = 2),\n  nrow = n_samples,\n  ncol = n_features\n)\ncolnames(spectral_features) &lt;- paste0(\"feature_\", 1:n_features)\nrownames(spectral_features) &lt;- paste0(\"sample_\", 1:n_samples)\n\n# Create sample metadata\nsample_metadata &lt;- data.frame(\n  sample_id = paste0(\"sample_\", 1:n_samples),\n  class = factor(rep(c(\"Healthy\", \"Disease\"), each = n_samples/2)),\n  batch = factor(rep(1:5, length.out = n_samples))\n)\n\ncat(\"Created synthetic dataset:\\n\")\n\nCreated synthetic dataset:\n\nCodecat(\"  Samples:\", n_samples, \"\\n\")\n\n  Samples: 50 \n\nCodecat(\"  Features:\", n_features, \"\\n\")\n\n  Features: 100 \n\nCodecat(\"  Classes:\", paste(levels(sample_metadata$class), collapse = \", \"), \"\\n\")\n\n  Classes: Disease, Healthy \n\n\n\nCode# Feature preprocessing\npreprocess_features &lt;- function(features, metadata) {\n  # Log transform\n  log_features &lt;- log2(features + 1)\n  \n  # Normalize by sample (row-wise)\n  normalized_features &lt;- t(scale(t(log_features)))\n  \n  # Handle missing values\n  normalized_features[is.na(normalized_features)] &lt;- 0\n  \n  return(normalized_features)\n}\n\n# Apply preprocessing\nprocessed_features &lt;- preprocess_features(spectral_features, sample_metadata)\ncat(\"Preprocessed features: log2 transformation and normalization\\n\")\n\nPreprocessed features: log2 transformation and normalization\n\n\n\nCode# Feature selection using statistical tests\nfeature_selection_stats &lt;- function(features, classes) {\n  p_values &lt;- numeric(ncol(features))\n  fold_changes &lt;- numeric(ncol(features))\n  \n  for (i in 1:ncol(features)) {\n    healthy_vals &lt;- features[classes == \"Healthy\", i]\n    disease_vals &lt;- features[classes == \"Disease\", i]\n    \n    # t-test\n    test_result &lt;- t.test(disease_vals, healthy_vals)\n    p_values[i] &lt;- test_result$p.value\n    \n    # Fold change\n    fold_changes[i] &lt;- log2(mean(disease_vals) / mean(healthy_vals))\n  }\n  \n  # Multiple testing correction\n  adj_p_values &lt;- p.adjust(p_values, method = \"fdr\")\n  \n  feature_stats &lt;- data.frame(\n    feature = colnames(features),\n    p_value = p_values,\n    adj_p_value = adj_p_values,\n    log2_fc = fold_changes,\n    significant = adj_p_values &lt; 0.05\n  )\n  \n  return(feature_stats[order(feature_stats$adj_p_value), ])\n}\n\nfeature_stats &lt;- feature_selection_stats(processed_features, sample_metadata$class)\n\n# Visualize feature selection results\nggplot(feature_stats, aes(x = log2_fc, y = -log10(p_value))) +\n  geom_point(aes(color = significant), alpha = 0.7) +\n  scale_color_manual(values = c(\"FALSE\" = \"gray\", \"TRUE\" = \"red\")) +\n  labs(title = \"Feature Selection - Volcano Plot\",\n       x = \"log2 Fold Change\", y = \"-log10 P-value\",\n       color = \"Significant\") +\n  theme_minimal()\n\n\n\n\n\n\nCode# Select top features\ntop_features &lt;- head(feature_stats$feature[feature_stats$significant], 50)\n\n# If no significant features, use top 50 by p-value\nif (length(top_features) == 0) {\n  top_features &lt;- head(feature_stats$feature, 50)\n  cat(\"No significant features found. Using top 50 features by p-value.\\n\")\n}\n\nNo significant features found. Using top 50 features by p-value.\n\nCodeselected_features &lt;- processed_features[, top_features, drop = FALSE]\n\ncat(\"Selected\", ncol(selected_features), \"features for ML\\n\")\n\nSelected 50 features for ML\n\n\n\n12.5.2 Classification Models\n\nCode# Load required ML packages\nif (!requireNamespace(\"randomForest\", quietly = TRUE)) {\n  cat(\"Note: randomForest package not installed. Install with: install.packages('randomForest')\\n\")\n} else {\n  library(randomForest)\n}\n\nif (!requireNamespace(\"e1071\", quietly = TRUE)) {\n  cat(\"Note: e1071 package not installed. Install with: install.packages('e1071')\\n\")\n} else {\n  library(e1071)\n}\n\n# Prepare data for machine learning\nprepare_ml_data &lt;- function(features, metadata) {\n  # Create training/testing split\n  set.seed(42)\n  train_indices &lt;- sample(1:nrow(features), size = 0.7 * nrow(features))\n  \n  ml_data &lt;- list(\n    train_x = features[train_indices, ],\n    train_y = factor(metadata$class[train_indices]),\n    test_x = features[-train_indices, ],\n    test_y = factor(metadata$class[-train_indices]),\n    train_indices = train_indices\n  )\n  \n  return(ml_data)\n}\n\nml_data &lt;- prepare_ml_data(selected_features, sample_metadata)\n\ncat(\"Training set:\", nrow(ml_data$train_x), \"samples\\n\")\n\nTraining set: 35 samples\n\nCodecat(\"Test set:\", nrow(ml_data$test_x), \"samples\\n\")\n\nTest set: 15 samples\n\nCode# Random Forest Classification\ntrain_random_forest &lt;- function(train_x, train_y, ntree = 500) {\n  if (!requireNamespace(\"randomForest\", quietly = TRUE)) {\n    cat(\"randomForest package not available, skipping...\\n\")\n    return(NULL)\n  }\n  rf_model &lt;- randomForest(\n    x = train_x,\n    y = train_y,\n    ntree = ntree,\n    importance = TRUE\n  )\n  return(rf_model)\n}\n\nrf_model &lt;- train_random_forest(ml_data$train_x, ml_data$train_y)\n\n# SVM Classification\ntrain_svm &lt;- function(train_x, train_y) {\n  if (!requireNamespace(\"e1071\", quietly = TRUE)) {\n    cat(\"e1071 package not available, skipping...\\n\")\n    return(NULL)\n  }\n  svm_model &lt;- svm(\n    x = train_x,\n    y = train_y,\n    kernel = \"radial\",\n    probability = TRUE\n  )\n  return(svm_model)\n}\n\nsvm_model &lt;- train_svm(ml_data$train_x, ml_data$train_y)\n\nif (!is.null(rf_model) && !is.null(svm_model)) {\n  cat(\"Trained Random Forest and SVM models\\n\")\n} else {\n  cat(\"Some models could not be trained due to missing packages\\n\")\n}\n\nTrained Random Forest and SVM models\n\n\n\n12.5.3 Model Evaluation\n\nCode# Load required packages for evaluation\nif (!requireNamespace(\"ROCR\", quietly = TRUE)) {\n  cat(\"Note: ROCR package not installed. Install with: install.packages('ROCR')\\n\")\n} else {\n  library(ROCR)\n}\n\nNote: ROCR package not installed. Install with: install.packages('ROCR')\n\nCode# Custom confusion matrix function if caret is not available\ncalculate_confusion_matrix &lt;- function(predictions, actual) {\n  conf_mat &lt;- table(Predicted = predictions, Actual = actual)\n  \n  # Calculate metrics\n  accuracy &lt;- sum(diag(conf_mat)) / sum(conf_mat)\n  \n  # For binary classification\n  if (nrow(conf_mat) == 2) {\n    tp &lt;- conf_mat[2, 2]\n    tn &lt;- conf_mat[1, 1]\n    fp &lt;- conf_mat[2, 1]\n    fn &lt;- conf_mat[1, 2]\n    \n    sensitivity &lt;- tp / (tp + fn)\n    specificity &lt;- tn / (tn + fp)\n    precision &lt;- tp / (tp + fp)\n  } else {\n    sensitivity &lt;- NA\n    specificity &lt;- NA\n    precision &lt;- NA\n  }\n  \n  return(list(\n    table = conf_mat,\n    overall = c(Accuracy = accuracy, Sensitivity = sensitivity, \n                Specificity = specificity, Precision = precision)\n  ))\n}\n\n# Evaluate model performance\nevaluate_model &lt;- function(model, test_x, test_y, model_name) {\n  if (is.null(model)) {\n    cat(\"Model\", model_name, \"is NULL, skipping evaluation\\n\")\n    return(NULL)\n  }\n  \n  if (model_name == \"RandomForest\") {\n    predictions &lt;- predict(model, test_x)\n    probabilities &lt;- predict(model, test_x, type = \"prob\")[, \"Disease\"]\n  } else if (model_name == \"SVM\") {\n    predictions &lt;- predict(model, test_x)\n    prob_matrix &lt;- attr(predict(model, test_x, probability = TRUE), \"probabilities\")\n    probabilities &lt;- prob_matrix[, \"Disease\"]\n  }\n  \n  # Confusion matrix\n  conf_matrix &lt;- calculate_confusion_matrix(predictions, test_y)\n  \n  # ROC analysis (if ROCR is available)\n  if (requireNamespace(\"ROCR\", quietly = TRUE)) {\n    pred_obj &lt;- ROCR::prediction(probabilities, test_y)\n    perf_obj &lt;- ROCR::performance(pred_obj, \"tpr\", \"fpr\")\n    auc_obj &lt;- ROCR::performance(pred_obj, \"auc\")\n    auc_value &lt;- auc_obj@y.values[[1]]\n    \n    roc_data &lt;- data.frame(\n      fpr = perf_obj@x.values[[1]],\n      tpr = perf_obj@y.values[[1]]\n    )\n  } else {\n    auc_value &lt;- NA\n    roc_data &lt;- data.frame(fpr = numeric(0), tpr = numeric(0))\n  }\n  \n  return(list(\n    model_name = model_name,\n    confusion_matrix = conf_matrix,\n    predictions = predictions,\n    probabilities = probabilities,\n    roc_data = roc_data,\n    auc = auc_value\n  ))\n}\n\n# Evaluate both models\nrf_eval &lt;- evaluate_model(rf_model, ml_data$test_x, ml_data$test_y, \"RandomForest\")\nsvm_eval &lt;- evaluate_model(svm_model, ml_data$test_x, ml_data$test_y, \"SVM\")\n\n# Print performance metrics\nif (!is.null(rf_eval)) {\n  cat(\"Random Forest Performance:\\n\")\n  print(rf_eval$confusion_matrix$overall)\n  cat(\"AUC:\", round(rf_eval$auc, 3), \"\\n\\n\")\n}\n\nRandom Forest Performance:\n   Accuracy Sensitivity Specificity   Precision \n  0.6000000   0.3333333   1.0000000   1.0000000 \nAUC: NA \n\nCodeif (!is.null(svm_eval)) {\n  cat(\"SVM Performance:\\n\")\n  print(svm_eval$confusion_matrix$overall)\n  cat(\"AUC:\", round(svm_eval$auc, 3), \"\\n\")\n}\n\nSVM Performance:\n   Accuracy Sensitivity Specificity   Precision \n  0.7333333   0.5555556   1.0000000   1.0000000 \nAUC: NA \n\nCode# Plot ROC curves (if both models were evaluated)\nif (!is.null(rf_eval) && !is.null(svm_eval) && \n    nrow(rf_eval$roc_data) &gt; 0 && nrow(svm_eval$roc_data) &gt; 0) {\n  \n  roc_combined &lt;- rbind(\n    data.frame(rf_eval$roc_data, Model = \"Random Forest\"),\n    data.frame(svm_eval$roc_data, Model = \"SVM\")\n  )\n  \n  print(ggplot(roc_combined, aes(x = fpr, y = tpr, color = Model)) +\n    geom_line(size = 1) +\n    geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray\") +\n    labs(title = \"ROC Curves Comparison\",\n         x = \"False Positive Rate\", y = \"True Positive Rate\") +\n    annotate(\"text\", x = 0.6, y = 0.3, \n             label = paste(\"RF AUC =\", round(rf_eval$auc, 3))) +\n    annotate(\"text\", x = 0.6, y = 0.2, \n             label = paste(\"SVM AUC =\", round(svm_eval$auc, 3))) +\n    theme_minimal())\n}\n\n\n\n12.5.4 Feature Importance Analysis\n\nCode# Analyze feature importance\nanalyze_feature_importance &lt;- function(rf_model, feature_names) {\n  importance_scores &lt;- importance(rf_model)\n  \n  importance_df &lt;- data.frame(\n    feature = feature_names,\n    mean_decrease_accuracy = importance_scores[, \"MeanDecreaseAccuracy\"],\n    mean_decrease_gini = importance_scores[, \"MeanDecreaseGini\"]\n  ) %&gt;%\n    arrange(desc(mean_decrease_accuracy))\n  \n  return(importance_df)\n}\n\nfeature_importance &lt;- analyze_feature_importance(rf_model, colnames(selected_features))\n\n# Plot feature importance\ntop_features_plot &lt;- head(feature_importance, 20)\n\nggplot(top_features_plot, aes(x = reorder(feature, mean_decrease_accuracy), \n                             y = mean_decrease_accuracy)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\", alpha = 0.7) +\n  coord_flip() +\n  labs(title = \"Top 20 Most Important Features (Random Forest)\",\n       x = \"Features\", y = \"Mean Decrease Accuracy\") +\n  theme_minimal()",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "11-advanced-topics.html#ion-mobility-spectrometry-ms-ims-ms",
    "href": "11-advanced-topics.html#ion-mobility-spectrometry-ms-ims-ms",
    "title": "\n12  Advanced Topics and Applications\n",
    "section": "\n12.6 Ion Mobility Spectrometry-MS (IMS-MS)",
    "text": "12.6 Ion Mobility Spectrometry-MS (IMS-MS)\n\n12.6.1 IMS Data Simulation and Processing\n\nCode# Simulate IMS-MS data\nsimulate_ims_data &lt;- function(n_scans = 1000, n_drift_bins = 200, n_mz_bins = 500) {\n  # Create coordinate system\n  drift_times &lt;- seq(10, 50, length.out = n_drift_bins)  # ms\n  mz_values &lt;- seq(100, 1000, length.out = n_mz_bins)\n  retention_times &lt;- seq(60, 1800, length.out = n_scans)  # seconds\n  \n  # Simulate IMS-MS data structure\n  ims_data &lt;- list()\n  \n  for (scan in 1:min(100, n_scans)) {  # Limit for memory\n    # Create 2D IMS-MS spectrum (drift time x m/z)\n    intensity_matrix &lt;- matrix(0, nrow = n_drift_bins, ncol = n_mz_bins)\n    \n    # Add some peaks\n    n_peaks &lt;- sample(20:80, 1)\n    \n    for (peak in 1:n_peaks) {\n      # Random peak position\n      dt_center &lt;- sample(1:n_drift_bins, 1)\n      mz_center &lt;- sample(1:n_mz_bins, 1)\n      \n      # Peak intensity\n      peak_intensity &lt;- rlnorm(1, meanlog = 8, sdlog = 1)\n      \n      # Add Gaussian peak\n      for (dt in max(1, dt_center-5):min(n_drift_bins, dt_center+5)) {\n        for (mz in max(1, mz_center-3):min(n_mz_bins, mz_center+3)) {\n          distance &lt;- sqrt((dt - dt_center)^2 + (mz - mz_center)^2)\n          if (distance &lt; 5) {\n            intensity_matrix[dt, mz] &lt;- intensity_matrix[dt, mz] + \n              peak_intensity * exp(-distance^2 / 4)\n          }\n        }\n      }\n    }\n    \n    ims_data[[scan]] &lt;- list(\n      scan_number = scan,\n      retention_time = retention_times[scan],\n      drift_times = drift_times,\n      mz_values = mz_values,\n      intensity_matrix = intensity_matrix\n    )\n  }\n  \n  return(ims_data)\n}\n\n# Generate IMS data\nims_dataset &lt;- simulate_ims_data(n_scans = 50)\ncat(\"Created IMS dataset with\", length(ims_dataset), \"scans\\n\")\n\nCreated IMS dataset with 50 scans\n\nCode# Example: visualize one IMS-MS spectrum\nif (length(ims_dataset) &gt; 0) {\n  example_scan &lt;- ims_dataset[[25]]\n  \n  # Create heatmap data\n  heatmap_data &lt;- expand.grid(\n    drift_time = example_scan$drift_times,\n    mz = example_scan$mz_values\n  )\n  heatmap_data$intensity &lt;- as.vector(example_scan$intensity_matrix)\n  \n  # Filter to show only non-zero intensities\n  heatmap_data &lt;- heatmap_data[heatmap_data$intensity &gt; 0, ]\n  \n  if (nrow(heatmap_data) &gt; 0) {\n    ggplot(heatmap_data, aes(x = mz, y = drift_time, fill = intensity)) +\n      geom_raster() +\n      scale_fill_viridis_c(trans = \"sqrt\") +\n      labs(title = paste(\"IMS-MS Spectrum - Scan\", example_scan$scan_number),\n           subtitle = paste(\"RT =\", round(example_scan$retention_time, 1), \"s\"),\n           x = \"m/z\", y = \"Drift Time (ms)\") +\n      theme_minimal()\n  }\n}\n\n\n\n\n\n\n\n\n12.6.2 IMS Peak Detection\n\nCode# IMS peak detection algorithm\ndetect_ims_peaks &lt;- function(ims_scan, intensity_threshold = 1000) {\n  intensity_matrix &lt;- ims_scan$intensity_matrix\n  drift_times &lt;- ims_scan$drift_times\n  mz_values &lt;- ims_scan$mz_values\n  \n  # Find peaks above threshold\n  peak_positions &lt;- which(intensity_matrix &gt; intensity_threshold, arr.ind = TRUE)\n  \n  if (nrow(peak_positions) == 0) {\n    return(data.frame())\n  }\n  \n  # Extract peak information\n  peaks &lt;- data.frame(\n    drift_time_index = peak_positions[, 1],\n    mz_index = peak_positions[, 2],\n    drift_time = drift_times[peak_positions[, 1]],\n    mz = mz_values[peak_positions[, 2]],\n    intensity = intensity_matrix[peak_positions]\n  )\n  \n  # Calculate collision cross section (CCS) - simplified relationship\n  peaks$ccs &lt;- calculate_ccs(peaks$drift_time, peaks$mz)\n  \n  return(peaks)\n}\n\n# Simplified CCS calculation\ncalculate_ccs &lt;- function(drift_time, mz, temperature = 293, pressure = 760) {\n  # Simplified Mason-Schamp equation\n  # CCS proportional to drift_time / sqrt(mz)\n  ccs &lt;- drift_time * 100 / sqrt(mz)  # Arbitrary units for demonstration\n  return(ccs)\n}\n\n# Detect peaks in example scan\nif (length(ims_dataset) &gt; 0) {\n  example_peaks &lt;- detect_ims_peaks(ims_dataset[[25]], intensity_threshold = 500)\n  \n  if (nrow(example_peaks) &gt; 0) {\n    cat(\"Detected\", nrow(example_peaks), \"peaks in example scan\\n\")\n    \n    # Plot peaks in CCS vs m/z space\n    ggplot(example_peaks, aes(x = mz, y = ccs, size = intensity)) +\n      geom_point(alpha = 0.7, color = \"blue\") +\n      scale_size_continuous(range = c(1, 5)) +\n      labs(title = \"IMS Peaks in CCS vs m/z Space\",\n           x = \"m/z\", y = \"CCS (Arbitrary Units)\",\n           size = \"Intensity\") +\n      theme_minimal()\n  }\n}\n\nDetected 1653 peaks in example scan",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "11-advanced-topics.html#advanced-statistical-methods",
    "href": "11-advanced-topics.html#advanced-statistical-methods",
    "title": "\n12  Advanced Topics and Applications\n",
    "section": "\n12.7 Advanced Statistical Methods",
    "text": "12.7 Advanced Statistical Methods\n\n12.7.1 Survival Analysis for MS Data\n\nCodelibrary(survival)\nlibrary(survminer)\n\n# Simulate clinical data with MS measurements\nsimulate_survival_data &lt;- function(n_patients = 200) {\n  # Patient characteristics\n  patients &lt;- data.frame(\n    patient_id = paste0(\"P\", 1:n_patients),\n    age = rnorm(n_patients, 65, 10),\n    gender = sample(c(\"M\", \"F\"), n_patients, replace = TRUE),\n    stage = sample(c(\"I\", \"II\", \"III\", \"IV\"), n_patients, replace = TRUE, \n                  prob = c(0.2, 0.3, 0.3, 0.2))\n  )\n  \n  # Simulate biomarker levels (e.g., protein concentrations)\n  biomarker_data &lt;- data.frame(\n    patient_id = patients$patient_id,\n    protein_A = rlnorm(n_patients, 3, 0.5),\n    protein_B = rlnorm(n_patients, 4, 0.8),\n    protein_C = rlnorm(n_patients, 3.5, 0.6)\n  )\n  \n  # Merge data\n  combined_data &lt;- merge(patients, biomarker_data, by = \"patient_id\")\n  \n  # Simulate survival times\n  # Higher protein levels associated with better survival\n  hazard_ratio &lt;- exp(-0.5 * scale(combined_data$protein_A)[,1] + \n                     0.3 * (combined_data$stage == \"IV\") +\n                     0.2 * (combined_data$age &gt; 70))\n  \n  # Generate survival times using exponential distribution\n  survival_times &lt;- rexp(n_patients, rate = hazard_ratio * 0.1)\n  \n  # Generate censoring (assume some patients are still alive at end of study)\n  max_followup &lt;- 60  # months\n  event_status &lt;- ifelse(survival_times &lt; max_followup, 1, 0)\n  observed_times &lt;- pmin(survival_times, max_followup)\n  \n  combined_data$time &lt;- observed_times\n  combined_data$event = event_status\n  \n  # Categorize biomarker levels\n  combined_data$protein_A_high &lt;- combined_data$protein_A &gt; median(combined_data$protein_A)\n  \n  return(combined_data)\n}\n\nsurvival_data &lt;- simulate_survival_data()\n\n# Perform survival analysis\nsurv_object &lt;- Surv(time = survival_data$time, event = survival_data$event)\n\n# Kaplan-Meier analysis\nkm_fit &lt;- survfit(surv_object ~ protein_A_high, data = survival_data)\n\n# Plot survival curves\nggsurvplot(km_fit, \n          data = survival_data,\n          pval = TRUE,\n          conf.int = TRUE,\n          xlab = \"Time (months)\",\n          ylab = \"Overall Survival Probability\",\n          title = \"Survival Analysis by Protein A Level\",\n          legend.title = \"Protein A\",\n          legend.labs = c(\"Low\", \"High\"))\n\n\n\n\n\n\nCode# Cox proportional hazards model\ncox_model &lt;- coxph(surv_object ~ protein_A + protein_B + protein_C + \n                  age + gender + stage, data = survival_data)\n\nsummary(cox_model)\n\nCall:\ncoxph(formula = surv_object ~ protein_A + protein_B + protein_C + \n    age + gender + stage, data = survival_data)\n\n  n= 200, number of events= 195 \n\n                coef  exp(coef)   se(coef)      z Pr(&gt;|z|)    \nprotein_A -0.0381232  0.9625943  0.0065579 -5.813 6.12e-09 ***\nprotein_B -0.0009910  0.9990095  0.0009303 -1.065    0.287    \nprotein_C -0.0038282  0.9961791  0.0026701 -1.434    0.152    \nage        0.0011187  1.0011194  0.0073091  0.153    0.878    \ngenderM    0.0755537  1.0784811  0.1492284  0.506    0.613    \nstageII    0.1418220  1.1523715  0.2093917  0.677    0.498    \nstageIII  -0.0572136  0.9443923  0.2076276 -0.276    0.783    \nstageIV    0.4426659  1.5568521  0.2545010  1.739    0.082 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n          exp(coef) exp(-coef) lower .95 upper .95\nprotein_A    0.9626     1.0389    0.9503     0.975\nprotein_B    0.9990     1.0010    0.9972     1.001\nprotein_C    0.9962     1.0038    0.9910     1.001\nage          1.0011     0.9989    0.9869     1.016\ngenderM      1.0785     0.9272    0.8050     1.445\nstageII      1.1524     0.8678    0.7645     1.737\nstageIII     0.9444     1.0589    0.6287     1.419\nstageIV      1.5569     0.6423    0.9454     2.564\n\nConcordance= 0.606  (se = 0.024 )\nLikelihood ratio test= 52.71  on 8 df,   p=1e-08\nWald test            = 39.87  on 8 df,   p=3e-06\nScore (logrank) test = 40.17  on 8 df,   p=3e-06\n\n\n\n12.7.2 Network Analysis\n\nCodelibrary(igraph)\n\n# Create protein-protein interaction network from correlation data\ncreate_protein_network &lt;- function(correlation_matrix, threshold = 0.7) {\n  # Convert correlation matrix to adjacency matrix\n  adj_matrix &lt;- abs(correlation_matrix) &gt; threshold\n  diag(adj_matrix) &lt;- FALSE\n  \n  # Create igraph object\n  network &lt;- graph_from_adjacency_matrix(adj_matrix, mode = \"undirected\")\n  \n  # Add edge weights\n  edges &lt;- get.edgelist(network)\n  edge_weights &lt;- numeric(nrow(edges))\n  \n  for (i in 1:nrow(edges)) {\n    from_node &lt;- edges[i, 1]\n    to_node &lt;- edges[i, 2]\n    edge_weights[i] &lt;- abs(correlation_matrix[from_node, to_node])\n  }\n  \n  E(network)$weight &lt;- edge_weights\n  \n  return(network)\n}\n\n# Create example correlation matrix from earlier proteomics data\nif (exists(\"normalized_intensities\")) {\n  # Calculate correlation for subset of proteins\n  protein_subset &lt;- normalized_intensities[1:20, ]  # Use first 20 proteins\n  protein_cor &lt;- cor(t(log2(protein_subset)), use = \"complete.obs\")\n  \n  # Create network\n  protein_network &lt;- create_protein_network(protein_cor, threshold = 0.6)\n  \n  # Network analysis\n  cat(\"Network properties:\\n\")\n  cat(\"  Nodes:\", vcount(protein_network), \"\\n\")\n  cat(\"  Edges:\", ecount(protein_network), \"\\n\")\n  cat(\"  Density:\", edge_density(protein_network), \"\\n\")\n  \n  # Calculate centrality measures\n  betweenness_centrality &lt;- betweenness(protein_network)\n  degree_centrality &lt;- degree(protein_network)\n  \n  # Plot network\n  if (ecount(protein_network) &gt; 0) {\n    plot(protein_network, \n         vertex.size = degree_centrality * 3 + 5,\n         vertex.color = \"lightblue\",\n         vertex.label.cex = 0.8,\n         edge.width = E(protein_network)$weight * 3,\n         main = \"Protein Correlation Network\",\n         layout = layout_with_fr)\n  }\n}",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "11-advanced-topics.html#method-development-and-validation",
    "href": "11-advanced-topics.html#method-development-and-validation",
    "title": "\n12  Advanced Topics and Applications\n",
    "section": "\n12.8 Method Development and Validation",
    "text": "12.8 Method Development and Validation\n\n12.8.1 Analytical Method Validation\n\nCode# Simulate analytical validation data\nsimulate_validation_data &lt;- function() {\n  # Calibration standards\n  concentrations &lt;- c(0, 0.1, 0.5, 1, 5, 10, 50, 100)  # ng/mL\n  \n  validation_data &lt;- list()\n  \n  # Linearity assessment\n  linearity_data &lt;- data.frame()\n  for (conc in concentrations) {\n    for (replicate in 1:5) {\n      # Simulate instrument response with some noise\n      true_response &lt;- 1000 * conc + 500  # Linear relationship\n      observed_response &lt;- true_response + rnorm(1, 0, true_response * 0.05)  # 5% CV\n      \n      linearity_data &lt;- rbind(linearity_data, data.frame(\n        concentration = conc,\n        response = observed_response,\n        replicate = replicate\n      ))\n    }\n  }\n  \n  # Precision (repeatability and intermediate precision)\n  precision_data &lt;- data.frame()\n  test_concentrations &lt;- c(1, 10, 50)  # Low, medium, high\n  \n  for (day in 1:3) {\n    for (conc in test_concentrations) {\n      for (replicate in 1:6) {\n        true_response &lt;- 1000 * conc + 500\n        day_effect &lt;- rnorm(1, 0, true_response * 0.02)  # Day-to-day variation\n        observed_response &lt;- true_response + day_effect + \n                           rnorm(1, 0, true_response * 0.03)  # Within-day variation\n        \n        precision_data &lt;- rbind(precision_data, data.frame(\n          day = day,\n          concentration = conc,\n          response = observed_response,\n          replicate = replicate\n        ))\n      }\n    }\n  }\n  \n  # Accuracy (spike recovery)\n  accuracy_data &lt;- data.frame()\n  spike_levels &lt;- c(0.5, 5, 50)  # ng/mL\n  \n  for (spike in spike_levels) {\n    for (replicate in 1:5) {\n      expected_response &lt;- 1000 * spike + 500\n      recovery_rate &lt;- runif(1, 0.85, 1.15)  # 85-115% recovery\n      observed_response &lt;- expected_response * recovery_rate\n      \n      accuracy_data &lt;- rbind(accuracy_data, data.frame(\n        spiked_concentration = spike,\n        observed_response = observed_response,\n        expected_response = expected_response,\n        recovery_percent = recovery_rate * 100,\n        replicate = replicate\n      ))\n    }\n  }\n  \n  return(list(\n    linearity = linearity_data,\n    precision = precision_data,\n    accuracy = accuracy_data\n  ))\n}\n\nvalidation_results &lt;- simulate_validation_data()\n\n# Linearity assessment\nlinearity_summary &lt;- validation_results$linearity %&gt;%\n  group_by(concentration) %&gt;%\n  summarise(\n    mean_response = mean(response),\n    sd_response = sd(response),\n    cv_percent = sd(response) / mean(response) * 100,\n    .groups = 'drop'\n  )\n\n# Linear regression\nlm_model &lt;- lm(mean_response ~ concentration, data = linearity_summary)\nr_squared &lt;- summary(lm_model)$r.squared\n\n# Plot linearity\nggplot(validation_results$linearity, aes(x = concentration, y = response)) +\n  geom_point(alpha = 0.6, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(title = paste(\"Linearity Assessment (R² =\", round(r_squared, 4), \")\"),\n       x = \"Concentration (ng/mL)\", y = \"Instrument Response\") +\n  theme_minimal()\n\n\n\n\n\n\nCode# Precision assessment\nprecision_summary &lt;- validation_results$precision %&gt;%\n  group_by(concentration, day) %&gt;%\n  summarise(\n    mean_response = mean(response),\n    sd_response = sd(response),\n    cv_percent = sd(response) / mean(response) * 100,\n    .groups = 'drop'\n  )\n\n# Overall precision summary\noverall_precision &lt;- validation_results$precision %&gt;%\n  group_by(concentration) %&gt;%\n  summarise(\n    repeatability_cv = sd(response) / mean(response) * 100,\n    .groups = 'drop'\n  )\n\nprint(\"Precision Assessment:\")\n\n[1] \"Precision Assessment:\"\n\nCodeprint(overall_precision)\n\n# A tibble: 3 × 2\n  concentration repeatability_cv\n          &lt;dbl&gt;            &lt;dbl&gt;\n1             1             3.53\n2            10             3.36\n3            50             2.81\n\nCode# Accuracy assessment\naccuracy_summary &lt;- validation_results$accuracy %&gt;%\n  group_by(spiked_concentration) %&gt;%\n  summarise(\n    mean_recovery = mean(recovery_percent),\n    sd_recovery = sd(recovery_percent),\n    .groups = 'drop'\n  )\n\nprint(\"Accuracy Assessment:\")\n\n[1] \"Accuracy Assessment:\"\n\nCodeprint(accuracy_summary)\n\n# A tibble: 3 × 3\n  spiked_concentration mean_recovery sd_recovery\n                 &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1                  0.5          98.8       10.0 \n2                  5           102.         9.55\n3                 50            99.3       10.5 \n\n\n\n12.8.2 Quality Control Charts\n\nCode# Create QC charts for method monitoring\ncreate_qc_charts &lt;- function(qc_data) {\n  # Simulate QC data over time\n  n_days &lt;- 30\n  qc_measurements &lt;- data.frame()\n  \n  for (day in 1:n_days) {\n    # QC low level\n    qc_low &lt;- rnorm(1, 950, 50)  # Target = 1000, ±5%\n    \n    # QC medium level  \n    qc_medium &lt;- rnorm(1, 9500, 300)  # Target = 10000, ±3%\n    \n    # QC high level\n    qc_high &lt;- rnorm(1, 47500, 1200)  # Target = 50000, ±2.5%\n    \n    qc_measurements &lt;- rbind(qc_measurements, data.frame(\n      day = day,\n      qc_level = rep(c(\"Low\", \"Medium\", \"High\"), each = 1),\n      measurement = c(qc_low, qc_medium, qc_high),\n      target = c(1000, 10000, 50000),\n      ucl = c(1100, 10300, 51250),  # Upper control limit\n      lcl = c(900, 9700, 48750),    # Lower control limit\n      uwl = c(1050, 10150, 50625),  # Upper warning limit\n      lwl = c(950, 9850, 49375)     # Lower warning limit\n    ))\n  }\n  \n  return(qc_measurements)\n}\n\nqc_data &lt;- create_qc_charts()\n\n# Plot QC charts\nggplot(qc_data, aes(x = day, y = measurement)) +\n  geom_line(color = \"blue\") +\n  geom_point(aes(color = ifelse(measurement &gt; ucl | measurement &lt; lcl, \"Out of Control\", \"In Control\"))) +\n  geom_hline(aes(yintercept = target), color = \"green\", linetype = \"solid\") +\n  geom_hline(aes(yintercept = ucl), color = \"red\", linetype = \"dashed\") +\n  geom_hline(aes(yintercept = lcl), color = \"red\", linetype = \"dashed\") +\n  geom_hline(aes(yintercept = uwl), color = \"orange\", linetype = \"dotted\") +\n  geom_hline(aes(yintercept = lwl), color = \"orange\", linetype = \"dotted\") +\n  facet_wrap(~qc_level, scales = \"free_y\") +\n  scale_color_manual(values = c(\"In Control\" = \"blue\", \"Out of Control\" = \"red\")) +\n  labs(title = \"Quality Control Charts\",\n       x = \"Day\", y = \"QC Measurement\",\n       color = \"QC Status\") +\n  theme_minimal()",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "11-advanced-topics.html#exercises",
    "href": "11-advanced-topics.html#exercises",
    "title": "\n12  Advanced Topics and Applications\n",
    "section": "\n12.9 Exercises",
    "text": "12.9 Exercises\n\nImplement a deep learning model for mass spectral classification\nDevelop an algorithm for automatic peak alignment across multiple samples\nCreate a method for isotope pattern recognition and deconvolution\nBuild a comprehensive data processing pipeline with quality control\nImplement real-time data analysis for online MS monitoring",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "11-advanced-topics.html#summary",
    "href": "11-advanced-topics.html#summary",
    "title": "\n12  Advanced Topics and Applications\n",
    "section": "\n12.10 Summary",
    "text": "12.10 Summary\nThis chapter covered advanced topics in mass spectrometry data analysis, including machine learning applications, ion mobility spectrometry, survival analysis, network analysis, and analytical method validation. These advanced techniques enable sophisticated analysis of complex MS datasets and support method development and validation efforts.",
    "crumbs": [
      "Advanced Topics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Advanced Topics and Applications</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "\n13  Summary and Future Directions\n",
    "section": "",
    "text": "13.1 Book Learning Path\nThis book has provided a comprehensive journey through mass spectrometry data analysis using R and the R for Mass Spectrometry ecosystem. Let’s review the key concepts and look toward future developments.\nCodeflowchart TD\n    subgraph Foundations[\"Part I: Foundations\"]\n        A1[MS Principles&lt;br/&gt;Theory & Instrumentation]\n        A2[Getting Started&lt;br/&gt;Hands-on R Introduction]\n        A1 --&gt; A2\n    end\n    \n    subgraph Core[\"Part II: Core Techniques\"]\n        B1[R Fundamentals&lt;br/&gt;Packages & Ecosystem]\n        B2[Data Formats&lt;br/&gt;Spectra Objects]\n        B3[Preprocessing&lt;br/&gt;Baseline & Smoothing]\n        B4[Peak Detection&lt;br/&gt;MAD & Quantification]\n        B1 --&gt; B2 --&gt; B3 --&gt; B4\n    end\n    \n    subgraph Analysis[\"Part III: Analysis & Visualization\"]\n        C1[Visualization&lt;br/&gt;Plots & Graphics]\n        C2[Statistical Analysis&lt;br/&gt;PCA, limma, Clustering]\n        C1 --&gt; C2\n    end\n    \n    subgraph Applications[\"Part IV: Applications\"]\n        D1[Metabolomics&lt;br/&gt;xcms Workflow]\n        D2[Proteomics&lt;br/&gt;PSM & Protein Inference]\n        D3[QFeatures&lt;br/&gt;Quantitative Analysis]\n        D1 --&gt; D2 --&gt; D3\n    end\n    \n    subgraph Advanced[\"Part V: Advanced Topics\"]\n        E1[Backends&lt;br/&gt;Performance & Scale]\n        E2[Parallel Processing&lt;br/&gt;BiocParallel]\n        E3[Integration&lt;br/&gt;Databases & Resources]\n        E1 --&gt; E2 --&gt; E3\n    end\n    \n    A2 --&gt; B1\n    B4 --&gt; C1\n    C2 --&gt; D1\n    D3 --&gt; E1\n    \n  style Foundations fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style Core fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n  style Analysis fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style Applications fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n  style Advanced fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n\n\n\n\nflowchart TD\n    subgraph Foundations[\"Part I: Foundations\"]\n        A1[MS Principles&lt;br/&gt;Theory & Instrumentation]\n        A2[Getting Started&lt;br/&gt;Hands-on R Introduction]\n        A1 --&gt; A2\n    end\n    \n    subgraph Core[\"Part II: Core Techniques\"]\n        B1[R Fundamentals&lt;br/&gt;Packages & Ecosystem]\n        B2[Data Formats&lt;br/&gt;Spectra Objects]\n        B3[Preprocessing&lt;br/&gt;Baseline & Smoothing]\n        B4[Peak Detection&lt;br/&gt;MAD & Quantification]\n        B1 --&gt; B2 --&gt; B3 --&gt; B4\n    end\n    \n    subgraph Analysis[\"Part III: Analysis & Visualization\"]\n        C1[Visualization&lt;br/&gt;Plots & Graphics]\n        C2[Statistical Analysis&lt;br/&gt;PCA, limma, Clustering]\n        C1 --&gt; C2\n    end\n    \n    subgraph Applications[\"Part IV: Applications\"]\n        D1[Metabolomics&lt;br/&gt;xcms Workflow]\n        D2[Proteomics&lt;br/&gt;PSM & Protein Inference]\n        D3[QFeatures&lt;br/&gt;Quantitative Analysis]\n        D1 --&gt; D2 --&gt; D3\n    end\n    \n    subgraph Advanced[\"Part V: Advanced Topics\"]\n        E1[Backends&lt;br/&gt;Performance & Scale]\n        E2[Parallel Processing&lt;br/&gt;BiocParallel]\n        E3[Integration&lt;br/&gt;Databases & Resources]\n        E1 --&gt; E2 --&gt; E3\n    end\n    \n    A2 --&gt; B1\n    B4 --&gt; C1\n    C2 --&gt; D1\n    D3 --&gt; E1\n    \n  style Foundations fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style Core fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n  style Analysis fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43\n  style Applications fill:#FBE0FA,stroke:#B000B0,stroke-width:3px,color:#102A43\n  style Advanced fill:#D7E6FB,stroke:#27408B,stroke-width:3px,color:#102A43",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#book-learning-path",
    "href": "summary.html#book-learning-path",
    "title": "\n13  Summary and Future Directions\n",
    "section": "",
    "text": "📊 By the Numbers\n\n\n\n\n\n13 Chapters organized into 5 logical parts\n\n40+ R Packages for comprehensive MS analysis\n\n100+ Code Examples with error handling\n\n5 Workflow Diagrams illustrating key concepts\n\n~7,000 Lines of content and working code",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#what-weve-covered",
    "href": "summary.html#what-weve-covered",
    "title": "\n13  Summary and Future Directions\n",
    "section": "\n13.2 What We’ve Covered",
    "text": "13.2 What We’ve Covered\n\n13.2.1 Core Infrastructure (Chapters 1-2)\n\n\nR Fundamentals: Package installation, data structures, and the Bioconductor ecosystem\n\nData Formats: Working with mzML, MGF, and other MS file formats using Spectra\n\n\nBackend Architecture: Understanding MsBackendMzR, MsBackendDataFrame, and MsBackendHdf5Peaks for efficient data storage\n\n13.2.2 Data Processing (Chapters 3-4)\n\n\nPreprocessing: Baseline correction, smoothing (Savitzky-Golay), and noise reduction\n\nPeak Detection: MAD-based peak picking, noise estimation, and signal-to-noise calculations\n\nQuantification: Peak integration, area calculation, and quality metrics\n\n13.2.3 Analysis and Visualization (Chapters 5-6)\n\n\nVisualization: Spectral plots, chromatograms (TIC/BPC), mirror plots, and interactive graphics\n\nStatistical Methods: Descriptive statistics, PCA, clustering, differential analysis with limma\n\nQuality Control: CV analysis, missing value patterns, batch effect detection\n\n13.2.4 Application Areas (Chapters 7-8)\n\n\nMetabolomics: XCMS workflows, peak detection with CentWave, retention time correction, and correspondence\n\nProteomics: PSM handling, protein inference, database searching, and peptide-centric analysis\n\nQuantitative Proteomics: QFeatures framework for hierarchical data (PSMs → peptides → proteins)\n\n13.2.5 Advanced Topics (Chapters 9-10)\n\n\nBackend Management: Choosing appropriate backends for different data scales\n\nParallel Processing: BiocParallel for large-scale data processing\n\nQFeatures Workflows: Aggregation strategies, missing value handling, robust summarization\n\nIntegration: Connecting to online resources (GNPS, MassBank, MetaboLights)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#key-packages-in-the-r-for-mass-spectrometry-ecosystem",
    "href": "summary.html#key-packages-in-the-r-for-mass-spectrometry-ecosystem",
    "title": "\n13  Summary and Future Directions\n",
    "section": "\n13.3 Key Packages in the R for Mass Spectrometry Ecosystem",
    "text": "13.3 Key Packages in the R for Mass Spectrometry Ecosystem\n\n\n\n\n\n\n\n\n\nPackage\nPurpose\nKey Functions\n\n\n\nSpectra\nCore MS data infrastructure and spectral data handling\nSpectra(), filterMsLevel(), pickPeaks()\n\n\nQFeatures\nQuantitative features for proteomics workflows\nQFeatures(), aggregateFeatures(), filterNA()\n\n\nxcms\nLC-MS data processing and metabolomics\nfindChromPeaks(), adjustRtime(), groupChromPeaks()\n\n\nPSMatch\nPeptide-spectrum matching and protein identification\nPSM(), addFragments(), filterPSMs()\n\n\nMsCoreUtils\nCore utilities for MS data processing\nnoise(), compareSpectra(), robustSummary()\n\n\nMetaboCoreUtils\nUtilities specific to metabolomics analysis\nmass2mz(), calculateMass(), adductNames()\n\n\nProtGenerics\nGeneric functions for proteomics packages\nspectra(), peaks(), intensity()\n\n\nmsdata\nExample MS datasets for learning and testing\nproteomics(), sciex(), msdata()\n\n\nMsDataHub\nAccess to online MS data resources\nMsDataHub(), query(), recordTitle()",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#best-practices-for-ms-data-analysis-in-r",
    "href": "summary.html#best-practices-for-ms-data-analysis-in-r",
    "title": "\n13  Summary and Future Directions\n",
    "section": "\n13.4 Best Practices for MS Data Analysis in R",
    "text": "13.4 Best Practices for MS Data Analysis in R\n\n13.4.1 1. Choose the Right Backend\n\nCode# Small datasets: In-memory for speed\nsmall_data &lt;- Spectra(files, backend = MsBackendDataFrame())\n\n# Large datasets: On-disk for memory efficiency\nlarge_data &lt;- Spectra(files, backend = MsBackendMzR())\n\n# Very large or processed: HDF5 for balanced performance\nlibrary(MsBackendHdf5Peaks)\narchived_data &lt;- setBackend(data, MsBackendHdf5Peaks())\n\n\n\n13.4.2 2. Implement Quality Control\n\nCheck coefficient of variation (CV &lt; 30% for technical replicates)\nAssess missing value patterns\nMonitor batch effects with PCA\nValidate feature detection rates\n\n13.4.3 3. Use Appropriate Normalization\n\n\nMedian normalization: General purpose, robust to outliers\n\nTIC normalization: For consistent total signal across samples\n\nQuantile normalization: When distributions should be identical\n\nInternal standards: When available, most accurate\n\n13.4.4 4. Proper Statistical Testing\n\nUse limma for differential analysis (handles small sample sizes)\nApply multiple testing correction (FDR/Benjamini-Hochberg)\nCheck assumptions (normality, homoscedasticity)\nConsider batch effects in design matrix",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#reproducible-research-practices",
    "href": "summary.html#reproducible-research-practices",
    "title": "\n13  Summary and Future Directions\n",
    "section": "\n13.5 Reproducible Research Practices",
    "text": "13.5 Reproducible Research Practices\n\nCode# Document your analysis pipeline\n# 1. Record package versions\nsessionInfo()\n\n# 2. Use project-based workflows\nlibrary(here)\ndata_path &lt;- here(\"data\", \"raw\")\n\n# 3. Version control your analysis\n# Use Git for tracking changes\n\n# 4. Create R Markdown/Quarto reports\n# This entire book is an example!\n\n# 5. Share data and code\n# Deposit raw data in public repositories (PRIDE, MetaboLights)\n# Share analysis code on GitHub",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#future-directions-in-ms-data-analysis",
    "href": "summary.html#future-directions-in-ms-data-analysis",
    "title": "\n13  Summary and Future Directions\n",
    "section": "\n13.6 Future Directions in MS Data Analysis",
    "text": "13.6 Future Directions in MS Data Analysis\n\n13.6.1 Emerging Technologies\n\n\nIon Mobility MS: Additional separation dimension requiring new algorithms\n\nImaging MS: Spatial metabolomics and proteomics visualization\n\nTop-Down Proteomics: Intact protein analysis without digestion\n\nData-Independent Acquisition (DIA): Comprehensive MS/MS coverage\n\n13.6.2 Computational Advances\n\n\nDeep Learning: Neural networks for spectrum prediction and identification\n\nCloud Computing: Scalable processing of large cohort studies\n\nReal-Time Analysis: Online processing for quality control\n\nIntegration: Multi-omics data fusion (proteomics + metabolomics + genomics)\n\n13.6.3 Community Development\nThe R for Mass Spectrometry initiative continues to evolve:\n\nNew backends for emerging data formats\nEnhanced visualization capabilities\nImproved integration with online databases\nBetter support for non-standard MS applications",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#resources-for-continued-learning",
    "href": "summary.html#resources-for-continued-learning",
    "title": "\n13  Summary and Future Directions\n",
    "section": "\n13.7 Resources for Continued Learning",
    "text": "13.7 Resources for Continued Learning\n\n13.7.1 Official Documentation\n\n\nR for Mass Spectrometry Book: https://rformassspectrometry.github.io/book/\n\nSpectra Documentation: https://rformassspectrometry.github.io/Spectra/\n\nxcms Documentation: https://bioconductor.org/packages/xcms/\n\n13.7.2 Community\n\n\nBioconductor Support: https://support.bioconductor.org/\n\nR for Mass Spectrometry GitHub: https://github.com/RforMassSpectrometry\n\nMetabolomics Society: https://metabolomicssociety.org/\n\n13.7.3 Publications\nKey papers describing the R for Mass Spectrometry ecosystem provide deeper technical details and validation studies. Check package citations using citation(\"packagename\").",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  },
  {
    "objectID": "summary.html#final-thoughts",
    "href": "summary.html#final-thoughts",
    "title": "\n13  Summary and Future Directions\n",
    "section": "\n13.8 Final Thoughts",
    "text": "13.8 Final Thoughts\nMass spectrometry data analysis is a rapidly evolving field. The R for Mass Spectrometry ecosystem provides a robust, flexible, and open-source foundation for tackling both routine and cutting-edge analytical challenges.\nThe skills you’ve developed through this book - from basic data import to advanced statistical analysis - will serve as a strong foundation for your research. Remember:\n\n\nStart simple: Use built-in functions before implementing custom solutions\n\nValidate thoroughly: Test your analysis pipeline with known standards\n\nDocument everything: Future you (and collaborators) will be grateful\n\nEngage with the community: Share code, ask questions, contribute improvements\n\nThank you for joining this journey through R for Mass Spectrometry. Now, go forth and analyze!\n\n\nHappy analyzing! 🔬📊",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Summary and Future Directions</span>"
    ]
  }
]